{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0:\n",
      "Original:\n",
      "tensor([[-1.5667, -0.8848, -0.0823, -1.0963],\n",
      "        [-0.2630, -1.9768, -1.0156,  1.9896],\n",
      "        [-0.4240,  0.9485, -2.6145, -1.4046],\n",
      "        [ 0.6807,  0.2921, -0.8919,  1.0357],\n",
      "        [-0.3071,  0.4782, -0.0400,  0.0223],\n",
      "        [-0.0794, -0.3305,  0.8095, -1.4395],\n",
      "        [ 0.2538, -0.8903,  0.3576, -0.7388],\n",
      "        [-1.0737,  0.1499, -0.8428,  0.6036],\n",
      "        [ 2.2194,  1.8892, -0.3977, -0.5240],\n",
      "        [ 0.2136, -0.3794, -1.0174,  1.5791],\n",
      "        [-0.7023,  0.5966,  0.8818,  0.3092],\n",
      "        [-1.2846,  0.4203, -2.1773,  0.3069]])\n",
      "Masked:\n",
      "tensor([[-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [ 6.8074e-01,  2.9214e-01, -8.9190e-01,  1.0357e+00],\n",
      "        [-3.0712e-01,  4.7820e-01, -4.0047e-02,  2.2303e-02],\n",
      "        [-7.9409e-02, -3.3050e-01,  8.0954e-01, -1.4395e+00],\n",
      "        [ 2.5378e-01, -8.9029e-01,  3.5758e-01, -7.3885e-01],\n",
      "        [-1.0737e+00,  1.4991e-01, -8.4285e-01,  6.0360e-01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-1.2846e+00,  4.2026e-01, -2.1773e+00,  3.0694e-01]])\n",
      "Mask positions: [0, 1, 2, 8, 9, 10]\n",
      "\n",
      "Sample 1:\n",
      "Original:\n",
      "tensor([[-0.9034,  0.5856,  1.4360,  1.5973],\n",
      "        [-1.3649, -1.2709,  0.1486, -0.5789],\n",
      "        [-0.5987, -1.2989,  1.6562, -2.2106],\n",
      "        [-1.0345,  0.3012, -1.0185,  0.3474],\n",
      "        [ 1.1421, -0.0552, -1.9057,  0.8582],\n",
      "        [ 0.4565, -0.5931,  0.1846,  0.3480],\n",
      "        [ 0.4704,  0.1593, -1.6408, -0.3704],\n",
      "        [-0.3444, -0.0763,  0.2898, -0.8294],\n",
      "        [ 2.0255, -0.7030,  0.0207, -0.4025],\n",
      "        [-0.6412, -2.1793,  0.9652, -0.7101],\n",
      "        [-0.2891,  0.7326,  0.0354, -1.6045],\n",
      "        [ 0.1476, -0.1946,  0.1133,  1.7815]])\n",
      "Masked:\n",
      "tensor([[-9.0343e-01,  5.8557e-01,  1.4360e+00,  1.5973e+00],\n",
      "        [-1.3649e+00, -1.2709e+00,  1.4857e-01, -5.7889e-01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [ 1.1421e+00, -5.5213e-02, -1.9057e+00,  8.5823e-01],\n",
      "        [ 4.5653e-01, -5.9313e-01,  1.8457e-01,  3.4795e-01],\n",
      "        [ 4.7040e-01,  1.5928e-01, -1.6408e+00, -3.7044e-01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-9.9000e+01, -9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
      "        [-2.8914e-01,  7.3263e-01,  3.5388e-02, -1.6045e+00],\n",
      "        [ 1.4765e-01, -1.9457e-01,  1.1326e-01,  1.7815e+00]])\n",
      "Mask positions: [2, 3, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "\n",
    "\n",
    "class DummySpecAug:\n",
    "    def __init__(self, mask_token, patch_height=1, max_mask_pct=0.2):\n",
    "        self.mask_token = mask_token\n",
    "        self.patch_height = patch_height\n",
    "        self.max_mask_pct = max_mask_pct\n",
    "\n",
    "    def apply_specaugment_mask(self, X, X_len, num_masks=2):\n",
    "        \"\"\"\n",
    "        Fully vectorized SpecAugment-style time masking (no loops).\n",
    "        Args:\n",
    "            X: (B, P, D)\n",
    "            X_len: (B,) in timepoints\n",
    "        Returns:\n",
    "            X_masked: (B, P, D), mask: (B, P)\n",
    "        \"\"\"\n",
    "        B, P, D = X.shape\n",
    "        device = X.device\n",
    "\n",
    "        valid_lens = (X_len // self.patch_height).to(device)\n",
    "        max_mask_lens = (self.max_mask_pct * valid_lens).clamp(min=1).long()\n",
    "\n",
    "        B_rep = B * num_masks\n",
    "        valid_lens_rep = valid_lens.repeat_interleave(num_masks)\n",
    "        max_mask_lens_rep = max_mask_lens.repeat_interleave(num_masks)\n",
    "\n",
    "        t = (torch.rand(B_rep, device=device) * max_mask_lens_rep.float()).floor().long() + 1\n",
    "        max_start = (valid_lens_rep - t + 1).clamp(min=1)\n",
    "        t0 = (torch.rand(B_rep, device=device) * max_start.float()).floor().long()\n",
    "\n",
    "        # Build flattened mask indices\n",
    "        arange = torch.arange(P, device=device).unsqueeze(0)  # (1, P)\n",
    "        t0_exp = t0.unsqueeze(1)                              # (B_rep, 1)\n",
    "        t1_exp = (t0 + t).unsqueeze(1)                        # (B_rep, 1)\n",
    "        mask_chunks = (arange >= t0_exp) & (arange < t1_exp)  # (B_rep, P)\n",
    "\n",
    "        # Now gather the flat indices to write into (B, P)\n",
    "        batch_idx = torch.arange(B, device=device).repeat_interleave(num_masks)  # (B_rep,)\n",
    "        patch_idx = mask_chunks.nonzero(as_tuple=False)  # (N_masked, 2): [mask_row, patch_col]\n",
    "        b_indices = batch_idx[patch_idx[:, 0]]           # Map B_rep index â†’ actual batch index\n",
    "        p_indices = patch_idx[:, 1]                      # patch index\n",
    "\n",
    "        # Set those positions to True in the full mask\n",
    "        mask = torch.zeros(B, P, dtype=torch.bool, device=device)\n",
    "        mask[b_indices, p_indices] = True\n",
    "\n",
    "        # Apply mask token\n",
    "        X_masked = X.clone()\n",
    "        X_masked[mask] = self.mask_token\n",
    "\n",
    "        return X_masked, mask\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Create dummy input\n",
    "B, P, D = 2, 12, 4  # batch, patches, dim\n",
    "X = torch.randn(B, P, D)\n",
    "X_len = torch.tensor([12, 10])  # valid lengths (in timepoints)\n",
    "\n",
    "# Define mask token and module\n",
    "mask_token = torch.tensor([-99.0] * D)\n",
    "specaug = DummySpecAug(mask_token=mask_token, patch_height=1, max_mask_pct=0.3)\n",
    "\n",
    "# Apply\n",
    "X_masked, mask = specaug.apply_specaugment_mask(X, X_len, num_masks=2)\n",
    "\n",
    "# --------------------------\n",
    "# Inspect the result\n",
    "for b in range(B):\n",
    "    print(f\"\\nSample {b}:\")\n",
    "    print(\"Original:\")\n",
    "    print(X[b])\n",
    "    print(\"Masked:\")\n",
    "    print(X_masked[b])\n",
    "    print(\"Mask positions:\", mask[b].nonzero(as_tuple=True)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
