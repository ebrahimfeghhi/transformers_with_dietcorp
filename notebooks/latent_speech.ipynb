{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from g2p_en import G2p\n",
    "g2p = G2p()\n",
    "\n",
    "PHONE_DEF = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "    'AY', 'B',  'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "PHONE_DEF_SIL = PHONE_DEF + ['SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF_SIL.index(p)\n",
    "\n",
    "def convert_to_phonemes(transcript):\n",
    "    \n",
    "    thisTranscription = transcript.strip()\n",
    "    thisTranscription = re.sub(r'[^a-zA-Z\\- \\']', '', thisTranscription)\n",
    "    thisTranscription = thisTranscription.replace('--', '').lower()\n",
    "    addInterWordSymbol = True\n",
    "\n",
    "    phonemes = []\n",
    "    \n",
    "    for p in g2p(thisTranscription):\n",
    "        if addInterWordSymbol and p==' ':\n",
    "            phonemes.append('SIL')\n",
    "        p = re.sub(r'[0-9]', '', p)  # Remove stress\n",
    "        if re.match(r'[A-Z]+', p):  # Only keep phonemes\n",
    "            phonemes.append(p)\n",
    "\n",
    "    #add one SIL symbol at the end so there's one at the end of each word\n",
    "    if addInterWordSymbol:\n",
    "        phonemes.append('SIL')\n",
    "        \n",
    "    seqLen = len(phonemes)\n",
    "    maxSeqLen = 500\n",
    "    seqClassIDs = np.zeros([maxSeqLen]).astype(np.int32)\n",
    "    seqClassIDs[0:seqLen] = [phoneToId(p) + 1 for p in phonemes]\n",
    "    return seqClassIDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:speechbrain.lobes.models.huggingface_transformers.huggingface:speechbrain.lobes.models.huggingface_transformers.huggingface - HubertModel is frozen.\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.lobes.models.huggingface_transformers.hubert import HuBERT\n",
    "\n",
    "hubert_path = \"facebook/hubert-large-ls960-ft\"\n",
    "model_hubert = HuBERT(hubert_path, save_path=\"/data/LLMs/\", freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sample in the dataset\n",
    "\n",
    "librispeech_path = \"/data/LLMs/librispeech/\"\n",
    "\n",
    "dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "    root=librispeech_path,  # or the full path to the parent directory\n",
    "    url=\"train-clean-100\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "# Unpack the returned tuple\n",
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = sample\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Custom collate function with padding\n",
    "def collate_fn(batch):\n",
    "    waveforms = [item[0].squeeze(0) for item in batch]  # Remove channel dimension\n",
    "    lengths = torch.tensor([wav.shape[0] for wav in waveforms])\n",
    "    \n",
    "    # Pad sequences to match longest in batch\n",
    "    padded_waveforms = torch.nn.utils.rnn.pad_sequence(\n",
    "        waveforms, \n",
    "        batch_first=True\n",
    "    )\n",
    "    \n",
    "    transcripts = [convert_to_phonemes(item[2]) for item in batch]  # Extract transcripts\n",
    "    \n",
    "    \n",
    "    return padded_waveforms, lengths, transcripts\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "for waveforms, lengths, transcripts in dataloader:\n",
    "    # HuBERT expects float32 inputs in range [-1, 1]\n",
    "    waveforms = waveforms.float()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        embeddings = model_hubert(waveforms)\n",
    "    \n",
    "    print(f\"Input shape: {waveforms.shape}\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_hubert = model_hubert(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.models import Conformer\n",
    "\n",
    "class LightweightHuBERTConformer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, num_heads=4, ffn_dim=512, num_layers=6, depthwise_conv_kernel_size=31, \n",
    "                 dropout=0.1, use_group_norm=False, convolution_first):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Core Conformer Encoder\n",
    "        self.conformer = Conformer(\n",
    "            input_dim=1024,        # HuBERT feature dimension\n",
    "            num_heads=4,           # Reduced from typical 8-16 heads\n",
    "            ffn_dim=512,           # Balanced capacity vs size (original: 2048)\n",
    "            num_layers=6,          # 6 layers vs typical 12-16\n",
    "            depthwise_conv_kernel_size=31,\n",
    "            dropout=0.1,\n",
    "            use_group_norm=True,   # Better for small batches\n",
    "            convolution_first=True # Better convergence per [2][4]\n",
    "        )\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "\n",
    "    def forward(self, features, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, T, 1024) HuBERT features\n",
    "            lengths: (B,) sequence lengths\n",
    "        Returns:\n",
    "            (B, T, 256) compressed representations\n",
    "        \"\"\"\n",
    "        # Conformer processing\n",
    "        x, _ = self.conformer(features, lengths)  # [B, T, 1024]\n",
    "        \n",
    "        # Projection to target dimension\n",
    "        return self.projection(x)  # [B, T, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(fea_hubert[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightweightHuBERTConformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = model(fea_hubert, torch.tensor([fea_hubert.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# Load the phoneme-based Wav2Vec2 model\n",
    "model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# Path to LibriSpeech dataset\n",
    "librispeech_dir = \"/data/LLMs/librispeech/LibriSpeech/train-clean-100\"\n",
    "\n",
    "# Function to process an audio file and extract phoneme representations\n",
    "def extract_wav2vec_phonemes(audio_path):\n",
    "    # Load and resample audio if needed\n",
    "    speech, sample_rate = sf.read(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(torch.tensor(speech)).numpy()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Get model output (hidden states, not logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state  # (batch_size, time_steps, hidden_size)\n",
    "\n",
    "    return last_hidden_states\n",
    "\n",
    "# Process all .flac files in LibriSpeech directory\n",
    "flac_files = []\n",
    "for root, _, files in os.walk(librispeech_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".flac\"):\n",
    "            flac_files.append(os.path.join(root, file))\n",
    "\n",
    "# Extract phonemes from all files\n",
    "for audio_file in flac_files[:10]:  # Process a subset first\n",
    "    embedding = extract_wav2vec_phonemes(audio_file)\n",
    "    print(f\"File: {audio_file}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
