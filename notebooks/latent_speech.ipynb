{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.81 seconds\n",
      "WAS ASCENDING THE STAIRS LEADING TO DEBRAY'S APARTMENTS\n"
     ]
    }
   ],
   "source": [
    "librispeech_path = \"/data/LLMs/librispeech/\"\n",
    "\n",
    "dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "    root=librispeech_path,  # or the full path to the parent directory\n",
    "    url=\"train-clean-100\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[111]\n",
    "\n",
    "def compute_waveform_length(waveform, sample_rate=16000):\n",
    "    \n",
    "    waveform_len = waveform.shape[1]\n",
    "    print(f\"{waveform_len / sample_rate} seconds\") \n",
    "    \n",
    "compute_waveform_length(waveform)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.lobes.models.huggingface_transformers.hubert import HuBERT\n",
    "\n",
    "hubert_path = \"facebook/hubert-large-ls960-ft\"\n",
    "model_hubert = HuBERT(hubert_path, save_path=\"/data/LLMs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_hubert = model_hubert(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio.models import Conformer\n",
    "\n",
    "class LightweightHuBERTConformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Core Conformer Encoder\n",
    "        self.conformer = Conformer(\n",
    "            input_dim=1024,        # HuBERT feature dimension\n",
    "            num_heads=4,           # Reduced from typical 8-16 heads\n",
    "            ffn_dim=512,           # Balanced capacity vs size (original: 2048)\n",
    "            num_layers=6,          # 6 layers vs typical 12-16\n",
    "            depthwise_conv_kernel_size=31,\n",
    "            dropout=0.1,\n",
    "            use_group_norm=True,   # Better for small batches\n",
    "            convolution_first=True # Better convergence per [2][4]\n",
    "        )\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "\n",
    "    def forward(self, features, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (B, T, 1024) HuBERT features\n",
    "            lengths: (B,) sequence lengths\n",
    "        Returns:\n",
    "            (B, T, 256) compressed representations\n",
    "        \"\"\"\n",
    "        # Conformer processing\n",
    "        x, _ = self.conformer(features, lengths)  # [B, T, 1024]\n",
    "        \n",
    "        # Projection to target dimension\n",
    "        return self.projection(x)  # [B, T, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightweightHuBERTConformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LightweightHuBERTConformer.forward() missing 1 required positional argument: 'lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output_model = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfea_hubert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/speechbrain/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/speechbrain/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: LightweightHuBERTConformer.forward() missing 1 required positional argument: 'lengths'"
     ]
    }
   ],
   "source": [
    "output_model = model(fea_hubert, torch.tensor([fea_hubert.shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Extract phonemes from all files\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m audio_file \u001b[38;5;129;01min\u001b[39;00m flac_files[:\u001b[32m10\u001b[39m]:  \u001b[38;5;66;03m# Process a subset first\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     embedding = \u001b[43mextract_wav2vec_phonemes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mextract_wav2vec_phonemes\u001b[39m\u001b[34m(audio_path)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     27\u001b[39m     outputs = model(**inputs)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     last_hidden_states = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast_hidden_state\u001b[49m  \u001b[38;5;66;03m# (batch_size, time_steps, hidden_size)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m last_hidden_states\n",
      "\u001b[31mAttributeError\u001b[39m: 'CausalLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# Load the phoneme-based Wav2Vec2 model\n",
    "model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# Path to LibriSpeech dataset\n",
    "librispeech_dir = \"/data/LLMs/librispeech/LibriSpeech/train-clean-100\"\n",
    "\n",
    "# Function to process an audio file and extract phoneme representations\n",
    "def extract_wav2vec_phonemes(audio_path):\n",
    "    # Load and resample audio if needed\n",
    "    speech, sample_rate = sf.read(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        speech = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(torch.tensor(speech)).numpy()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Get model output (hidden states, not logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state  # (batch_size, time_steps, hidden_size)\n",
    "\n",
    "    return last_hidden_states\n",
    "\n",
    "# Process all .flac files in LibriSpeech directory\n",
    "flac_files = []\n",
    "for root, _, files in os.walk(librispeech_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".flac\"):\n",
    "            flac_files.append(os.path.join(root, file))\n",
    "\n",
    "# Extract phonemes from all files\n",
    "for audio_file in flac_files[:10]:  # Process a subset first\n",
    "    embedding = extract_wav2vec_phonemes(audio_file)\n",
    "    print(f\"File: {audio_file}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
