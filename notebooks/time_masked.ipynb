{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sorted_indices_by_euclidean_distance_torch(coords: torch.Tensor):\n",
    "    \"\"\"\n",
    "    coords: LongTensor of shape (N,2) with (row, col) pairs.\n",
    "    Returns: dict {i: LongTensor([i, j1, j2, ...])} with all N indices\n",
    "             sorted by distance from coords[i].\n",
    "    \"\"\"\n",
    "    N = coords.shape[0]\n",
    "    dist_dict = {}\n",
    "    # Convert to float for distance computation\n",
    "    coords_f = coords.float()\n",
    "    for i in range(N):\n",
    "        # compute vector differences\n",
    "        diff = coords_f - coords_f[i:i+1]         # shape (N,2)\n",
    "        dists = diff.norm(dim=1)                 # shape (N,)\n",
    "        order = torch.argsort(dists)             # indices sorted by distance\n",
    "        dist_dict[i] = order                     # LongTensor of length N\n",
    "    return dist_dict\n",
    "\n",
    "def build_distance_dict_torch(shape=(8, 8), traversal=None):\n",
    "    \"\"\"\n",
    "    shape: tuple (H, W)\n",
    "    traversal: optional list or tensor of length H*W giving (row, col) coords\n",
    "               in the order your 1D array uses. If None, assumes standard\n",
    "               row-major flatten.\n",
    "    Returns: dict from flat-index -> LongTensor of neighbors sorted by distance.\n",
    "    \"\"\"\n",
    "    H, W = shape\n",
    "    N = H * W\n",
    "\n",
    "    if traversal is None:\n",
    "        # build rowâ€major coords\n",
    "        # coords[i] = (i//W, i%W)\n",
    "        rows = torch.arange(N, dtype=torch.long) // W\n",
    "        cols = torch.arange(N, dtype=torch.long) % W\n",
    "        coords = torch.stack([rows, cols], dim=1)  # (N,2)\n",
    "    else:\n",
    "        coords = torch.tensor(traversal, dtype=torch.long)\n",
    "        assert coords.shape == (N, 2), \"Traversal must be (H*W, 2)\"\n",
    "\n",
    "    return sorted_indices_by_euclidean_distance_torch(coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = build_distance_dict_torch((8,8))\n",
    "savePath = '/home3/skaasyap/willett/outputs/'\n",
    "torch.save(dist_dict, f'{savePath}dist_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def channel_specaugment_masks(\n",
    "    x,            # tensor [B, T, D]\n",
    "    num_masks, max_channels_to_mask,\n",
    "    dist_dict,\n",
    "    num_channels=64,\n",
    "    features_per_channel=2\n",
    "):\n",
    "    B, T, D = x.shape\n",
    "    device = x.device\n",
    "    masks = torch.zeros(B, D, dtype=torch.bool, device=device)\n",
    "\n",
    "    # build a [B, num_channels] of uniform weights\n",
    "    weights = torch.ones(B, num_channels, device=device)\n",
    "\n",
    "    # now sample *per-row*:\n",
    "    # starts1: [B, N], starts2: [B, M]\n",
    "    starts1 = torch.multinomial(weights, num_masks, replacement=False)\n",
    "    starts2 = torch.multinomial(weights, num_masks, replacement=False)\n",
    "    \n",
    "    # widths per mask, per sample\n",
    "    widths1 = torch.randint(0, max_channels_to_mask+1, (B, num_masks), device=device)\n",
    "    widths2 = torch.randint(0, max_channels_to_mask+1, (B, num_masks), device=device)\n",
    "    \n",
    "    # precompute feature-block offsets\n",
    "    off1 = [feat * num_channels for feat in range(features_per_channel)]\n",
    "    off2 = [features_per_channel * num_channels + feat * num_channels\n",
    "            for feat in range(features_per_channel)]\n",
    "    \n",
    "\n",
    "    for b in range(B):\n",
    "        # electrode 1\n",
    "        for start_ch, w in zip(starts1[b], widths1[b]):\n",
    "            w = int(w)\n",
    "            if w == 0: \n",
    "                continue\n",
    "            nearest = dist_dict[int(start_ch.item())][:w]\n",
    "            idxs = torch.tensor(nearest, dtype=torch.long, device=device)\n",
    "            for base in off1:\n",
    "                masks[b, base + idxs] = True\n",
    "\n",
    "        # electrode 2\n",
    "        for start_ch, w in zip(starts2[b], widths2[b]):\n",
    "            w = int(w)\n",
    "            if w == 0:\n",
    "                continue\n",
    "            nearest = dist_dict[int(start_ch.item())][:w]\n",
    "            idxs = torch.tensor(nearest, dtype=torch.long, device=device)\n",
    "            for base in off2:\n",
    "                masks[b, base + idxs] = True\n",
    "\n",
    "    # broadcast mask over time\n",
    "    masks = masks.unsqueeze(1).expand(-1, T, -1)\n",
    "    X_masked = x.clone()\n",
    "    X_masked[masks] = 0\n",
    "    return X_masked, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1195562/3414989549.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idxs = torch.tensor(nearest, dtype=torch.long, device=device)\n",
      "/tmp/ipykernel_1195562/3414989549.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  idxs = torch.tensor(nearest, dtype=torch.long, device=device)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "B, T, D = 5, 10, 256\n",
    "x = torch.randn(B, T, D)\n",
    "\n",
    "X_masked, masks = channel_specaugment_masks(x, 20, 5, dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_masked[0, :, 2+64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False,  True, False, False, False,  True, False,\n",
       "        False, False,  True, False, False, False,  True, False,  True,  True,\n",
       "         True, False, False,  True,  True,  True, False,  True,  True, False,\n",
       "         True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False,  True, False, False,  True, False, False, False,\n",
       "        False,  True, False, False,  True,  True, False, False,  True,  True,\n",
       "         True,  True,  True,  True, False, False,  True, False,  True, False,\n",
       "        False, False,  True, False, False, False,  True, False, False, False,\n",
       "         True, False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "        False,  True,  True, False,  True,  True, False, False,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False,  True, False, False,\n",
       "         True, False, False, False, False,  True, False, False,  True,  True,\n",
       "        False, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "        False, False, False,  True, False, False,  True, False,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False, False, False,  True,  True, False,\n",
       "         True, False, False, False,  True, False,  True,  True,  True,  True,\n",
       "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "        False,  True, False,  True, False, False, False,  True, False, False,\n",
       "         True, False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False,  True,  True, False,  True, False, False, False,  True, False,\n",
       "         True,  True,  True,  True, False, False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 100, 256])\n",
      "Masked output shape: torch.Size([1, 100, 256])\n",
      "Mask shape: torch.Size([1, 100])\n",
      "Masked indices: [tensor([28, 29, 30, 31, 32, 33, 34])]\n",
      "Mask sum: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/ebrahim2/miniconda3/envs/speech-bci/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TimeMasker:\n",
    "    def __init__(self, max_mask_pct=0.5, num_masks=1):\n",
    "        self.max_mask_pct = max_mask_pct\n",
    "        self.num_masks = num_masks\n",
    "\n",
    "    def apply_time_masking(self, X, X_len):\n",
    "        \"\"\"\n",
    "        Fully vectorized time masking (no loops at all).\n",
    "\n",
    "        Args:\n",
    "            X: (B, P, D) input tensor\n",
    "            X_len: (B,) valid lengths in timepoints\n",
    "\n",
    "        Returns:\n",
    "            X_masked: (B, P, D) with masked patches\n",
    "            mask: (B, P) boolean mask of where values were masked\n",
    "            masked_indices: list of 1D LongTensors, each with indices of masked patches per batch\n",
    "            unmasked_indices: list of 1D LongTensors, each with indices of unmasked patches per batch\n",
    "        \"\"\"\n",
    "        B, P, D = X.shape\n",
    "        device = X.device\n",
    "\n",
    "        valid_lens = X_len\n",
    "        max_mask_lens = (self.max_mask_pct * valid_lens).long()  # (B,)\n",
    "\n",
    "        B_rep = B * self.num_masks\n",
    "        valid_lens_rep = valid_lens.repeat_interleave(self.num_masks)           \n",
    "        max_mask_lens_rep = max_mask_lens.repeat_interleave(self.num_masks)\n",
    "\n",
    "        t = (torch.rand(B_rep, device=device) * (max_mask_lens_rep + 1).float()).floor().long()\n",
    "        max_start = (valid_lens_rep - t + 1).clamp(min=1)\n",
    "        t0 = (torch.rand(B_rep, device=device) * max_start.float()).floor().long()\n",
    "\n",
    "        arange = torch.arange(P, device=device).unsqueeze(0)         \n",
    "        t0_exp = t0.unsqueeze(1)                                     \n",
    "        t1_exp = (t0 + t).unsqueeze(1)                               \n",
    "        mask_chunks = (arange >= t0_exp) & (arange < t1_exp)        \n",
    "\n",
    "        batch_idx = torch.arange(B, device=device).repeat_interleave(self.num_masks)\n",
    "        patch_idx = mask_chunks.nonzero(as_tuple=False)\n",
    "        b_indices = batch_idx[patch_idx[:, 0]]\n",
    "        p_indices = patch_idx[:, 1]\n",
    "\n",
    "        mask = torch.zeros(B, P, dtype=torch.bool, device=device)\n",
    "        mask[b_indices, p_indices] = True\n",
    "\n",
    "        X_masked = X.clone()\n",
    "        X_masked[mask] = 0\n",
    "\n",
    "        # Get masked/unmasked indices per batch\n",
    "        masked_indices = [mask[b].nonzero(as_tuple=True)[0] for b in range(B)]\n",
    "        unmasked_indices = [~mask[b].nonzero(as_tuple=True)[0] for b in range(B)]\n",
    "\n",
    "        return X_masked, mask, masked_indices, unmasked_indices\n",
    "\n",
    "# Test setup\n",
    "B, T, F = 1, 100, 256\n",
    "X = torch.randn(B, T, F)\n",
    "X_len = torch.tensor([50])  # Only first 50 are \"valid\"\n",
    "\n",
    "masker = TimeMasker(max_mask_pct=0.5, num_masks=1)\n",
    "X_masked, mask, masked_idx, unmasked_idx = masker.apply_time_masking(X, X_len)\n",
    "\n",
    "# Print some quick diagnostics\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Masked output shape:\", X_masked.shape)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"Masked indices:\", masked_idx)\n",
    "print(\"Mask sum:\", mask.sum().item())  # Should be <= 25 since max_mask_pct is 0.5 of X_len (50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
