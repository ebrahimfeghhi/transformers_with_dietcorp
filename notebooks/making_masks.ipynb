{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_temporal_mask(seq_len, look_ahead=0, device=None):\n",
    "    \"\"\"\n",
    "    Create a temporal attention mask where each position i can attend to:\n",
    "    positions [0, ..., i + look_ahead], up to seq_len - 1\n",
    "\n",
    "    Returns: (1, 1, seq_len, seq_len) boolean tensor\n",
    "    \"\"\"\n",
    "    i = torch.arange(seq_len, device=device).unsqueeze(1)  # query positions\n",
    "    j = torch.arange(seq_len, device=device).unsqueeze(0)  # key positions\n",
    "\n",
    "    mask = j <= i + look_ahead  # allow attention to past and limited future\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # shape: (1, 1, seq_len, seq_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_temporal_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000, patches_per_time=16):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.repeat_interleave(torch.arange(max_len), repeats=patches_per_time).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        print(div_term)\n",
    "        pe = torch.zeros(max_len*patches_per_time, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]],\n",
       "\n",
       "        [[0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(8)\n",
    "pe.pe[16:32, :, 0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 64]) torch.Size([16, 64]) torch.Size([16, 64])\n",
      "torch.Size([8, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LearnablePositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, N, M, embedding_dim):\n",
    "        super(LearnablePositionalEmbeddings, self).__init__()\n",
    "        \n",
    "        # Set the size of the embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "\n",
    "        # Initialize the learnable positional embeddings\n",
    "        self.embeddings_N = nn.Parameter(torch.randn(N, embedding_dim))  # Embeddings for every Nth patch\n",
    "        self.embeddings_M = nn.Parameter(torch.randn(M, embedding_dim))  # Embeddings for every M consecutive patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_patches, embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, _ = x.size()\n",
    "        \n",
    "        pos_N_embedding = self.embeddings_N.tile(dims=(num_patches // N, 1))\n",
    "        pos_M_embedding = self.embeddings_M.repeat_interleave(num_patches // M, dim=0)\n",
    "        \n",
    "        \n",
    "        # Add the positional embeddings to the input tensor (x)\n",
    "        x = x + pos_N_embedding + pos_M_embedding\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "num_patches = 16   # Number of patches (this would be the number of spatial patches or time steps)\n",
    "embedding_dim = 64  # Embedding dimension size\n",
    "N = 4               # Nth patch interval for first set of embeddings\n",
    "M = 2               # M consecutive patches for second set of embeddings\n",
    "\n",
    "# Initialize the model\n",
    "model = LearnablePositionalEmbeddings(N, M, embedding_dim)\n",
    "\n",
    "# Example input tensor of shape (batch_size, num_patches, embedding_dim)\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, num_patches, embedding_dim)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(output.shape)  # Should print (batch_size, num_patches, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class HybridSpatiotemporalPosEmb(nn.Module):\n",
    "    def __init__(self, num_space, max_time, embedding_dim):\n",
    "        \"\"\"\n",
    "        num_space: number of spatial positions (N)\n",
    "        max_time: number of time steps (T)\n",
    "        embedding_dim: size of each positional embedding (must be even)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.N = num_space\n",
    "        self.T = max_time\n",
    "\n",
    "        assert embedding_dim % 2 == 0, \"Embedding dimension must be even for sin/cos\"\n",
    "\n",
    "        # Learnable spatial embeddings\n",
    "        self.space_embedding = nn.Parameter(torch.randn(num_space, embedding_dim))\n",
    "\n",
    "        # Fixed sinusoidal temporal embeddings\n",
    "        self.register_buffer(\"time_embedding\", self._build_sin_cos_embedding(max_time, embedding_dim))\n",
    "\n",
    "    def _build_sin_cos_embedding(self, length, dim):\n",
    "        \"\"\"\n",
    "        Generate fixed sinusoidal embeddings of shape (length, dim)\n",
    "        \"\"\"\n",
    "        position = torch.arange(1, length + 1).unsqueeze(1).float()  # (length, 1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))  # (dim/2,)\n",
    "        sinusoid = torch.zeros(length, dim)\n",
    "        sinusoid[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoid[:, 1::2] = torch.cos(position * div_term)\n",
    "        return sinusoid  # (length, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_patches, embedding_dim)\n",
    "        Assumes patches are ordered as:\n",
    "            [t0_p0, t0_p1, ..., t0_pN-1, t1_p0, ..., tT-1_pN-1]\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, _ = x.size()\n",
    "        T = num_patches // self.N\n",
    "\n",
    "        # Compute spatial and temporal indices\n",
    "        spatial_idx = torch.arange(num_patches, device=x.device) % self.N\n",
    "        temporal_idx = torch.arange(num_patches, device=x.device) // self.N\n",
    "\n",
    "        # Lookup embeddings\n",
    "        pos_space_embedding = self.space_embedding[spatial_idx]     # (num_patches, embedding_dim)\n",
    "        pos_time_embedding = self.time_embedding[temporal_idx]      # (num_patches, embedding_dim)\n",
    "\n",
    "        # Combine and expand to batch\n",
    "        pos_embedding = pos_space_embedding + pos_time_embedding\n",
    "        pos_embedding = pos_embedding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        return x + pos_embedding, pos_space_embedding, pos_time_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,20,6))\n",
    "learnable_embeds = HybridSpatiotemporalPosEmb(num_space=5, max_time=20, embedding_dim=6)\n",
    "x2, space_embeds, time_embeds = learnable_embeds(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_temporal_attention_mask(num_patches, patches_per_timestep=4, N=2):\n",
    "    \n",
    "    mask = torch.full((num_patches, num_patches), 0)\n",
    "\n",
    "    timesteps = num_patches // patches_per_timestep\n",
    "    \n",
    "    for t_q in range(timesteps):  # time index of query\n",
    "        for dt in range(N + 1):  # how far back to look\n",
    "            t_k = t_q - dt  # key timestep\n",
    "            if t_k < 0:\n",
    "                continue\n",
    "            q_start = t_q * patches_per_timestep\n",
    "            q_end = (t_q + 1) * patches_per_timestep\n",
    "            k_start = t_k * patches_per_timestep\n",
    "            k_end = (t_k + 1) * patches_per_timestep\n",
    "            # allow attention: set to 0 (non-masked)\n",
    "            mask[q_start:q_end, k_start:k_end] = 1\n",
    "\n",
    "    return mask  # shape: [Num Patches, Num Patches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_temporal_attention_mask(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
