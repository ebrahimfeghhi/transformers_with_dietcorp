{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4316910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/willett_data/competitionData/ /data/willett_data/ptDecoder_ctc_both\n"
     ]
    }
   ],
   "source": [
    "server = 'obi'\n",
    "logBoth = True\n",
    "\n",
    "if server == 'obi':\n",
    "    dataDir = '/data/willett_data/competitionData/'\n",
    "    if logBoth:\n",
    "        dataSave = '/data/willett_data/ptDecoder_ctc_both'\n",
    "    else:\n",
    "        dataSave = '/data/willett_data/ptDecoder_ctc'\n",
    "        \n",
    "elif server == 'leia':\n",
    "    dataDir = '/home3/skaasyap/willett/competitionData/'\n",
    "    if logBoth:\n",
    "        dataSave = '/home3/skaasyap/willett/data_log_both'\n",
    "    else:\n",
    "        dataSave = '/home3/skaasyap/willett/data'\n",
    "\n",
    "print(dataDir, dataSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f89411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home3/ebrahim2/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79569a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if server == 'leia':\n",
    "    \n",
    "    with open('/home3/skaasyap/willett/data', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    #with open('/home3/skaasyap/willett/data_held_out_days', 'rb') as f:\n",
    "    #    data_held_out = pickle.load(f)\n",
    "    with open('/home3/skaasyap/willett/data_log_both', 'rb') as f:\n",
    "        data_logged = pickle.load(f)\n",
    "    with open('/home3/skaasyap/willett/data_log_both_held_out_days', 'rb') as f:\n",
    "        data_logged_held_out = pickle.load(f)\n",
    "        \n",
    "    print(data['train'][0]['sentenceDat'][0].std(), data['train'][0]['sentenceDat'][0].mean())\n",
    "    print(data_logged['train'][0]['sentenceDat'][0].std(), data_logged['train'][0]['sentenceDat'][0].mean())\n",
    "\n",
    "if server == 'obi':\n",
    "    with open('/data/willett_data/ptDecoder_ctc', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    with open('/data/willett_data/ptDecoder_ctc_held_out_days', 'rb') as f:\n",
    "        data_held_out = pickle.load(f)\n",
    "    with open('/data/willett_data/ptDecoder_ctc_both', 'rb') as f:\n",
    "        data_logged = pickle.load(f)\n",
    "    with open('/data/willett_data/ptDecoder_ctc_both_held_out_days', 'rb') as f:\n",
    "        data_logged_held_out = pickle.load(f)\n",
    "        \n",
    "#i = 10\n",
    "#print(data['train'][i]['sentenceDat'][i].std(), data['train'][i]['sentenceDat'][i].mean())\n",
    "#print(data_logged['train'][i]['sentenceDat'][i].std(), data_logged['train'][i]['sentenceDat'][i].mean())\n",
    "#print(data_held_out['train'][i]['sentenceDat'][i].std(), data['train'][i]['sentenceDat'][i].mean())\n",
    "#print(data_logged_held_out['train'][i]['sentenceDat'][i].std(), data_logged['train'][i]['sentenceDat'][i].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionNames = ['t12.2022.04.28',  't12.2022.05.26',  't12.2022.06.21',  't12.2022.07.21',  't12.2022.08.13',\n",
    "'t12.2022.05.05',  't12.2022.06.02',  't12.2022.06.23',  't12.2022.07.27',  't12.2022.08.18',\n",
    "'t12.2022.05.17',  't12.2022.06.07',  't12.2022.06.28',  't12.2022.07.29',  't12.2022.08.23',\n",
    "'t12.2022.05.19',  't12.2022.06.14',  't12.2022.07.05',  't12.2022.08.02',  't12.2022.08.25',\n",
    "'t12.2022.05.24',  't12.2022.06.16',  't12.2022.07.14',  't12.2022.08.11']\n",
    "sessionNames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb2175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from g2p_en import G2p\n",
    "import numpy as np\n",
    "\n",
    "g2p = G2p()\n",
    "PHONE_DEF = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "    'AY', 'B',  'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "PHONE_DEF_SIL = PHONE_DEF + ['SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF_SIL.index(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6305c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_electrode_idxs(feats):\n",
    "    \n",
    "    '''\n",
    "    In the original array, first 128 indices of feats are txcrossings, \n",
    "    second 128 features are spikeband power.\n",
    "    Here what we do is rearrange feats so that:\n",
    "        0-64: tx for inferior\n",
    "        64-128: spikepow for inferior\n",
    "        128-194: tx for superior\n",
    "        194-256: spikepow for superior\n",
    "    '''\n",
    "    \n",
    "    area_6v_superior = np.array([\n",
    "    [62,  51,  43,  35,  94,  87,  79,  78],\n",
    "    [60,  53,  41,  33,  95,  86,  77,  76],\n",
    "    [63,  54,  47,  44,  93,  84,  75,  74],\n",
    "    [58,  55,  48,  40,  92,  85,  73,  72],\n",
    "    [59,  45,  46,  38,  91,  82,  71,  70],\n",
    "    [61,  49,  42,  36,  90,  83,  69,  68],\n",
    "    [56,  52,  39,  34,  89,  81,  67,  66],\n",
    "    [57,  50,  37,  32,  88,  80,  65,  64]\n",
    "    ])\n",
    "    \n",
    "    superior_combined_array = np.empty(area_6v_superior.size * 2, dtype=area_6v_superior.dtype)  \n",
    "    \n",
    "    # first 64 features are tx, second 64 are spikeband power\n",
    "    superior_combined_array[:64] = area_6v_superior.ravel()\n",
    "    superior_combined_array[64:] = area_6v_superior.ravel() + 128   \n",
    "\n",
    "    area_6v_inferior = np.array([\n",
    "        [125, 126, 112, 103,  31,  28,  11,  8],\n",
    "        [123, 124, 110, 102,  29,  26,   9,  5],\n",
    "        [121, 122, 109, 101,  27,  19,  18,  4],\n",
    "        [119, 120, 108, 100,  25,  15,  12,  6],\n",
    "        [117, 118, 107,  99,  23,  13,  10,  3],\n",
    "        [115, 116, 106,  97,  21,  20,   7,  2],\n",
    "        [113, 114, 105,  98,  17,  24,  14,  0],\n",
    "        [127, 111, 104,  96,  30,  22,  16,  1]\n",
    "    ])\n",
    "    \n",
    "    inferior_combined_array = np.empty(area_6v_inferior.size * 2, dtype=area_6v_inferior.dtype)  \n",
    "    inferior_combined_array[:64] = area_6v_inferior.ravel()\n",
    "    inferior_combined_array[64:] = area_6v_inferior.ravel() + 128   \n",
    "    \n",
    "    feats_reshaped = np.zeros_like(feats)\n",
    "    \n",
    "    feats_reshaped[:, 0:128] = feats[:, inferior_combined_array.ravel()]\n",
    "    feats_reshaped[:, 128:] = feats[:, superior_combined_array.ravel()]\n",
    "    \n",
    "    return feats_reshaped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20398fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def loadFeaturesAndNormalize(sessionPath, logBoth):\n",
    "    \n",
    "    dat = scipy.io.loadmat(sessionPath)\n",
    "\n",
    "    input_features = []\n",
    "    transcriptions = []\n",
    "    frame_lens = []\n",
    "    block_means = []\n",
    "    block_stds = []\n",
    "    n_trials = dat['sentenceText'].shape[0]\n",
    "\n",
    "    #collect area 6v tx1 and spikePow features\n",
    "    for i in range(n_trials):    \n",
    "        #get time series of TX and spike power for this trial\n",
    "        #first 128 columns = area 6v only\n",
    "        if logBoth:\n",
    "            tx_crossings = np.log1p(dat['tx1'][0, i][:, 0:128])\n",
    "            log_pow = np.log(dat['spikePow'][0,i][:,0:128])\n",
    "            features = np.concatenate([tx_crossings, log_pow], axis=1)\n",
    "        else:\n",
    "            features = np.concatenate([dat['tx1'][0,i][:,0:128], dat['spikePow'][0,i][:,0:128]], axis=1)\n",
    "\n",
    "        sentence_len = features.shape[0]\n",
    "        sentence = dat['sentenceText'][i].strip()\n",
    "\n",
    "        input_features.append(features)\n",
    "        transcriptions.append(sentence)\n",
    "        frame_lens.append(sentence_len)\n",
    "\n",
    "    #block-wise feature normalization\n",
    "    blockNums = np.squeeze(dat['blockIdx'])\n",
    "    blockList = np.unique(blockNums)\n",
    "    blocks = []\n",
    "    for b in range(len(blockList)):\n",
    "        sentIdx = np.argwhere(blockNums==blockList[b])\n",
    "        sentIdx = sentIdx[:,0].astype(np.int32)\n",
    "        blocks.append(sentIdx)\n",
    "\n",
    "    for b in range(len(blocks)):\n",
    "        feats = reorder_electrode_idxs(np.concatenate(input_features[blocks[b][0]:(blocks[b][-1]+1)], axis=0))\n",
    "        # feats = np.concatenate(input_features[blocks[b][0]:(blocks[b][-1]+1)], axis=0)\n",
    "\n",
    "        feats_mean = np.mean(feats, axis=0, keepdims=True)\n",
    "        feats_std = np.std(feats, axis=0, keepdims=True)\n",
    "        for i in blocks[b]:\n",
    "            input_features[i] = reorder_electrode_idxs(input_features[i])\n",
    "            input_features[i] = (input_features[i] - feats_mean) / (feats_std + 1e-8)\n",
    "\n",
    "    #convert to tfRecord file\n",
    "    session_data = {\n",
    "        'inputFeatures': input_features,\n",
    "        'transcriptions': transcriptions,\n",
    "        'frameLens': frame_lens\n",
    "    }\n",
    "\n",
    "    return session_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf55fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getDataset(fileName, logBoth):\n",
    "    \n",
    "    session_data = loadFeaturesAndNormalize(fileName, logBoth)\n",
    "        \n",
    "    allDat = []\n",
    "    trueSentences = []\n",
    "    seqElements = []\n",
    "    \n",
    "    for x in range(len(session_data['inputFeatures'])):\n",
    "        allDat.append(session_data['inputFeatures'][x])\n",
    "        trueSentences.append(session_data['transcriptions'][x])\n",
    "        \n",
    "        thisTranscription = str(session_data['transcriptions'][x]).strip()\n",
    "        thisTranscription = re.sub(r'[^a-zA-Z\\- \\']', '', thisTranscription)\n",
    "        thisTranscription = thisTranscription.replace('--', '').lower()\n",
    "        addInterWordSymbol = True\n",
    "\n",
    "        phonemes = []\n",
    "        for p in g2p(thisTranscription):\n",
    "            if addInterWordSymbol and p==' ':\n",
    "                phonemes.append('SIL')\n",
    "            p = re.sub(r'[0-9]', '', p)  # Remove stress\n",
    "            if re.match(r'[A-Z]+', p):  # Only keep phonemes\n",
    "                phonemes.append(p)\n",
    "\n",
    "        #add one SIL symbol at the end so there's one at the end of each word\n",
    "        if addInterWordSymbol:\n",
    "            phonemes.append('SIL')\n",
    "\n",
    "        seqLen = len(phonemes)\n",
    "        maxSeqLen = 500\n",
    "        seqClassIDs = np.zeros([maxSeqLen]).astype(np.int32)\n",
    "        seqClassIDs[0:seqLen] = [phoneToId(p) + 1 for p in phonemes]\n",
    "        seqElements.append(seqClassIDs)\n",
    "\n",
    "    newDataset = {}\n",
    "    newDataset['sentenceDat'] = allDat\n",
    "    newDataset['transcriptions'] = trueSentences\n",
    "    newDataset['phonemes'] = seqElements\n",
    "    \n",
    "    timeSeriesLens = []\n",
    "    phoneLens = []\n",
    "    for x in range(len(newDataset['sentenceDat'])):\n",
    "        timeSeriesLens.append(newDataset['sentenceDat'][x].shape[0])\n",
    "        \n",
    "        zeroIdx = np.argwhere(newDataset['phonemes'][x]==0)\n",
    "        phoneLens.append(zeroIdx[0,0])\n",
    "    \n",
    "    newDataset['timeSeriesLens'] = np.array(timeSeriesLens)\n",
    "    newDataset['phoneLens'] = np.array(phoneLens)\n",
    "    newDataset['phonePerTime'] = newDataset['phoneLens'].astype(np.float32) / newDataset['timeSeriesLens'].astype(np.float32)\n",
    "    return newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815eca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDatasets = []\n",
    "testDatasets = []\n",
    "competitionDatasets = []\n",
    "\n",
    "\n",
    "for dayIdx in range(len(sessionNames)):\n",
    "    trainDataset = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "    testDataset = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "\n",
    "    trainDatasets.append(trainDataset)\n",
    "    testDatasets.append(testDataset)\n",
    "\n",
    "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
    "        dataset = getDataset(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        competitionDatasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "allDatasets = {}\n",
    "allDatasets['train'] = trainDatasets\n",
    "allDatasets['test'] = testDatasets\n",
    "allDatasets['competition'] = competitionDatasets\n",
    "\n",
    "with open(dataSave, 'wb') as handle:\n",
    "    pickle.dump(allDatasets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03723b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "data = trainDatasets[0]['sentenceDat'][10].T\n",
    "np.save('figure1_neural_data', data)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(trainDatasets[0]['sentenceDat'][10].T[:128, 0:200], clim=[-1, 1], alpha=0.4, cmap='bone')\n",
    "plt.xlabel(\"Time (s)\", fontsize=16)\n",
    "plt.ylabel(\"Features\", fontsize=16)\n",
    "plt.tick_params(axis='x', length=0)\n",
    "plt.xticks([0, 200], [0, 4])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "# Transparent rectangle\n",
    "rect = patches.Rectangle((130, -5), 32, 138,\n",
    "                         facecolor='none', edgecolor='black', linewidth=2, clip_on=False)\n",
    "plt.gca().add_patch(rect)\n",
    "\n",
    "\n",
    "# Adjust y-limits to show full rectangle\n",
    "plt.ylim(-10, 135)\n",
    "\n",
    "# Despine all axes\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.imshow(trainDatasets[0]['sentenceDat'][10].T[:128, 0:200], clim=[-1, 1], alpha=0.4, cmap='bone')\n",
    "plt.xlabel(\"Time (s)\", fontsize=16)\n",
    "plt.ylabel(\"\")\n",
    "plt.tick_params(axis='x', length=0)\n",
    "plt.xticks([0, 200], [0, 4])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "# Transparent rectangle\n",
    "rect = patches.Rectangle((134, -5), 32, 138,\n",
    "                         facecolor='none', edgecolor='black', linewidth=2, clip_on=False)\n",
    "plt.gca().add_patch(rect)\n",
    "\n",
    "\n",
    "# Adjust y-limits to show full rectangle\n",
    "plt.ylim(-10, 135)\n",
    "\n",
    "# Despine all axes\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b00cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "data = trainDatasets[0]['sentenceDat'][10].T[:64, :260]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.imshow(data, vmin=-1, vmax=1, origin='lower', alpha=0.6)\n",
    "ax.set_yticks([])\n",
    "centers = [60, 130, 200]\n",
    "rect_width = 20\n",
    "rect_height = data.shape[0]  # full height of the electrode axis\n",
    "\n",
    "# Add rectangles\n",
    "y_start = -0.5  # start slightly below the image\n",
    "rect_height = data.shape[0] + 1  # extend slightly beyond top\n",
    "\n",
    "for x_center in centers:\n",
    "    rect = patches.Rectangle((x_center - rect_width / 2, y_start), rect_width, rect_height,\n",
    "                             linewidth=0, facecolor='white', alpha=1.0)\n",
    "    ax.add_patch(rect)\n",
    "sns.despine(trim=True, top=True, right=True, left=True, bottom=True)\n",
    "\n",
    "# Set x-ticks at the edges of each patch\n",
    "ax.set_xticks([])\n",
    "tick_positions = [0, 50, 70, 120, 140, 190, 210, 260]\n",
    "tick_positions = [2, 48, 72, 118, 142, 188, 212, 258]\n",
    "ax.set_xticks(tick_positions)\n",
    "tick_labels = np.array([0, 50, 50, 100, 100, 150, 150, 200])/10 # 4 labels for the 4 patches\n",
    "ax.set_xticklabels([str(int(lbl)) for lbl in tick_labels])\n",
    "ax.tick_params(axis='x', length=0)  # Set tick mark length to 0 on x-axis\n",
    "ax.tick_params(axis='x', labelsize=12)  \n",
    "\n",
    "ax.set_xlabel(\"Timesteps\", fontsize=16)\n",
    "\n",
    "plt.ylim(-0.5, data.shape[0] + 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.imshow(data, vmin=-1, vmax=1, origin='lower', alpha=0.6)\n",
    "ax.set_yticks([])\n",
    "centers = [60, 130, 200]\n",
    "rect_width = 20\n",
    "rect_height = data.shape[0]  # full height of the electrode axis\n",
    "\n",
    "# Add rectangles\n",
    "y_start = -0.5  # start slightly below the image\n",
    "rect_height = data.shape[0] + 1  # extend slightly beyond top\n",
    "\n",
    "for x_center in centers:\n",
    "    rect = patches.Rectangle((x_center - rect_width / 2, y_start), rect_width, rect_height,\n",
    "                             linewidth=0, facecolor='white', alpha=1.0)\n",
    "    ax.add_patch(rect)\n",
    "sns.despine(trim=True, top=True, right=True, left=True, bottom=True)\n",
    "\n",
    "# Set x-ticks at the edges of each patch\n",
    "ax.set_xticks([])\n",
    "tick_positions = [0, 50, 70, 120, 140, 190, 210, 260]\n",
    "tick_positions = [2, 48, 72, 118, 142, 188, 212, 258]\n",
    "ax.set_xticks(tick_positions)\n",
    "tick_labels = [0, 32, 4, 36, 8, 40, 12, 44] # 4 labels for the 4 patches\n",
    "ax.set_xticklabels([str(int(lbl)) for lbl in tick_labels])\n",
    "ax.tick_params(axis='x', length=0)  # Set tick mark length to 0 on x-axis\n",
    "ax.tick_params(axis='x', labelsize=12)  \n",
    "\n",
    "ax.set_xlabel(\"Timesteps\", fontsize=16)\n",
    "\n",
    "plt.ylim(-0.5, data.shape[0] + 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc86d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(dict1, dict2):\n",
    "    \n",
    "    combined_dict = {}\n",
    "    for key in dict1.keys():\n",
    "        if key in ['sentenceDat', 'transcriptions', 'phonemes']: \n",
    "            combined_dict[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            combined_dict[key] = np.concatenate([dict1[key], dict2[key]])\n",
    "        \n",
    "    return combined_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e25c8e4",
   "metadata": {},
   "source": [
    "# Held out days testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c7d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20]\n",
      "t12.2022.05.24\n",
      "t12.2022.05.26\n",
      "t12.2022.06.02\n",
      "t12.2022.06.07\n",
      "t12.2022.06.14\n",
      "t12.2022.06.16\n",
      "t12.2022.06.21\n",
      "t12.2022.06.28\n",
      "t12.2022.07.05\n",
      "t12.2022.07.14\n",
      "t12.2022.07.21\n",
      "t12.2022.07.27\n",
      "t12.2022.08.02\n",
      "t12.2022.08.11\n",
      "t12.2022.08.13\n"
     ]
    }
   ],
   "source": [
    "competitionDays = []\n",
    "for dayIdx in range(len(sessionNames)):\n",
    "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
    "        competitionDays.append(dayIdx)\n",
    "print(competitionDays)\n",
    "\n",
    "for cd in competitionDays:\n",
    "    print(sessionNames[cd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adb4351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t12.2022.04.28',\n",
       " 't12.2022.05.05',\n",
       " 't12.2022.05.17',\n",
       " 't12.2022.05.19',\n",
       " 't12.2022.05.24',\n",
       " 't12.2022.05.26',\n",
       " 't12.2022.06.02',\n",
       " 't12.2022.06.07',\n",
       " 't12.2022.06.14',\n",
       " 't12.2022.06.16',\n",
       " 't12.2022.06.21',\n",
       " 't12.2022.06.23',\n",
       " 't12.2022.06.28',\n",
       " 't12.2022.07.05',\n",
       " 't12.2022.07.14',\n",
       " 't12.2022.07.21',\n",
       " 't12.2022.07.27',\n",
       " 't12.2022.07.29',\n",
       " 't12.2022.08.02',\n",
       " 't12.2022.08.11',\n",
       " 't12.2022.08.13',\n",
       " 't12.2022.08.18',\n",
       " 't12.2022.08.23',\n",
       " 't12.2022.08.25']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessionNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cb63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN t12.2022.04.28\n",
      "TRAIN t12.2022.05.05\n",
      "TRAIN t12.2022.05.17\n",
      "TRAIN t12.2022.05.19\n",
      "EVAL t12.2022.05.24\n",
      "EVAL t12.2022.05.26\n",
      "EVAL t12.2022.06.02\n",
      "EVAL t12.2022.06.07\n",
      "EVAL t12.2022.06.14\n",
      "TRAIN t12.2022.08.18\n",
      "TRAIN t12.2022.08.23\n",
      "TRAIN t12.2022.08.25\n"
     ]
    }
   ],
   "source": [
    "trainDatasets = []\n",
    "testDatasets = []\n",
    "competitionDatasets = []\n",
    "\n",
    "held_out_run_set = 2\n",
    "\n",
    "if held_out_run_set == 0:\n",
    "\n",
    "    evaluation_days = ['07.21', '07.27', '07.29', '08.02', '08.11', '08.13'] \n",
    "    loop_until = -3\n",
    "    \n",
    "elif held_out_run_set == 1:\n",
    "    \n",
    "    evaluation_days = ['06.16', '06.21', '06.28', '07.05', '07.14']\n",
    "    loop_until = -9\n",
    "    \n",
    "elif held_out_run_set == 2:\n",
    "    \n",
    "    evaluation_days = ['05.24', '05.26', '06.02', '06.07', '06.14']\n",
    "    loop_until = -15\n",
    "    extra_training_days = [-3, -2, -1] # add silent speech days\n",
    "\n",
    "\n",
    "# skipping the last 3 days because they are not historical data. \n",
    "for dayIdx in range(len(sessionNames[:loop_until])):\n",
    "    \n",
    "    if sessionNames[dayIdx][-5:] in evaluation_days:\n",
    "\n",
    "        print(\"EVAL\", sessionNames[dayIdx])\n",
    "        \n",
    "        #print(f\"Placing {sessionNames[dayIdx]} in test/comp\")\n",
    "        \n",
    "        # put both train and test data for held-out days into \"test (really val)\".\n",
    "        testDataset1 = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        testDataset2 = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        testDataset = combine_dicts(testDataset1, testDataset2)\n",
    "        testDatasets.append(testDataset)\n",
    "        \n",
    "        # only testing on these held-out days now. \n",
    "        if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
    "            dataset = getDataset(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "            competitionDatasets.append(dataset)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"TRAIN\", sessionNames[dayIdx])\n",
    "        \n",
    "        # put both train and test data for non held-out days into train\n",
    "        trainDataset1 = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        trainDataset2 = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        trainDataset = combine_dicts(trainDataset1, trainDataset2)\n",
    "        trainDatasets.append(trainDataset)\n",
    "        \n",
    "if held_out_run_set == 2:\n",
    "    \n",
    "    for dayIdx in extra_training_days:\n",
    "        print(\"TRAIN\", sessionNames[dayIdx])\n",
    "        \n",
    "        trainDataset1 = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        trainDataset2 = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat', logBoth)\n",
    "        trainDataset = combine_dicts(trainDataset1, trainDataset2)\n",
    "        trainDatasets.append(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2da5be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max day held out 0  14\n",
      "max day held out 1  9\n",
      "max day held out 2  3\n"
     ]
    }
   ],
   "source": [
    "print(\"max day held out 0 \", 24 - 3 - 6 - 1)\n",
    "print(\"max day held out 1 \", 24 -9 - 5 - 1)\n",
    "print(\"max day held out 2 \", 24 - 15 - 5 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af6595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "allDatasets = {}\n",
    "allDatasets['train'] = trainDatasets\n",
    "allDatasets['test'] = testDatasets\n",
    "allDatasets['competition'] = competitionDatasets\n",
    "\n",
    "print(held_out_run_set)\n",
    "print(dataSave)\n",
    "if held_out_run_set > 0:\n",
    "    dataSave_held_out_days = f\"{dataSave}_held_out_days_{held_out_run_set}_sil\"\n",
    "else:\n",
    "    dataSave_held_out_days = f\"{dataSave}_held_out_days\"\n",
    "    \n",
    "with open(dataSave_held_out_days, 'wb') as handle:\n",
    "    pickle.dump(allDatasets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba68931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "evaluation_days = ['07.21', '07.27', '08.02', \n",
    "                 '08.11', '08.13', '07.29', '07.14', '07.05', '06.28', '06.23', '06.21', '06.16', \n",
    "                 '06.14', '06.07', '06.02', '05.26', '05.24', '05.19', '05.17', '05.05', '04.28']\n",
    "\n",
    "sessionNames_days = np.array([x[-5:] for x in sessionNames])\n",
    "# skipping the last 3 days because they are not historical data. \n",
    "for eval_day in evaluation_days:\n",
    "    \n",
    "    print(np.argwhere(eval_day==sessionNames_days)[0])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
