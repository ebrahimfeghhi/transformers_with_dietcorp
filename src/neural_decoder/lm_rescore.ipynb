{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77ffabb1295472495e88d8ab9ef461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-6.7b\"\n",
    "\n",
    "# Load tokenizer\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in 8-bit with automatic device placement\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example: Generate from a prompt\n",
    "inputs = llm_tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(llm.device)\n",
    "outputs = llm.generate(**inputs, max_new_tokens=50)\n",
    "print(llm_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_scale = 0.5\n",
    "llm_weight = 0.5\n",
    "seed_list = [1,2,3,4,5,6,7,8,9]\n",
    "from llm_utils import cer_with_gpt2_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR:  transformer_short_training_fixed_seed_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d79e3abf98d47fcb4d303166ff9ebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR:  transformer_short_training_fixed_seed_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a5db18c7ad4142a5c3f4d17d1f9ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in seed_list:\n",
    "    \n",
    "    saved_dir = '/home/ubuntu/data/model_transcriptions_finetune/'\n",
    "    model_name_str = f'transformer_short_training_fixed_seed_{seed}'\n",
    "    \n",
    "    print(\"RUNNING FOR: \", model_name_str)\n",
    "    \n",
    "    nbest_path = f\"{saved_dir}{model_name_str}_nbest.pkl\"\n",
    "    with open(nbest_path, mode = 'rb') as f:\n",
    "        nbest = pickle.load(f)\n",
    "        \n",
    "    model_outputs_path = f\"{saved_dir}{model_name_str}_model_outputs.pkl\"\n",
    "    with open(model_outputs_path, mode = 'rb') as f:\n",
    "        model_outputs = pickle.load(f)\n",
    "        \n",
    "    for i in range(len(model_outputs['transcriptions'])):\n",
    "        new_trans = [ord(c) for c in model_outputs['transcriptions'][i]] + [0]\n",
    "        model_outputs['transcriptions'][i] = np.array(new_trans)\n",
    "        \n",
    "    # Rescore nbest outputs with LLM\n",
    "    llm_out = cer_with_gpt2_decoder(\n",
    "        llm,\n",
    "        llm_tokenizer,\n",
    "        nbest[:],\n",
    "        acoustic_scale,\n",
    "        model_outputs,\n",
    "        outputType=\"speech_sil\",\n",
    "        returnCI=True,\n",
    "        lengthPenalty=0,\n",
    "        alpha=llm_weight,\n",
    "    )\n",
    "\n",
    "    with open(saved_dir + f\"{model_name_str}_llm_outs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(llm_out['decoded_transcripts'])+ \"\\n\")   # one line per LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_decode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
