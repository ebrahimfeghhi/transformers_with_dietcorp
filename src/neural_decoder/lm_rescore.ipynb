{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12e6869eb22400cbd7520c4d818f06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/ebrahim2/miniconda3/envs/llm_decode/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct this sentence: He were the best. He was the best.\n",
      "Answer: He was the best.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick‑start: Phi‑2 for N‑best rescoring or generation\n",
    "----------------------------------------------------------------\n",
    "• Model repo :  microsoft/phi-2\n",
    "• Context    :  4 k tokens\n",
    "• Dtype      :  fp16 recommended\n",
    "----------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, os, numpy as np\n",
    "\n",
    "# ── config ────────────────────────────────────────────────────\n",
    "device      = \"cuda:2\"              # GPU to use\n",
    "model_name  = \"microsoft/phi-2\"     # <-- swapped in Phi‑2\n",
    "dtype       = torch.float16         # fp16 is plenty for scoring\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"   # kill the fork warning\n",
    "\n",
    "# Phi‑2 uses a custom tokenizer implementation, so add trust_remote_code\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map       = device,      # let HF shard onto the chosen GPU\n",
    "    torch_dtype      = dtype,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ensure padding token exists (Phi‑2 shares EOS + PAD = \"<|endoftext|>\")\n",
    "tokenizer.pad_token     = tokenizer.eos_token\n",
    "tokenizer.padding_side  = \"right\"\n",
    "\n",
    "# ── quick test ────────────────────────────────────────────────\n",
    "prompt  = \"Correct this sentence: He were the best.\"\n",
    "inputs  = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# guidance: use temperature 0 & greedy decoding for grammar correction\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 50,\n",
    "    temperature    = 0.0,\n",
    ")\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import cer_with_gpt2_decoder\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [0]\n",
    "acoustic_scale = 0.8\n",
    "llm_weight = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FOR:  neurips_transformer_time_masked_seed_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e985ea879d5467d9a3b2e9d7b1f748d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in seed_list:\n",
    "    saved_dir = '/data/willett_data/model_transcriptions/'\n",
    "    model_name_str = f'neurips_transformer_time_masked_seed_{seed}'\n",
    "    \n",
    "    print(\"RUNNING FOR: \", model_name_str)\n",
    "    \n",
    "    nbest_path = f\"{saved_dir}{model_name_str}_nbest.pkl\"\n",
    "    with open(nbest_path, mode = 'rb') as f:\n",
    "        nbest = pickle.load(f)\n",
    "        \n",
    "    model_outputs_path = f\"{saved_dir}{model_name_str}_model_outputs.pkl\"\n",
    "    with open(model_outputs_path, mode = 'rb') as f:\n",
    "        model_outputs = pickle.load(f)\n",
    "        \n",
    "    for i in range(len(model_outputs['transcriptions'])):\n",
    "        new_trans = [ord(c) for c in model_outputs['transcriptions'][i]] + [0]\n",
    "        model_outputs['transcriptions'][i] = np.array(new_trans)\n",
    "        \n",
    "\n",
    "\n",
    "    # Rescore nbest outputs with LLM\n",
    "    start_t = time.time()\n",
    "    llm_out = cer_with_gpt2_decoder(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        nbest[:],\n",
    "        acoustic_scale,\n",
    "        model_outputs,\n",
    "        outputType=\"speech_sil\",\n",
    "        returnCI=True,\n",
    "        lengthPenalty=0,\n",
    "        alpha=llm_weight,\n",
    "    )\n",
    "\n",
    "    with open(saved_dir + f\"{model_name_str}_llm_outs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(llm_out['decoded_transcripts'])+ \"\\n\")   # one line per LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.10942436898492827, 0.10053771003512674, 0.11865446523197064)\n",
      "(0.1685761047463175, 0.15502609038445705, 0.18244167776031484)\n"
     ]
    }
   ],
   "source": [
    "print(llm_out['cer'])\n",
    "print(llm_out['wer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a9ee847dbe4c05bbc21db0ab0f1946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.2171875, 3.1532265625, 3.280942708333333) (3.08875, 3.029166666666667, 3.149177083333333)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_decode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
