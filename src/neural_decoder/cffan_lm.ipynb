{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from edit_distance import SequenceMatcher\n",
    "import torch\n",
    "from dataset import SpeechDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.model import GRUDecoder\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.lm_utils import build_llama_1B\n",
    "from neural_decoder.model import GRUDecoder\n",
    "from neural_decoder.bit import BiT_Phoneme\n",
    "import pickle\n",
    "import argparse\n",
    "from lm_utils import _cer_and_wer\n",
    "import json\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(s):\n",
    "    s = s.lower()\n",
    "    charMarks = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "                 \"'\", ' ']\n",
    "    ans = []\n",
    "    for i in s:\n",
    "        if(i in charMarks):\n",
    "            ans.append(i)\n",
    "    \n",
    "    return ''.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING IN N-GRAM MODE\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home3/skaasyap/willett\"\n",
    "\n",
    "load_lm = True\n",
    "# LM decoding hyperparameters\n",
    "acoustic_scale = 0.8\n",
    "blank_penalty = np.log(2)\n",
    "\n",
    "run_for_llm = False\n",
    "\n",
    "if run_for_llm:\n",
    "    return_n_best = True\n",
    "    rescore = False\n",
    "    nbest = 100\n",
    "    print(\"RUNNING IN LLM MODE\")\n",
    "else:\n",
    "    return_n_best = False\n",
    "    rescore = False\n",
    "    nbest = 1\n",
    "    print(\"RUNNING IN N-GRAM MODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_lm: \n",
    "        \n",
    "    lmDir = base_dir +'/lm/languageModel'\n",
    "    ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "        lmDir,\n",
    "        acoustic_scale=acoustic_scale, #1.2\n",
    "        nbest=nbest,\n",
    "        beam=18\n",
    "    )\n",
    "    print(\"loaded LM\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model predictions\n",
    "## Always check to make sure val perf matches wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to shared output file\n",
      "transformer_held_out_days_memo_seed_0\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_0\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1420767/3927999430.py:256: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER DAY:  2.5265625\n",
      "CER DAY:  2.7\n",
      "CER DAY:  2.4390625\n",
      "CER DAY:  2.3625\n",
      "CER DAY:  2.1125\n",
      "Model performance:  2.428125\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.03492524087429047 seconds per sample\n",
      "CER and WER after 3-gram LM:  (2.3834375, 2.2868671875000004, 2.479375) (2.64, 2.5425, 2.73625)\n",
      "transformer_held_out_days_memo_seed_0\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_0\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  3.24375\n",
      "CER DAY:  2.828125\n",
      "CER DAY:  2.5765625\n",
      "CER DAY:  2.696875\n",
      "CER DAY:  2.9375\n",
      "Model performance:  2.8565625\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.019733198881149293 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.0825, 2.9684375, 3.1971875) (3.12, 3.0162187499999997, 3.22625)\n",
      "transformer_held_out_days_memo_seed_0\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_0\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.903125\n",
      "CER DAY:  2.8546875\n",
      "CER DAY:  3.021875\n",
      "CER DAY:  3.2265625\n",
      "CER DAY:  2.8546875\n",
      "Model performance:  2.9721875\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.02052836775779724 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.2084375, 3.1065625, 3.3125) (3.26375, 3.165, 3.36503125)\n",
      "transformer_held_out_days_memo_seed_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_1\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.5421875\n",
      "CER DAY:  2.6921875\n",
      "CER DAY:  2.5\n",
      "CER DAY:  2.4453125\n",
      "CER DAY:  2.1609375\n",
      "Model performance:  2.468125\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.031775383353233336 seconds per sample\n",
      "CER and WER after 3-gram LM:  (2.4084375, 2.3071875, 2.509375) (2.65375, 2.55375, 2.75375)\n",
      "transformer_held_out_days_memo_seed_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_1\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  3.28125\n",
      "CER DAY:  2.83125\n",
      "CER DAY:  2.55\n",
      "CER DAY:  2.6875\n",
      "CER DAY:  2.965625\n",
      "Model performance:  2.863125\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.020575798749923706 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.1115625, 2.9953046875, 3.226875) (3.13625, 3.03, 3.2425)\n",
      "transformer_held_out_days_memo_seed_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_1\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.8828125\n",
      "CER DAY:  2.8734375\n",
      "CER DAY:  3.0640625\n",
      "CER DAY:  3.221875\n",
      "CER DAY:  2.8375\n",
      "Model performance:  2.9759375\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.020233400464057923 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.229375, 3.1303046875000002, 3.3325) (3.2675, 3.17, 3.36625)\n",
      "transformer_held_out_days_memo_seed_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_2\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.521875\n",
      "CER DAY:  2.6625\n",
      "CER DAY:  2.4265625\n",
      "CER DAY:  2.3734375\n",
      "CER DAY:  2.0734375\n",
      "Model performance:  2.4115625\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.03231586158275604 seconds per sample\n",
      "CER and WER after 3-gram LM:  (2.36125, 2.2646875, 2.4590625) (2.6225, 2.5225, 2.72375)\n",
      "transformer_held_out_days_memo_seed_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_2\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  3.3015625\n",
      "CER DAY:  2.8625\n",
      "CER DAY:  2.603125\n",
      "CER DAY:  2.6921875\n",
      "CER DAY:  2.975\n",
      "Model performance:  2.886875\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.0206740802526474 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.09625, 2.983125, 3.2140703124999996) (3.12125, 3.01875, 3.2275)\n",
      "transformer_held_out_days_memo_seed_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_2\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.921875\n",
      "CER DAY:  2.8703125\n",
      "CER DAY:  3.0421875\n",
      "CER DAY:  3.23125\n",
      "CER DAY:  2.8453125\n",
      "Model performance:  2.9821875\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.020211552381515504 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.2084375, 3.1078125, 3.3112578124999996) (3.2625, 3.165, 3.36125)\n",
      "transformer_held_out_days_memo_seed_3\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_3\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.5328125\n",
      "CER DAY:  2.6203125\n",
      "CER DAY:  2.3921875\n",
      "CER DAY:  2.378125\n",
      "CER DAY:  2.121875\n",
      "Model performance:  2.4090625\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.03369760930538177 seconds per sample\n",
      "CER and WER after 3-gram LM:  (2.4175, 2.3240625, 2.515625) (2.665, 2.57, 2.76375)\n",
      "transformer_held_out_days_memo_seed_3\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_3\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  3.25625\n",
      "CER DAY:  2.8421875\n",
      "CER DAY:  2.5578125\n",
      "CER DAY:  2.6796875\n",
      "CER DAY:  2.98125\n",
      "Model performance:  2.8634375\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.019275245666503907 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.108125, 2.9931171875, 3.2209375) (3.13, 3.02625, 3.235)\n",
      "transformer_held_out_days_memo_seed_3\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_3\n",
      "Loading TRANSFORMER\n",
      "RESTRICTED DAYS:  []\n",
      "CER DAY:  2.9\n",
      "CER DAY:  2.84375\n",
      "CER DAY:  3.009375\n",
      "CER DAY:  3.2078125\n",
      "CER DAY:  2.8734375\n",
      "Model performance:  2.966875\n",
      "Running n-gram LM\n",
      "N-gram decoding took 0.02034751296043396 seconds per sample\n",
      "CER and WER after 3-gram LM:  (3.2159375, 3.111875, 3.3196875) (3.2575, 3.15625, 3.3575)\n"
     ]
    }
   ],
   "source": [
    "output_file = 'obi'\n",
    "    \n",
    "device = \"cuda:2\"\n",
    "\n",
    "if output_file == 'obi':\n",
    "    model_storage_path = '/data/willett_data/outputs/'\n",
    "elif output_file == 'leia':\n",
    "    model_storage_path = '/data/willett_data/leia_outputs/'\n",
    "    \n",
    "models_to_run = ['tf_no_time_mask_blue']\n",
    "\n",
    "shared_output_file = 'transformer_held_out_days_memo'\n",
    "\n",
    "if len(shared_output_file) > 0:\n",
    "    print(\"Writing to shared output file\")\n",
    "    write_mode = \"a\"\n",
    "else:\n",
    "    write_mode = \"w\"\n",
    "    \n",
    "seeds_list = [0,1,2,3]\n",
    "partition = \"competition\" # \"test\"\n",
    "run_lm = True\n",
    "\n",
    "comp_on_reduced = True\n",
    "fill_max_day = False\n",
    "memo = True\n",
    "memo_epochs = 1\n",
    "memo_augs = 16\n",
    "memo_lr = [6e-5, 6e-5, 6e-5]\n",
    "skip_days = [[], [], []]\n",
    "\n",
    "day_edit_distance = 0\n",
    "day_seq_length = 0\n",
    "prev_day = None\n",
    "\n",
    "\n",
    "if partition == 'test':\n",
    "    saveFolder_transcripts = \"/data/willett_data/model_transcriptions/\"\n",
    "else:\n",
    "    saveFolder_transcripts = \"/data/willett_data/model_transcriptions_comp/\"\n",
    "    \n",
    "for seed in seeds_list:\n",
    "    \n",
    "    for mn, model_name_str in enumerate(models_to_run):\n",
    "        \n",
    "        modelPath = f\"{model_storage_path}{model_name_str}_seed_{seed}\"\n",
    "        \n",
    "        if len(shared_output_file) > 0:\n",
    "            output_file = f\"{shared_output_file}_seed_{seed}\"\n",
    "            print(output_file)\n",
    "        else:\n",
    "            output_file = f\"{model_name_str}_seed_{seed}\"\n",
    "            \n",
    "        print(f\"Running model: {model_name_str}_seed_{seed}\")\n",
    "            \n",
    "        with open(modelPath + \"/args\", \"rb\") as handle:\n",
    "            args = pickle.load(handle)\n",
    "            \n",
    "        if args['datasetPath'].rsplit('/', 1)[-1] == 'data_log_both':\n",
    "            data_file = '/data/willett_data/ptDecoder_ctc_both'\n",
    "            \n",
    "        elif args['datasetPath'].rsplit('/', 1)[-1] == 'data':\n",
    "            data_file = '/data/willett_data/ptDecoder_ctc'\n",
    "            \n",
    "        elif args['datasetPath'].rsplit('/', 1)[-1] == 'data_log_both_held_out_days':\n",
    "            data_file = '/data/willett_data/ptDecoder_ctc_both_held_out_days'\n",
    "            \n",
    "        elif args['datasetPath'].rsplit('/', 1)[-1] == 'data_log_both_held_out_days_1':\n",
    "            data_file = '/data/willett_data/ptDecoder_ctc_both_held_out_days_1'\n",
    "            \n",
    "        elif args['datasetPath'].rsplit('/', 1)[-1] == 'data_log_both_held_out_days_2':\n",
    "            data_file = '/data/willett_data/ptDecoder_ctc_both_held_out_days_2'\n",
    "            \n",
    "        else:\n",
    "            data_file = args['datasetPath']\n",
    "            \n",
    "        trainLoaders, testLoaders, loadedData = getDatasetLoaders(\n",
    "            data_file, 8\n",
    "        )\n",
    "        \n",
    "        # if true, model is a GRU\n",
    "        if 'nInputFeatures' in args.keys():\n",
    "            \n",
    "            if 'max_mask_pct' not in args:\n",
    "                args['max_mask_pct'] = 0\n",
    "            if 'num_masks' not in args:\n",
    "                args['num_masks'] = 0\n",
    "            if 'input_dropout' not in args:\n",
    "                args['input_dropout'] = 0\n",
    "                \n",
    "            print(\"Loading GRU\")\n",
    "            model = GRUDecoder(\n",
    "                neural_dim=args[\"nInputFeatures\"],\n",
    "                n_classes=args[\"nClasses\"],\n",
    "                hidden_dim=args[\"nUnits\"],\n",
    "                layer_dim=args[\"nLayers\"],\n",
    "                nDays=args['nDays'],\n",
    "                dropout=args[\"dropout\"],\n",
    "                device=device,\n",
    "                strideLen=args[\"strideLen\"],\n",
    "                kernelLen=args[\"kernelLen\"],\n",
    "                gaussianSmoothWidth=args[\"gaussianSmoothWidth\"],\n",
    "                bidirectional=args[\"bidirectional\"],\n",
    "                input_dropout=args['input_dropout'], \n",
    "                max_mask_pct=args['max_mask_pct'],\n",
    "                num_masks=args['num_masks']\n",
    "            ).to(device)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if 'mask_token_zero' not in args:\n",
    "                args['mask_token_zero'] = False\n",
    "            \n",
    "            print(\"Loading TRANSFORMER\")\n",
    "            \n",
    "            # Instantiate model\n",
    "            # set training relevant parameters for MEMO, doesn't matter for other runs because they are \n",
    "            # only run in eval mode.\n",
    "            model = BiT_Phoneme(\n",
    "                patch_size=args['patch_size'],\n",
    "                dim=args['dim'],\n",
    "                dim_head=args['dim_head'],\n",
    "                nClasses=args['nClasses'],\n",
    "                depth=args['depth'],\n",
    "                heads=args['heads'],\n",
    "                mlp_dim_ratio=args['mlp_dim_ratio'],\n",
    "                dropout=0,\n",
    "                input_dropout=0,\n",
    "                look_ahead=args['look_ahead'],\n",
    "                gaussianSmoothWidth=args['gaussianSmoothWidth'],\n",
    "                T5_style_pos=args['T5_style_pos'],\n",
    "                max_mask_pct=0.05,\n",
    "                num_masks=20, \n",
    "                mask_token_zeros=args['mask_token_zero'], \n",
    "                num_masks_channels=0, \n",
    "                max_mask_channels=0, \n",
    "                dist_dict_path=0, \n",
    "            ).to(device)\n",
    "            \n",
    "            \n",
    "        ckpt_path = modelPath + '/modelWeights'\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if memo: \n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=memo_lr[mn], weight_decay=0, \n",
    "                                    betas=(args['beta1'], args['beta2']))\n",
    "            original_state_dict = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            # Unfreeze patch‑embedding linear projection (assumed third module)\n",
    "            for p in model.to_patch_embedding[2].parameters():\n",
    "                p.requires_grad = True\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        model_outputs = {\n",
    "            \"logits\": [],\n",
    "            \"logitLengths\": [],\n",
    "            \"trueSeqs\": [],\n",
    "            \"transcriptions\": [],\n",
    "        }\n",
    "        \n",
    "        total_edit_distance = 0\n",
    "        total_seq_length = 0\n",
    "\n",
    "        if partition == \"competition\":\n",
    "            \n",
    "            if comp_on_reduced:\n",
    "                testDayIdxs = np.arange(5)\n",
    "            else:\n",
    "                testDayIdxs = [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20]\n",
    "                \n",
    "        elif partition == \"test\":\n",
    "            \n",
    "            testDayIdxs = range(len(loadedData[partition])) \n",
    "            \n",
    "        ground_truth_sentences = []\n",
    "        \n",
    "        print(\"RESTRICTED DAYS: \", args['restricted_days'])\n",
    "        \n",
    "        for i, testDayIdx in enumerate(testDayIdxs):\n",
    "            \n",
    "            if len(skip_days[mn]) > 0:\n",
    "                if testDayIdx in skip_days[mn]:\n",
    "                    continue\n",
    "            \n",
    "            if len(args['restricted_days']) > 0:\n",
    "                if testDayIdx not in args['restricted_days']:\n",
    "                    continue\n",
    "                \n",
    "            test_ds = SpeechDataset([loadedData[partition][i]])\n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_ds, batch_size=1, shuffle=False, num_workers=0\n",
    "            )\n",
    "            \n",
    "            for j, (X, y, X_len, y_len, _) in enumerate(test_loader):\n",
    "                        \n",
    "                X, y, X_len, y_len, dayIdx = (\n",
    "                    X.to(device),\n",
    "                    y.to(device),\n",
    "                    X_len.to(device),\n",
    "                    y_len.to(device),\n",
    "                    torch.tensor([testDayIdx], dtype=torch.int64).to(device),\n",
    "                )\n",
    "                \n",
    "                if fill_max_day:\n",
    "                    dayIdx.fill_(args['maxDay'])\n",
    "                     \n",
    "                if memo: \n",
    "                                        \n",
    "                    model.train()\n",
    "                    \n",
    "                    for _ in range(memo_epochs):\n",
    "                        \n",
    "                        logits_aug = model.forward(X, X_len, testDayIdx, memo_augs)  # [memo_augs, T, D]\n",
    "                        probs_aug = torch.nn.functional.softmax(logits_aug, dim=-1)          # [memo_augs, T, D]\n",
    "                        marginal_probs = probs_aug.mean(dim=0)                               # [T, D]\n",
    "\n",
    "                        adjustedLens = model.compute_length(X_len)\n",
    "                        marginal_probs = marginal_probs[:adjustedLens]\n",
    "\n",
    "                        loss = - (marginal_probs * marginal_probs.log()).sum(dim=-1).mean()\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    model.eval()\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    pred = model.forward(X, X_len, dayIdx)\n",
    "                \n",
    "                if hasattr(model, 'compute_length'):\n",
    "                    adjustedLens = model.compute_length(X_len)\n",
    "                else:\n",
    "                    adjustedLens = ((X_len - model.kernelLen) / model.strideLen).to(torch.int32)\n",
    "                    \n",
    "                for iterIdx in range(pred.shape[0]):\n",
    "                    \n",
    "                    trueSeq = np.array(y[iterIdx][0 : y_len[iterIdx]].cpu().detach())\n",
    "                    model_outputs[\"logits\"].append(pred[iterIdx].cpu().detach().numpy())\n",
    "                    \n",
    "                    model_outputs[\"logitLengths\"].append(\n",
    "                        adjustedLens[iterIdx].cpu().detach().item()\n",
    "                    )\n",
    "                    \n",
    "                    model_outputs[\"trueSeqs\"].append(trueSeq)\n",
    "                    \n",
    "                    decodedSeq = torch.argmax(\n",
    "                        torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n",
    "                        dim=-1,\n",
    "                    ) \n",
    "                    \n",
    "                    decodedSeq = torch.unique_consecutive(decodedSeq, dim=-1)\n",
    "                    decodedSeq = decodedSeq.cpu().detach().numpy()\n",
    "                    decodedSeq = np.array([i for i in decodedSeq if i != 0])\n",
    "                    \n",
    "                    matcher = SequenceMatcher(\n",
    "                        a=trueSeq.tolist(), b=decodedSeq.tolist()\n",
    "                    )\n",
    "                    \n",
    "                    total_edit_distance += matcher.distance()\n",
    "                    total_seq_length += len(trueSeq)\n",
    "                    \n",
    "                    day_edit_distance += matcher.distance()\n",
    "                    day_seq_length += len(trueSeq)\n",
    "                    \n",
    "                transcript = loadedData[partition][i][\"transcriptions\"][j].strip()\n",
    "                transcript = re.sub(r\"[^a-zA-Z\\- \\']\", \"\", transcript)\n",
    "                transcript = transcript.replace(\"--\", \"\").lower()\n",
    "                model_outputs[\"transcriptions\"].append(transcript)\n",
    "                \n",
    "            cer_day = day_edit_distance / day_seq_length\n",
    "            print(\"CER DAY: \", cer_day)\n",
    "            day_edit_distance = 0 \n",
    "            day_seq_length = 0\n",
    "\n",
    "        cer = total_edit_distance / total_seq_length\n",
    "        \n",
    "        print(\"Model performance: \", cer)\n",
    "\n",
    "        if run_lm:\n",
    "            \n",
    "            print(\"Running n-gram LM\")\n",
    "            \n",
    "            llm_outputs = []\n",
    "            start_t = time.time()\n",
    "            nbest_outputs = []\n",
    "            \n",
    "            for j in range(len(model_outputs[\"logits\"])):\n",
    "                \n",
    "                logits = model_outputs[\"logits\"][j]\n",
    "                \n",
    "                logits = np.concatenate(\n",
    "                    [logits[:, 1:], logits[:, 0:1]], axis=-1\n",
    "                )  # Blank is last token\n",
    "                \n",
    "                logits = lmDecoderUtils.rearrange_speech_logits(logits[None, :, :], has_sil=True)\n",
    "                \n",
    "                nbest = lmDecoderUtils.lm_decode(\n",
    "                    ngramDecoder,\n",
    "                    logits[0],\n",
    "                    blankPenalty=blank_penalty,\n",
    "                    returnNBest=return_n_best,\n",
    "                    rescore=rescore,\n",
    "                )\n",
    "                \n",
    "                nbest_outputs.append(nbest)\n",
    "                \n",
    "            time_per_sample = (time.time() - start_t) / len(model_outputs[\"logits\"])\n",
    "            print(f\"N-gram decoding took {time_per_sample} seconds per sample\")\n",
    "            \n",
    "            if run_for_llm:\n",
    "                print(\"SAVING OUTPUTS FOR LLM\")\n",
    "                with open(f\"{saveFolder_transcripts}{model_name_str}_seed_{seed}_model_outputs.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(model_outputs, f)\n",
    "                    \n",
    "                with open(f\"{saveFolder_transcripts}{model_name_str}_seed_{seed}_nbest.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(nbest_outputs, f)\n",
    "                \n",
    "            else:\n",
    "                # just get perf with greedy decoding\n",
    "                for i in range(len(model_outputs[\"transcriptions\"])):\n",
    "                    model_outputs[\"transcriptions\"][i] = model_outputs[\"transcriptions\"][i].strip()\n",
    "                    nbest_outputs[i] = nbest_outputs[i].strip()\n",
    "                \n",
    "                # lower case + remove puncs\n",
    "                for i in range(len(model_outputs[\"transcriptions\"])):\n",
    "                    model_outputs[\"transcriptions\"][i] = convert_sentence(model_outputs[\"transcriptions\"][i])\n",
    "\n",
    "                cer, wer = _cer_and_wer(nbest_outputs, model_outputs[\"transcriptions\"], \n",
    "                                    outputType='speech', returnCI=True)\n",
    "\n",
    "                print(\"CER and WER after 3-gram LM: \", cer, wer)       \n",
    "                \n",
    "                out_file = os.path.join(saveFolder_transcripts, output_file)   # no extension per your spec\n",
    "                \n",
    "                with open(out_file + '.txt', write_mode, encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(nbest_outputs)+ \"\\n\")   # one line per LLM output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running model: neurips_transformer_time_masked_held_out_days_1_seed_0\n",
    "CER DAY:  0.18701323074035844\n",
    "CER DAY:  0.20014556040756915\n",
    "CER DAY:  0.2676017601760176\n",
    "CER DAY:  0.30618311533888226\n",
    "CER DAY:  0.341395752058951\n",
    "Model performance:  0.26254497254307896\n",
    "Running n-gram LM\n",
    "N-gram decoding took 0.01895871174578764 seconds per sample\n",
    "CER and WER after 3-gram LM:  (0.24947886266989078, 0.24185914701801067, 0.25716652364444587) (0.3511564287490674, 0.33995621145518856, 0.36229499823919814)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with memo\n",
    "# without\n",
    "Running model: neurips_transformer_time_masked_held_out_days_2_seed_0\n",
    "Output exceeds the size limit. Open the full output data in a text editorCER DAY:  0.3284115035707392\n",
    "CER DAY:  0.380046403712297\n",
    "CER DAY:  0.5224416517055656\n",
    "CER DAY:  0.5491719863077066\n",
    "CER DAY:  0.5892558916311004\n",
    "Model performance:  0.47584572132081365\n",
    "Running n-gram LM\n",
    "N-gram decoding took 0.04170605278015137 seconds per sample\n",
    "CER and WER after 3-gram LM:  (0.4804508971108348, 0.4715959430873275, 0.48947669469917093) (0.6677740863787376, 0.6558902882064046, 0.6799936068139665)\n",
    "Running model: neurips_transformer_time_masked_held_out_days_1_seed_0\n",
    "Loading TRANSFORMER\n",
    "RESTRICTED DAYS:  []\n",
    "CER DAY:  0.1893591066904382\n",
    "CER DAY:  0.2011852776044916\n",
    "CER DAY:  0.27035203520352036\n",
    "CER DAY:  0.33125247720967105\n",
    "CER DAY:  0.38925010836584306\n",
    "Model performance:  0.27901912516568833\n",
    "Running n-gram LM\n",
    "N-gram decoding took 0.02109781369871023 seconds per sample\n",
    "CER and WER after 3-gram LM:  (0.25845076294505126, 0.25050590982146115, 0.2666196134722149) (0.36906242228301417, 0.35733628898284037, 0.3811317908198835)\n",
    "Running model: neurips_transformer_time_masked_held_out_days_seed_0\n",
    "CER DAY:  0.1958174904942966\n",
    "CER DAY:  0.21266887357967254\n",
    "CER DAY:  0.2108843537414966\n",
    "CER DAY:  0.2558579384259746\n",
    "CER DAY:  0.25327416387054874\n",
    "Model performance:  0.22340826085319476\n",
    "Running n-gram LM\n",
    "N-gram decoding took 0.019061621614530976 seconds per sample\n",
    "CER and WER after 3-gram LM:  (0.19087837837837837, 0.1840513671786001, 0.19784976888843056) (0.28044250645994834, 0.2702872083074558, 0.2907064326560632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without memo\n",
    "Running model: neurips_transformer_time_masked_held_out_days_2_seed_0\n",
    "CER DAY:  0.3284115035707392\n",
    "CER DAY:  0.380046403712297\n",
    "CER DAY:  0.5224416517055656\n",
    "CER DAY:  0.5491719863077066\n",
    "CER DAY:  0.5892558916311004\n",
    "Model performance:  0.47584572132081365\n",
    "neurips_transformer_time_masked_held_out_days_1_seed_0\n",
    "CER DAY:  0.1893591066904382\n",
    "CER DAY:  0.2011852776044916\n",
    "CER DAY:  0.27035203520352036\n",
    "CER DAY:  0.33125247720967105\n",
    "CER DAY:  0.38925010836584306\n",
    "Model performance:  0.27901912516568833\n",
    "Running model: neurips_transformer_time_masked_held_out_days_seed_0\n",
    "Loading TRANSFORMER\n",
    "RESTRICTED DAYS:  []\n",
    "CER DAY:  0.1958174904942966\n",
    "CER DAY:  0.21266887357967254\n",
    "CER DAY:  0.2041424760709242\n",
    "CER DAY:  0.2108843537414966\n",
    "CER DAY:  0.2558579384259746\n",
    "CER DAY:  0.25327416387054874\n",
    "Model performance:  0.22134440503605587\n",
    "\n",
    "# with memo (6e-5)\n",
    "Running model: neurips_transformer_time_masked_held_out_days_2_seed_0\n",
    "CER DAY:  0.31596216946535416\n",
    "CER DAY:  0.34960556844547563\n",
    "CER DAY:  0.47755834829443444\n",
    "CER DAY:  0.5047645480618004\n",
    "CER DAY:  0.5534421970681017\n",
    "Model performance:  0.4418882416714136\n",
    "\n",
    "\n",
    "# with memo (1e-4)\n",
    "neurips_transformer_time_masked_held_out_days_1_seed_0\n",
    "CER DAY:  0.18710706577836164\n",
    "CER DAY:  0.1988978997712622\n",
    "CER DAY:  0.2666850018335167\n",
    "CER DAY:  0.3053904082441538\n",
    "CER DAY:  0.34442999566536625\n",
    "Model performance:  0.26265858738875214\n",
    "Running model: neurips_transformer_time_masked_held_out_days_seed_0\n",
    "Loading TRANSFORMER\n",
    "RESTRICTED DAYS:  []\n",
    "CER DAY:  0.19218804009678533\n",
    "CER DAY:  0.20622707345441532\n",
    "CER DAY:  0.19708143731366703\n",
    "CER DAY:  0.2010677688797038\n",
    "CER DAY:  0.24188294630660923\n",
    "CER DAY:  0.2463470072518671\n",
    "Model performance:  0.21344405036055875\n",
    "\n",
    "# 1.5e-4 No help\n",
    "\n",
    "# 9e-5 No help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
