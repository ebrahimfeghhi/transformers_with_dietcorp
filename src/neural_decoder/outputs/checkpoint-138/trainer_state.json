{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 50,
  "global_step": 138,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 2.868858575820923,
      "learning_rate": 0.0,
      "loss": 0.4377,
      "step": 1
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 2.4481637477874756,
      "learning_rate": 4e-05,
      "loss": 0.3194,
      "step": 2
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 1307.41064453125,
      "learning_rate": 8e-05,
      "loss": 0.4101,
      "step": 3
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 2149.394287109375,
      "learning_rate": 0.00012,
      "loss": 0.1688,
      "step": 4
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.7509116530418396,
      "learning_rate": 0.00016,
      "loss": 0.1599,
      "step": 5
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.7061238288879395,
      "learning_rate": 0.0002,
      "loss": 0.2409,
      "step": 6
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.27227386832237244,
      "learning_rate": 0.00019849624060150375,
      "loss": 0.0795,
      "step": 7
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.2148728370666504,
      "learning_rate": 0.00019699248120300754,
      "loss": 0.0286,
      "step": 8
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.30292826890945435,
      "learning_rate": 0.00019548872180451127,
      "loss": 0.1178,
      "step": 9
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.19776758551597595,
      "learning_rate": 0.00019398496240601503,
      "loss": 0.105,
      "step": 10
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.1962316483259201,
      "learning_rate": 0.0001924812030075188,
      "loss": 0.0564,
      "step": 11
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.287210077047348,
      "learning_rate": 0.00019097744360902256,
      "loss": 0.1237,
      "step": 12
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.3748473823070526,
      "learning_rate": 0.00018947368421052632,
      "loss": 0.2626,
      "step": 13
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.2463805079460144,
      "learning_rate": 0.00018796992481203009,
      "loss": 0.1248,
      "step": 14
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.42493027448654175,
      "learning_rate": 0.00018646616541353382,
      "loss": 0.0739,
      "step": 15
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.23316851258277893,
      "learning_rate": 0.0001849624060150376,
      "loss": 0.0942,
      "step": 16
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.23873235285282135,
      "learning_rate": 0.00018345864661654135,
      "loss": 0.0906,
      "step": 17
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 455.78662109375,
      "learning_rate": 0.0001819548872180451,
      "loss": 0.0818,
      "step": 18
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.29031625390052795,
      "learning_rate": 0.00018045112781954887,
      "loss": 0.1608,
      "step": 19
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.40371084213256836,
      "learning_rate": 0.00017894736842105264,
      "loss": 0.0759,
      "step": 20
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.40550491213798523,
      "learning_rate": 0.0001774436090225564,
      "loss": 0.0805,
      "step": 21
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.34601616859436035,
      "learning_rate": 0.00017593984962406016,
      "loss": 0.1789,
      "step": 22
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.170612171292305,
      "learning_rate": 0.0001744360902255639,
      "loss": 0.0862,
      "step": 23
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.19196325540542603,
      "learning_rate": 0.0001729323308270677,
      "loss": 0.0636,
      "step": 24
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.1892496794462204,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.0586,
      "step": 25
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.2580764889717102,
      "learning_rate": 0.0001699248120300752,
      "loss": 0.1445,
      "step": 26
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.18287315964698792,
      "learning_rate": 0.00016842105263157895,
      "loss": 0.1128,
      "step": 27
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.3269885778427124,
      "learning_rate": 0.00016691729323308271,
      "loss": 0.1966,
      "step": 28
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 14.57458209991455,
      "learning_rate": 0.00016541353383458648,
      "loss": 0.1593,
      "step": 29
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.28213420510292053,
      "learning_rate": 0.00016390977443609024,
      "loss": 0.1833,
      "step": 30
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.22797539830207825,
      "learning_rate": 0.00016240601503759398,
      "loss": 0.0943,
      "step": 31
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.20409445464611053,
      "learning_rate": 0.00016090225563909777,
      "loss": 0.0861,
      "step": 32
    },
    {
      "epoch": 0.24,
      "grad_norm": 16.87847328186035,
      "learning_rate": 0.0001593984962406015,
      "loss": 0.0783,
      "step": 33
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.23691801726818085,
      "learning_rate": 0.00015789473684210527,
      "loss": 0.0961,
      "step": 34
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.2901511788368225,
      "learning_rate": 0.00015639097744360903,
      "loss": 0.0859,
      "step": 35
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.37573957443237305,
      "learning_rate": 0.0001548872180451128,
      "loss": 0.1304,
      "step": 36
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.29742923378944397,
      "learning_rate": 0.00015338345864661653,
      "loss": 0.1289,
      "step": 37
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.3038875162601471,
      "learning_rate": 0.00015187969924812032,
      "loss": 0.1141,
      "step": 38
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.23010748624801636,
      "learning_rate": 0.00015037593984962405,
      "loss": 0.0692,
      "step": 39
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.22638356685638428,
      "learning_rate": 0.00014887218045112784,
      "loss": 0.0686,
      "step": 40
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.12523353099822998,
      "learning_rate": 0.00014736842105263158,
      "loss": 0.0188,
      "step": 41
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.19986629486083984,
      "learning_rate": 0.00014586466165413534,
      "loss": 0.1287,
      "step": 42
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.10885854810476303,
      "learning_rate": 0.0001443609022556391,
      "loss": 0.0512,
      "step": 43
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.16892880201339722,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.0587,
      "step": 44
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.13543230295181274,
      "learning_rate": 0.0001413533834586466,
      "loss": 0.062,
      "step": 45
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 0.1808425337076187,
      "learning_rate": 0.0001398496240601504,
      "loss": 0.0807,
      "step": 46
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 0.2146451771259308,
      "learning_rate": 0.00013834586466165413,
      "loss": 0.1076,
      "step": 47
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.3921067416667938,
      "learning_rate": 0.0001368421052631579,
      "loss": 0.2332,
      "step": 48
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 0.17247891426086426,
      "learning_rate": 0.00013533834586466166,
      "loss": 0.0574,
      "step": 49
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.18811264634132385,
      "learning_rate": 0.00013383458646616542,
      "loss": 0.0892,
      "step": 50
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.24234923720359802,
      "learning_rate": 0.00013233082706766918,
      "loss": 0.1117,
      "step": 51
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 54.72597885131836,
      "learning_rate": 0.00013082706766917294,
      "loss": 0.0196,
      "step": 52
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.14559748768806458,
      "learning_rate": 0.00012932330827067668,
      "loss": 0.0382,
      "step": 53
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.1952543407678604,
      "learning_rate": 0.00012781954887218047,
      "loss": 0.0742,
      "step": 54
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.1868746131658554,
      "learning_rate": 0.0001263157894736842,
      "loss": 0.0866,
      "step": 55
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.19738329946994781,
      "learning_rate": 0.00012481203007518797,
      "loss": 0.0348,
      "step": 56
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 0.3268079459667206,
      "learning_rate": 0.00012330827067669173,
      "loss": 0.1218,
      "step": 57
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.3146916925907135,
      "learning_rate": 0.0001218045112781955,
      "loss": 0.1582,
      "step": 58
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.17280082404613495,
      "learning_rate": 0.00012030075187969925,
      "loss": 0.0957,
      "step": 59
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.20505116879940033,
      "learning_rate": 0.00011879699248120302,
      "loss": 0.0913,
      "step": 60
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.19081318378448486,
      "learning_rate": 0.00011729323308270677,
      "loss": 0.0847,
      "step": 61
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 2.987401008605957,
      "learning_rate": 0.00011578947368421053,
      "loss": 0.1432,
      "step": 62
    },
    {
      "epoch": 0.4581818181818182,
      "grad_norm": 0.2151539921760559,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.1309,
      "step": 63
    },
    {
      "epoch": 0.46545454545454545,
      "grad_norm": 0.18672209978103638,
      "learning_rate": 0.00011278195488721806,
      "loss": 0.1058,
      "step": 64
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.2130396068096161,
      "learning_rate": 0.00011127819548872181,
      "loss": 0.1109,
      "step": 65
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.18546588718891144,
      "learning_rate": 0.00010977443609022557,
      "loss": 0.068,
      "step": 66
    },
    {
      "epoch": 0.48727272727272725,
      "grad_norm": 0.15475012362003326,
      "learning_rate": 0.00010827067669172932,
      "loss": 0.0778,
      "step": 67
    },
    {
      "epoch": 0.49454545454545457,
      "grad_norm": 0.17815715074539185,
      "learning_rate": 0.0001067669172932331,
      "loss": 0.1017,
      "step": 68
    },
    {
      "epoch": 0.5018181818181818,
      "grad_norm": 0.11193688213825226,
      "learning_rate": 0.00010526315789473685,
      "loss": 0.0539,
      "step": 69
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.18856088817119598,
      "learning_rate": 0.00010375939849624061,
      "loss": 0.1041,
      "step": 70
    },
    {
      "epoch": 0.5163636363636364,
      "grad_norm": 0.1576494574546814,
      "learning_rate": 0.00010225563909774436,
      "loss": 0.0693,
      "step": 71
    },
    {
      "epoch": 0.5236363636363637,
      "grad_norm": 0.11223829537630081,
      "learning_rate": 0.00010075187969924814,
      "loss": 0.0507,
      "step": 72
    },
    {
      "epoch": 0.5309090909090909,
      "grad_norm": 9.182149887084961,
      "learning_rate": 9.924812030075187e-05,
      "loss": 0.0913,
      "step": 73
    },
    {
      "epoch": 0.5381818181818182,
      "grad_norm": 0.1553633213043213,
      "learning_rate": 9.774436090225564e-05,
      "loss": 0.0733,
      "step": 74
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.17838174104690552,
      "learning_rate": 9.62406015037594e-05,
      "loss": 0.0853,
      "step": 75
    },
    {
      "epoch": 0.5527272727272727,
      "grad_norm": 0.14431948959827423,
      "learning_rate": 9.473684210526316e-05,
      "loss": 0.0513,
      "step": 76
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.1974986344575882,
      "learning_rate": 9.323308270676691e-05,
      "loss": 0.1046,
      "step": 77
    },
    {
      "epoch": 0.5672727272727273,
      "grad_norm": 0.2308424860239029,
      "learning_rate": 9.172932330827067e-05,
      "loss": 0.0631,
      "step": 78
    },
    {
      "epoch": 0.5745454545454546,
      "grad_norm": 0.23051415383815765,
      "learning_rate": 9.022556390977444e-05,
      "loss": 0.1074,
      "step": 79
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.1725303828716278,
      "learning_rate": 8.87218045112782e-05,
      "loss": 0.0604,
      "step": 80
    },
    {
      "epoch": 0.5890909090909091,
      "grad_norm": 0.1924595683813095,
      "learning_rate": 8.721804511278195e-05,
      "loss": 0.0903,
      "step": 81
    },
    {
      "epoch": 0.5963636363636363,
      "grad_norm": 0.16419921815395355,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.073,
      "step": 82
    },
    {
      "epoch": 0.6036363636363636,
      "grad_norm": 0.21528412401676178,
      "learning_rate": 8.421052631578948e-05,
      "loss": 0.079,
      "step": 83
    },
    {
      "epoch": 0.610909090909091,
      "grad_norm": 0.1652979850769043,
      "learning_rate": 8.270676691729324e-05,
      "loss": 0.0644,
      "step": 84
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.1763015240430832,
      "learning_rate": 8.120300751879699e-05,
      "loss": 0.0628,
      "step": 85
    },
    {
      "epoch": 0.6254545454545455,
      "grad_norm": 0.10366430878639221,
      "learning_rate": 7.969924812030075e-05,
      "loss": 0.0192,
      "step": 86
    },
    {
      "epoch": 0.6327272727272727,
      "grad_norm": 0.12022925913333893,
      "learning_rate": 7.819548872180451e-05,
      "loss": 0.0236,
      "step": 87
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.12747789919376373,
      "learning_rate": 7.669172932330826e-05,
      "loss": 0.0667,
      "step": 88
    },
    {
      "epoch": 0.6472727272727272,
      "grad_norm": 9.563582420349121,
      "learning_rate": 7.518796992481203e-05,
      "loss": 0.1762,
      "step": 89
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.14122837781906128,
      "learning_rate": 7.368421052631579e-05,
      "loss": 0.0482,
      "step": 90
    },
    {
      "epoch": 0.6618181818181819,
      "grad_norm": 0.279802531003952,
      "learning_rate": 7.218045112781955e-05,
      "loss": 0.1511,
      "step": 91
    },
    {
      "epoch": 0.6690909090909091,
      "grad_norm": 0.11827490478754044,
      "learning_rate": 7.06766917293233e-05,
      "loss": 0.0421,
      "step": 92
    },
    {
      "epoch": 0.6763636363636364,
      "grad_norm": 0.20371219515800476,
      "learning_rate": 6.917293233082706e-05,
      "loss": 0.1141,
      "step": 93
    },
    {
      "epoch": 0.6836363636363636,
      "grad_norm": 0.20977947115898132,
      "learning_rate": 6.766917293233083e-05,
      "loss": 0.1339,
      "step": 94
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.22864237427711487,
      "learning_rate": 6.616541353383459e-05,
      "loss": 0.1333,
      "step": 95
    },
    {
      "epoch": 0.6981818181818182,
      "grad_norm": 0.2667064964771271,
      "learning_rate": 6.466165413533834e-05,
      "loss": 0.1574,
      "step": 96
    },
    {
      "epoch": 0.7054545454545454,
      "grad_norm": 0.26065266132354736,
      "learning_rate": 6.31578947368421e-05,
      "loss": 0.0806,
      "step": 97
    },
    {
      "epoch": 0.7127272727272728,
      "grad_norm": 0.1911158263683319,
      "learning_rate": 6.165413533834587e-05,
      "loss": 0.0642,
      "step": 98
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.16060875356197357,
      "learning_rate": 6.015037593984962e-05,
      "loss": 0.0864,
      "step": 99
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.16095155477523804,
      "learning_rate": 5.8646616541353386e-05,
      "loss": 0.0766,
      "step": 100
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.18351344764232635,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.0933,
      "step": 101
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.24854658544063568,
      "learning_rate": 5.5639097744360905e-05,
      "loss": 0.1522,
      "step": 102
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.15221358835697174,
      "learning_rate": 5.413533834586466e-05,
      "loss": 0.0633,
      "step": 103
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.23103496432304382,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 0.088,
      "step": 104
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.2018062025308609,
      "learning_rate": 5.112781954887218e-05,
      "loss": 0.112,
      "step": 105
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.18188820779323578,
      "learning_rate": 4.9624060150375936e-05,
      "loss": 0.0753,
      "step": 106
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.21595560014247894,
      "learning_rate": 4.81203007518797e-05,
      "loss": 0.0564,
      "step": 107
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.11618991196155548,
      "learning_rate": 4.6616541353383456e-05,
      "loss": 0.0196,
      "step": 108
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 0.19034241139888763,
      "learning_rate": 4.511278195488722e-05,
      "loss": 0.0536,
      "step": 109
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.16078047454357147,
      "learning_rate": 4.3609022556390975e-05,
      "loss": 0.0863,
      "step": 110
    },
    {
      "epoch": 0.8072727272727273,
      "grad_norm": 0.1624625325202942,
      "learning_rate": 4.210526315789474e-05,
      "loss": 0.0846,
      "step": 111
    },
    {
      "epoch": 0.8145454545454546,
      "grad_norm": 0.2144753336906433,
      "learning_rate": 4.0601503759398494e-05,
      "loss": 0.1101,
      "step": 112
    },
    {
      "epoch": 0.8218181818181818,
      "grad_norm": 0.16754524409770966,
      "learning_rate": 3.909774436090226e-05,
      "loss": 0.063,
      "step": 113
    },
    {
      "epoch": 0.8290909090909091,
      "grad_norm": 0.16007356345653534,
      "learning_rate": 3.759398496240601e-05,
      "loss": 0.0784,
      "step": 114
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.207082137465477,
      "learning_rate": 3.6090225563909776e-05,
      "loss": 0.084,
      "step": 115
    },
    {
      "epoch": 0.8436363636363636,
      "grad_norm": 0.09563130140304565,
      "learning_rate": 3.458646616541353e-05,
      "loss": 0.0281,
      "step": 116
    },
    {
      "epoch": 0.850909090909091,
      "grad_norm": 0.20237691700458527,
      "learning_rate": 3.3082706766917295e-05,
      "loss": 0.1001,
      "step": 117
    },
    {
      "epoch": 0.8581818181818182,
      "grad_norm": 0.21304620802402496,
      "learning_rate": 3.157894736842105e-05,
      "loss": 0.0956,
      "step": 118
    },
    {
      "epoch": 0.8654545454545455,
      "grad_norm": 0.2365497201681137,
      "learning_rate": 3.007518796992481e-05,
      "loss": 0.0875,
      "step": 119
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.12951822578907013,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.045,
      "step": 120
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.1175759881734848,
      "learning_rate": 2.706766917293233e-05,
      "loss": 0.0516,
      "step": 121
    },
    {
      "epoch": 0.8872727272727273,
      "grad_norm": 0.09382159262895584,
      "learning_rate": 2.556390977443609e-05,
      "loss": 0.0212,
      "step": 122
    },
    {
      "epoch": 0.8945454545454545,
      "grad_norm": 0.17234884202480316,
      "learning_rate": 2.406015037593985e-05,
      "loss": 0.0665,
      "step": 123
    },
    {
      "epoch": 0.9018181818181819,
      "grad_norm": 0.24883495271205902,
      "learning_rate": 2.255639097744361e-05,
      "loss": 0.1639,
      "step": 124
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.15515723824501038,
      "learning_rate": 2.105263157894737e-05,
      "loss": 0.0472,
      "step": 125
    },
    {
      "epoch": 0.9163636363636364,
      "grad_norm": 0.1971248984336853,
      "learning_rate": 1.954887218045113e-05,
      "loss": 0.0848,
      "step": 126
    },
    {
      "epoch": 0.9236363636363636,
      "grad_norm": 0.1213860809803009,
      "learning_rate": 1.8045112781954888e-05,
      "loss": 0.0464,
      "step": 127
    },
    {
      "epoch": 0.9309090909090909,
      "grad_norm": 9.94221019744873,
      "learning_rate": 1.6541353383458648e-05,
      "loss": 0.0147,
      "step": 128
    },
    {
      "epoch": 0.9381818181818182,
      "grad_norm": 0.1257011592388153,
      "learning_rate": 1.5037593984962406e-05,
      "loss": 0.0213,
      "step": 129
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.24723558127880096,
      "learning_rate": 1.3533834586466165e-05,
      "loss": 0.1374,
      "step": 130
    },
    {
      "epoch": 0.9527272727272728,
      "grad_norm": 2.9303348064422607,
      "learning_rate": 1.2030075187969925e-05,
      "loss": 0.0956,
      "step": 131
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.172160804271698,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 0.0729,
      "step": 132
    },
    {
      "epoch": 0.9672727272727273,
      "grad_norm": 0.20301496982574463,
      "learning_rate": 9.022556390977444e-06,
      "loss": 0.1014,
      "step": 133
    },
    {
      "epoch": 0.9745454545454545,
      "grad_norm": 6.7369608879089355,
      "learning_rate": 7.518796992481203e-06,
      "loss": 0.0737,
      "step": 134
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 29.495094299316406,
      "learning_rate": 6.015037593984962e-06,
      "loss": 0.0413,
      "step": 135
    },
    {
      "epoch": 0.9890909090909091,
      "grad_norm": 70.52523040771484,
      "learning_rate": 4.511278195488722e-06,
      "loss": 0.0023,
      "step": 136
    },
    {
      "epoch": 0.9963636363636363,
      "grad_norm": 0.2325296550989151,
      "learning_rate": 3.007518796992481e-06,
      "loss": 0.1057,
      "step": 137
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1182006448507309,
      "learning_rate": 1.5037593984962406e-06,
      "loss": 0.0381,
      "step": 138
    }
  ],
  "logging_steps": 1,
  "max_steps": 138,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0387726068586906e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
