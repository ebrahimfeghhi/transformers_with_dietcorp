{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 50,
  "global_step": 276,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 1.8589177131652832,
      "learning_rate": 0.0,
      "loss": 0.3727,
      "step": 1
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 713.9871826171875,
      "learning_rate": 4e-05,
      "loss": 0.2405,
      "step": 2
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 1.6795772314071655,
      "learning_rate": 8e-05,
      "loss": 0.4063,
      "step": 3
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 1.2436425685882568,
      "learning_rate": 0.00012,
      "loss": 0.1518,
      "step": 4
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.4833201766014099,
      "learning_rate": 0.00016,
      "loss": 0.1172,
      "step": 5
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.8870460391044617,
      "learning_rate": 0.0002,
      "loss": 0.2522,
      "step": 6
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.3683168888092041,
      "learning_rate": 0.00019926199261992622,
      "loss": 0.0857,
      "step": 7
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.5272585153579712,
      "learning_rate": 0.0001985239852398524,
      "loss": 0.0427,
      "step": 8
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.24861261248588562,
      "learning_rate": 0.00019778597785977863,
      "loss": 0.1078,
      "step": 9
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.18952076137065887,
      "learning_rate": 0.0001970479704797048,
      "loss": 0.1026,
      "step": 10
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.16934503614902496,
      "learning_rate": 0.000196309963099631,
      "loss": 0.0541,
      "step": 11
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.3346015512943268,
      "learning_rate": 0.0001955719557195572,
      "loss": 0.1283,
      "step": 12
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.4247085750102997,
      "learning_rate": 0.0001948339483394834,
      "loss": 0.254,
      "step": 13
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.20222385227680206,
      "learning_rate": 0.0001940959409594096,
      "loss": 0.1253,
      "step": 14
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 4.697387218475342,
      "learning_rate": 0.0001933579335793358,
      "loss": 0.0637,
      "step": 15
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.6036581993103027,
      "learning_rate": 0.000192619926199262,
      "loss": 0.1184,
      "step": 16
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.551861047744751,
      "learning_rate": 0.0001918819188191882,
      "loss": 0.1185,
      "step": 17
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.4592282176017761,
      "learning_rate": 0.0001911439114391144,
      "loss": 0.1529,
      "step": 18
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.3334507942199707,
      "learning_rate": 0.0001904059040590406,
      "loss": 0.1815,
      "step": 19
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.22975578904151917,
      "learning_rate": 0.0001896678966789668,
      "loss": 0.0719,
      "step": 20
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 19.559846878051758,
      "learning_rate": 0.000188929889298893,
      "loss": 0.0393,
      "step": 21
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.28253182768821716,
      "learning_rate": 0.0001881918819188192,
      "loss": 0.1853,
      "step": 22
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.14212927222251892,
      "learning_rate": 0.0001874538745387454,
      "loss": 0.0796,
      "step": 23
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 2.575847625732422,
      "learning_rate": 0.0001867158671586716,
      "loss": 0.0649,
      "step": 24
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 2.316971778869629,
      "learning_rate": 0.00018597785977859778,
      "loss": 0.0674,
      "step": 25
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.36657392978668213,
      "learning_rate": 0.000185239852398524,
      "loss": 0.1806,
      "step": 26
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.4771326184272766,
      "learning_rate": 0.0001845018450184502,
      "loss": 0.1474,
      "step": 27
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.5244191288948059,
      "learning_rate": 0.0001837638376383764,
      "loss": 0.2322,
      "step": 28
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.41307422518730164,
      "learning_rate": 0.0001830258302583026,
      "loss": 0.178,
      "step": 29
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.34094417095184326,
      "learning_rate": 0.0001822878228782288,
      "loss": 0.1959,
      "step": 30
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.23415185511112213,
      "learning_rate": 0.00018154981549815499,
      "loss": 0.1124,
      "step": 31
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.18683838844299316,
      "learning_rate": 0.00018081180811808117,
      "loss": 0.0922,
      "step": 32
    },
    {
      "epoch": 0.24,
      "grad_norm": 24.686330795288086,
      "learning_rate": 0.0001800738007380074,
      "loss": 0.0909,
      "step": 33
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 5.772577285766602,
      "learning_rate": 0.00017933579335793358,
      "loss": 0.0969,
      "step": 34
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.2445685863494873,
      "learning_rate": 0.0001785977859778598,
      "loss": 0.0889,
      "step": 35
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.21627789735794067,
      "learning_rate": 0.00017785977859778598,
      "loss": 0.1228,
      "step": 36
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.8691692352294922,
      "learning_rate": 0.0001771217712177122,
      "loss": 0.1231,
      "step": 37
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.24041353166103363,
      "learning_rate": 0.00017638376383763839,
      "loss": 0.136,
      "step": 38
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.2577968239784241,
      "learning_rate": 0.00017564575645756457,
      "loss": 0.0817,
      "step": 39
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 2.760286569595337,
      "learning_rate": 0.0001749077490774908,
      "loss": 0.0789,
      "step": 40
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 5.18417501449585,
      "learning_rate": 0.00017416974169741698,
      "loss": 0.0272,
      "step": 41
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 1.3947116136550903,
      "learning_rate": 0.0001734317343173432,
      "loss": 0.1511,
      "step": 42
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.9304709434509277,
      "learning_rate": 0.00017269372693726938,
      "loss": 0.135,
      "step": 43
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.9296104907989502,
      "learning_rate": 0.0001719557195571956,
      "loss": 0.1439,
      "step": 44
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.6346086859703064,
      "learning_rate": 0.00017121771217712176,
      "loss": 0.1285,
      "step": 45
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 1.7453100681304932,
      "learning_rate": 0.00017047970479704797,
      "loss": 0.0993,
      "step": 46
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 2.4190289974212646,
      "learning_rate": 0.0001697416974169742,
      "loss": 0.1275,
      "step": 47
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.29563412070274353,
      "learning_rate": 0.00016900369003690038,
      "loss": 0.232,
      "step": 48
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 60.16678237915039,
      "learning_rate": 0.0001682656826568266,
      "loss": 0.0016,
      "step": 49
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.4759919345378876,
      "learning_rate": 0.00016752767527675278,
      "loss": 0.132,
      "step": 50
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.6405545473098755,
      "learning_rate": 0.00016678966789667897,
      "loss": 0.1603,
      "step": 51
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 1.7482558488845825,
      "learning_rate": 0.00016605166051660516,
      "loss": 0.0836,
      "step": 52
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.844176709651947,
      "learning_rate": 0.00016531365313653137,
      "loss": 0.1183,
      "step": 53
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.776005744934082,
      "learning_rate": 0.00016457564575645756,
      "loss": 0.1408,
      "step": 54
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5110009908676147,
      "learning_rate": 0.00016383763837638377,
      "loss": 0.1336,
      "step": 55
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.31110140681266785,
      "learning_rate": 0.00016309963099630996,
      "loss": 0.0536,
      "step": 56
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 66.91677856445312,
      "learning_rate": 0.00016236162361623618,
      "loss": 0.1066,
      "step": 57
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.23903241753578186,
      "learning_rate": 0.00016162361623616237,
      "loss": 0.1685,
      "step": 58
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.1843833178281784,
      "learning_rate": 0.00016088560885608855,
      "loss": 0.1158,
      "step": 59
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.16988758742809296,
      "learning_rate": 0.00016014760147601477,
      "loss": 0.1018,
      "step": 60
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.19114364683628082,
      "learning_rate": 0.00015940959409594096,
      "loss": 0.1044,
      "step": 61
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 0.3695494532585144,
      "learning_rate": 0.00015867158671586717,
      "loss": 0.1698,
      "step": 62
    },
    {
      "epoch": 0.4581818181818182,
      "grad_norm": 0.21232406795024872,
      "learning_rate": 0.00015793357933579336,
      "loss": 0.1493,
      "step": 63
    },
    {
      "epoch": 0.46545454545454545,
      "grad_norm": 0.16952396929264069,
      "learning_rate": 0.00015719557195571958,
      "loss": 0.124,
      "step": 64
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.21889926493167877,
      "learning_rate": 0.00015645756457564577,
      "loss": 0.1196,
      "step": 65
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.16851681470870972,
      "learning_rate": 0.00015571955719557195,
      "loss": 0.0764,
      "step": 66
    },
    {
      "epoch": 0.48727272727272725,
      "grad_norm": 0.1661466807126999,
      "learning_rate": 0.00015498154981549817,
      "loss": 0.0868,
      "step": 67
    },
    {
      "epoch": 0.49454545454545457,
      "grad_norm": 0.18160247802734375,
      "learning_rate": 0.00015424354243542436,
      "loss": 0.1139,
      "step": 68
    },
    {
      "epoch": 0.5018181818181818,
      "grad_norm": 0.11264761537313461,
      "learning_rate": 0.00015350553505535057,
      "loss": 0.0603,
      "step": 69
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.19270636141300201,
      "learning_rate": 0.00015276752767527676,
      "loss": 0.1238,
      "step": 70
    },
    {
      "epoch": 0.5163636363636364,
      "grad_norm": 5.421335220336914,
      "learning_rate": 0.00015202952029520298,
      "loss": 0.0544,
      "step": 71
    },
    {
      "epoch": 0.5236363636363637,
      "grad_norm": 0.11250755935907364,
      "learning_rate": 0.00015129151291512916,
      "loss": 0.0591,
      "step": 72
    },
    {
      "epoch": 0.5309090909090909,
      "grad_norm": 0.18058277666568756,
      "learning_rate": 0.00015055350553505535,
      "loss": 0.1076,
      "step": 73
    },
    {
      "epoch": 0.5381818181818182,
      "grad_norm": 0.16449463367462158,
      "learning_rate": 0.00014981549815498154,
      "loss": 0.0918,
      "step": 74
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.1842186003923416,
      "learning_rate": 0.00014907749077490776,
      "loss": 0.0884,
      "step": 75
    },
    {
      "epoch": 0.5527272727272727,
      "grad_norm": 0.16768160462379456,
      "learning_rate": 0.00014833948339483394,
      "loss": 0.0632,
      "step": 76
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.17956115305423737,
      "learning_rate": 0.00014760147601476016,
      "loss": 0.1132,
      "step": 77
    },
    {
      "epoch": 0.5672727272727273,
      "grad_norm": 0.29097679257392883,
      "learning_rate": 0.00014686346863468637,
      "loss": 0.0772,
      "step": 78
    },
    {
      "epoch": 0.5745454545454546,
      "grad_norm": 0.21788793802261353,
      "learning_rate": 0.00014612546125461256,
      "loss": 0.1072,
      "step": 79
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.17365452647209167,
      "learning_rate": 0.00014538745387453875,
      "loss": 0.0614,
      "step": 80
    },
    {
      "epoch": 0.5890909090909091,
      "grad_norm": 1.622758150100708,
      "learning_rate": 0.00014464944649446494,
      "loss": 0.0887,
      "step": 81
    },
    {
      "epoch": 0.5963636363636363,
      "grad_norm": 0.18980848789215088,
      "learning_rate": 0.00014391143911439115,
      "loss": 0.0765,
      "step": 82
    },
    {
      "epoch": 0.6036363636363636,
      "grad_norm": 0.2702271342277527,
      "learning_rate": 0.00014317343173431734,
      "loss": 0.0842,
      "step": 83
    },
    {
      "epoch": 0.610909090909091,
      "grad_norm": 0.2001657634973526,
      "learning_rate": 0.00014243542435424356,
      "loss": 0.0714,
      "step": 84
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.2129906415939331,
      "learning_rate": 0.00014169741697416975,
      "loss": 0.0704,
      "step": 85
    },
    {
      "epoch": 0.6254545454545455,
      "grad_norm": 0.16299211978912354,
      "learning_rate": 0.00014095940959409593,
      "loss": 0.0297,
      "step": 86
    },
    {
      "epoch": 0.6327272727272727,
      "grad_norm": 0.17292669415473938,
      "learning_rate": 0.00014022140221402215,
      "loss": 0.0331,
      "step": 87
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.14801710844039917,
      "learning_rate": 0.00013948339483394834,
      "loss": 0.0725,
      "step": 88
    },
    {
      "epoch": 0.6472727272727272,
      "grad_norm": 0.2445685714483261,
      "learning_rate": 0.00013874538745387455,
      "loss": 0.1965,
      "step": 89
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.13296711444854736,
      "learning_rate": 0.00013800738007380074,
      "loss": 0.0508,
      "step": 90
    },
    {
      "epoch": 0.6618181818181819,
      "grad_norm": 31.45923614501953,
      "learning_rate": 0.00013726937269372696,
      "loss": 0.1291,
      "step": 91
    },
    {
      "epoch": 0.6690909090909091,
      "grad_norm": 0.11425381898880005,
      "learning_rate": 0.00013653136531365315,
      "loss": 0.0453,
      "step": 92
    },
    {
      "epoch": 0.6763636363636364,
      "grad_norm": 0.19240808486938477,
      "learning_rate": 0.00013579335793357933,
      "loss": 0.1134,
      "step": 93
    },
    {
      "epoch": 0.6836363636363636,
      "grad_norm": 0.20556969940662384,
      "learning_rate": 0.00013505535055350552,
      "loss": 0.1309,
      "step": 94
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.18499472737312317,
      "learning_rate": 0.00013431734317343174,
      "loss": 0.1244,
      "step": 95
    },
    {
      "epoch": 0.6981818181818182,
      "grad_norm": 0.32862576842308044,
      "learning_rate": 0.00013357933579335793,
      "loss": 0.1612,
      "step": 96
    },
    {
      "epoch": 0.7054545454545454,
      "grad_norm": 0.1341329663991928,
      "learning_rate": 0.00013284132841328414,
      "loss": 0.0732,
      "step": 97
    },
    {
      "epoch": 0.7127272727272728,
      "grad_norm": 0.18255312740802765,
      "learning_rate": 0.00013210332103321036,
      "loss": 0.0721,
      "step": 98
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1589641124010086,
      "learning_rate": 0.00013136531365313654,
      "loss": 0.0926,
      "step": 99
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.1393091380596161,
      "learning_rate": 0.00013062730627306273,
      "loss": 0.0763,
      "step": 100
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.18708284199237823,
      "learning_rate": 0.00012988929889298892,
      "loss": 0.1031,
      "step": 101
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.26321491599082947,
      "learning_rate": 0.00012915129151291514,
      "loss": 0.1739,
      "step": 102
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.13389433920383453,
      "learning_rate": 0.00012841328413284132,
      "loss": 0.0613,
      "step": 103
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.24436788260936737,
      "learning_rate": 0.00012767527675276754,
      "loss": 0.1013,
      "step": 104
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.20693838596343994,
      "learning_rate": 0.00012693726937269373,
      "loss": 0.1017,
      "step": 105
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.16350512206554413,
      "learning_rate": 0.00012619926199261994,
      "loss": 0.0899,
      "step": 106
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.19771508872509003,
      "learning_rate": 0.00012546125461254613,
      "loss": 0.052,
      "step": 107
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.14246965944766998,
      "learning_rate": 0.00012472324723247232,
      "loss": 0.0244,
      "step": 108
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 3.5660581588745117,
      "learning_rate": 0.00012398523985239854,
      "loss": 0.05,
      "step": 109
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1747637540102005,
      "learning_rate": 0.00012324723247232472,
      "loss": 0.0831,
      "step": 110
    },
    {
      "epoch": 0.8072727272727273,
      "grad_norm": 0.15025757253170013,
      "learning_rate": 0.00012250922509225094,
      "loss": 0.0793,
      "step": 111
    },
    {
      "epoch": 0.8145454545454546,
      "grad_norm": 0.2330658882856369,
      "learning_rate": 0.00012177121771217713,
      "loss": 0.1228,
      "step": 112
    },
    {
      "epoch": 0.8218181818181818,
      "grad_norm": 0.1885075867176056,
      "learning_rate": 0.00012103321033210333,
      "loss": 0.0624,
      "step": 113
    },
    {
      "epoch": 0.8290909090909091,
      "grad_norm": 0.14794181287288666,
      "learning_rate": 0.00012029520295202952,
      "loss": 0.079,
      "step": 114
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 10.217923164367676,
      "learning_rate": 0.00011955719557195573,
      "loss": 0.0718,
      "step": 115
    },
    {
      "epoch": 0.8436363636363636,
      "grad_norm": 0.1118142157793045,
      "learning_rate": 0.00011881918819188192,
      "loss": 0.0326,
      "step": 116
    },
    {
      "epoch": 0.850909090909091,
      "grad_norm": 0.18137548863887787,
      "learning_rate": 0.00011808118081180812,
      "loss": 0.1034,
      "step": 117
    },
    {
      "epoch": 0.8581818181818182,
      "grad_norm": 0.19458316266536713,
      "learning_rate": 0.00011734317343173434,
      "loss": 0.1003,
      "step": 118
    },
    {
      "epoch": 0.8654545454545455,
      "grad_norm": 0.28680846095085144,
      "learning_rate": 0.00011660516605166053,
      "loss": 0.0944,
      "step": 119
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.13303983211517334,
      "learning_rate": 0.00011586715867158673,
      "loss": 0.0594,
      "step": 120
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.14434462785720825,
      "learning_rate": 0.00011512915129151292,
      "loss": 0.06,
      "step": 121
    },
    {
      "epoch": 0.8872727272727273,
      "grad_norm": 0.11251695454120636,
      "learning_rate": 0.00011439114391143913,
      "loss": 0.0311,
      "step": 122
    },
    {
      "epoch": 0.8945454545454545,
      "grad_norm": 0.21232767403125763,
      "learning_rate": 0.0001136531365313653,
      "loss": 0.0703,
      "step": 123
    },
    {
      "epoch": 0.9018181818181819,
      "grad_norm": 0.20267228782176971,
      "learning_rate": 0.00011291512915129152,
      "loss": 0.178,
      "step": 124
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.15453596413135529,
      "learning_rate": 0.00011217712177121771,
      "loss": 0.0567,
      "step": 125
    },
    {
      "epoch": 0.9163636363636364,
      "grad_norm": 0.1768776923418045,
      "learning_rate": 0.00011143911439114391,
      "loss": 0.0988,
      "step": 126
    },
    {
      "epoch": 0.9236363636363636,
      "grad_norm": 0.12089107185602188,
      "learning_rate": 0.00011070110701107013,
      "loss": 0.0501,
      "step": 127
    },
    {
      "epoch": 0.9309090909090909,
      "grad_norm": 0.10307525098323822,
      "learning_rate": 0.00010996309963099631,
      "loss": 0.0358,
      "step": 128
    },
    {
      "epoch": 0.9381818181818182,
      "grad_norm": 3.2586967945098877,
      "learning_rate": 0.00010922509225092252,
      "loss": 0.0213,
      "step": 129
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.21678398549556732,
      "learning_rate": 0.0001084870848708487,
      "loss": 0.1404,
      "step": 130
    },
    {
      "epoch": 0.9527272727272728,
      "grad_norm": 0.26864615082740784,
      "learning_rate": 0.00010774907749077492,
      "loss": 0.113,
      "step": 131
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.15786607563495636,
      "learning_rate": 0.00010701107011070111,
      "loss": 0.0732,
      "step": 132
    },
    {
      "epoch": 0.9672727272727273,
      "grad_norm": 12.152109146118164,
      "learning_rate": 0.00010627306273062731,
      "loss": 0.0872,
      "step": 133
    },
    {
      "epoch": 0.9745454545454545,
      "grad_norm": 0.18570834398269653,
      "learning_rate": 0.0001055350553505535,
      "loss": 0.0881,
      "step": 134
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 2.1034483909606934,
      "learning_rate": 0.00010479704797047971,
      "loss": 0.0689,
      "step": 135
    },
    {
      "epoch": 0.9890909090909091,
      "grad_norm": 0.16195344924926758,
      "learning_rate": 0.0001040590405904059,
      "loss": 0.0245,
      "step": 136
    },
    {
      "epoch": 0.9963636363636363,
      "grad_norm": 0.28909656405448914,
      "learning_rate": 0.0001033210332103321,
      "loss": 0.1154,
      "step": 137
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.27818331122398376,
      "learning_rate": 0.00010258302583025832,
      "loss": 0.0581,
      "step": 138
    },
    {
      "epoch": 1.0072727272727273,
      "grad_norm": 0.2842140793800354,
      "learning_rate": 0.00010184501845018451,
      "loss": 0.084,
      "step": 139
    },
    {
      "epoch": 1.0145454545454546,
      "grad_norm": 0.3096645474433899,
      "learning_rate": 0.00010110701107011071,
      "loss": 0.0387,
      "step": 140
    },
    {
      "epoch": 1.0218181818181817,
      "grad_norm": 16.239879608154297,
      "learning_rate": 0.0001003690036900369,
      "loss": 0.0546,
      "step": 141
    },
    {
      "epoch": 1.029090909090909,
      "grad_norm": 0.28860461711883545,
      "learning_rate": 9.963099630996311e-05,
      "loss": 0.0957,
      "step": 142
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.3182019591331482,
      "learning_rate": 9.889298892988931e-05,
      "loss": 0.0682,
      "step": 143
    },
    {
      "epoch": 1.0436363636363637,
      "grad_norm": 0.31642723083496094,
      "learning_rate": 9.81549815498155e-05,
      "loss": 0.0544,
      "step": 144
    },
    {
      "epoch": 1.050909090909091,
      "grad_norm": 0.257764995098114,
      "learning_rate": 9.74169741697417e-05,
      "loss": 0.1055,
      "step": 145
    },
    {
      "epoch": 1.0581818181818181,
      "grad_norm": 0.21674565970897675,
      "learning_rate": 9.66789667896679e-05,
      "loss": 0.0798,
      "step": 146
    },
    {
      "epoch": 1.0654545454545454,
      "grad_norm": 0.2171114683151245,
      "learning_rate": 9.59409594095941e-05,
      "loss": 0.0523,
      "step": 147
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 6.899311542510986,
      "learning_rate": 9.52029520295203e-05,
      "loss": 0.0263,
      "step": 148
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.30840447545051575,
      "learning_rate": 9.44649446494465e-05,
      "loss": 0.1848,
      "step": 149
    },
    {
      "epoch": 1.0872727272727274,
      "grad_norm": 0.2006741315126419,
      "learning_rate": 9.37269372693727e-05,
      "loss": 0.0475,
      "step": 150
    },
    {
      "epoch": 1.0945454545454545,
      "grad_norm": 0.2582131326198578,
      "learning_rate": 9.298892988929889e-05,
      "loss": 0.1423,
      "step": 151
    },
    {
      "epoch": 1.1018181818181818,
      "grad_norm": 0.22082355618476868,
      "learning_rate": 9.22509225092251e-05,
      "loss": 0.0906,
      "step": 152
    },
    {
      "epoch": 1.1090909090909091,
      "grad_norm": 0.18590448796749115,
      "learning_rate": 9.15129151291513e-05,
      "loss": 0.0692,
      "step": 153
    },
    {
      "epoch": 1.1163636363636364,
      "grad_norm": 0.19542378187179565,
      "learning_rate": 9.077490774907749e-05,
      "loss": 0.0307,
      "step": 154
    },
    {
      "epoch": 1.1236363636363635,
      "grad_norm": 0.9778691530227661,
      "learning_rate": 9.00369003690037e-05,
      "loss": 0.0663,
      "step": 155
    },
    {
      "epoch": 1.1309090909090909,
      "grad_norm": 3.9481022357940674,
      "learning_rate": 8.92988929889299e-05,
      "loss": 0.0213,
      "step": 156
    },
    {
      "epoch": 1.1381818181818182,
      "grad_norm": 0.24373558163642883,
      "learning_rate": 8.85608856088561e-05,
      "loss": 0.0718,
      "step": 157
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 2.5368874073028564,
      "learning_rate": 8.782287822878229e-05,
      "loss": 0.063,
      "step": 158
    },
    {
      "epoch": 1.1527272727272728,
      "grad_norm": 0.1966751217842102,
      "learning_rate": 8.708487084870849e-05,
      "loss": 0.071,
      "step": 159
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.20622941851615906,
      "learning_rate": 8.634686346863469e-05,
      "loss": 0.0674,
      "step": 160
    },
    {
      "epoch": 1.1672727272727272,
      "grad_norm": 0.31927719712257385,
      "learning_rate": 8.560885608856088e-05,
      "loss": 0.1207,
      "step": 161
    },
    {
      "epoch": 1.1745454545454546,
      "grad_norm": 0.3091299831867218,
      "learning_rate": 8.48708487084871e-05,
      "loss": 0.0929,
      "step": 162
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.33274853229522705,
      "learning_rate": 8.41328413284133e-05,
      "loss": 0.1024,
      "step": 163
    },
    {
      "epoch": 1.189090909090909,
      "grad_norm": 1.3454176187515259,
      "learning_rate": 8.339483394833948e-05,
      "loss": 0.1618,
      "step": 164
    },
    {
      "epoch": 1.1963636363636363,
      "grad_norm": 0.3014000952243805,
      "learning_rate": 8.265682656826569e-05,
      "loss": 0.0705,
      "step": 165
    },
    {
      "epoch": 1.2036363636363636,
      "grad_norm": 6.589539051055908,
      "learning_rate": 8.191881918819189e-05,
      "loss": 0.0931,
      "step": 166
    },
    {
      "epoch": 1.210909090909091,
      "grad_norm": 0.2796325087547302,
      "learning_rate": 8.118081180811809e-05,
      "loss": 0.0628,
      "step": 167
    },
    {
      "epoch": 1.2181818181818183,
      "grad_norm": 0.32063186168670654,
      "learning_rate": 8.044280442804428e-05,
      "loss": 0.0605,
      "step": 168
    },
    {
      "epoch": 1.2254545454545456,
      "grad_norm": 0.2982335388660431,
      "learning_rate": 7.970479704797048e-05,
      "loss": 0.0799,
      "step": 169
    },
    {
      "epoch": 1.2327272727272727,
      "grad_norm": 0.35159051418304443,
      "learning_rate": 7.896678966789668e-05,
      "loss": 0.1404,
      "step": 170
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.4890515804290771,
      "learning_rate": 7.822878228782288e-05,
      "loss": 0.0369,
      "step": 171
    },
    {
      "epoch": 1.2472727272727273,
      "grad_norm": 0.2495502084493637,
      "learning_rate": 7.749077490774908e-05,
      "loss": 0.07,
      "step": 172
    },
    {
      "epoch": 1.2545454545454544,
      "grad_norm": 0.3126187026500702,
      "learning_rate": 7.675276752767529e-05,
      "loss": 0.0995,
      "step": 173
    },
    {
      "epoch": 1.2618181818181817,
      "grad_norm": 0.27930665016174316,
      "learning_rate": 7.601476014760149e-05,
      "loss": 0.0875,
      "step": 174
    },
    {
      "epoch": 1.269090909090909,
      "grad_norm": 0.2565128803253174,
      "learning_rate": 7.527675276752768e-05,
      "loss": 0.0719,
      "step": 175
    },
    {
      "epoch": 1.2763636363636364,
      "grad_norm": 0.2250540554523468,
      "learning_rate": 7.453874538745388e-05,
      "loss": 0.063,
      "step": 176
    },
    {
      "epoch": 1.2836363636363637,
      "grad_norm": 0.20064279437065125,
      "learning_rate": 7.380073800738008e-05,
      "loss": 0.0269,
      "step": 177
    },
    {
      "epoch": 1.290909090909091,
      "grad_norm": 0.1936725378036499,
      "learning_rate": 7.306273062730628e-05,
      "loss": 0.0566,
      "step": 178
    },
    {
      "epoch": 1.298181818181818,
      "grad_norm": 60.55177307128906,
      "learning_rate": 7.232472324723247e-05,
      "loss": 0.0322,
      "step": 179
    },
    {
      "epoch": 1.3054545454545454,
      "grad_norm": 0.22581562399864197,
      "learning_rate": 7.158671586715867e-05,
      "loss": 0.0764,
      "step": 180
    },
    {
      "epoch": 1.3127272727272727,
      "grad_norm": 0.16945528984069824,
      "learning_rate": 7.084870848708487e-05,
      "loss": 0.0385,
      "step": 181
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.1815754175186157,
      "learning_rate": 7.011070110701108e-05,
      "loss": 0.0956,
      "step": 182
    },
    {
      "epoch": 1.3272727272727272,
      "grad_norm": 0.3593220114707947,
      "learning_rate": 6.937269372693728e-05,
      "loss": 0.0905,
      "step": 183
    },
    {
      "epoch": 1.3345454545454545,
      "grad_norm": 0.22860829532146454,
      "learning_rate": 6.863468634686348e-05,
      "loss": 0.0671,
      "step": 184
    },
    {
      "epoch": 1.3418181818181818,
      "grad_norm": 0.19664208590984344,
      "learning_rate": 6.789667896678967e-05,
      "loss": 0.0392,
      "step": 185
    },
    {
      "epoch": 1.3490909090909091,
      "grad_norm": 18.66675567626953,
      "learning_rate": 6.715867158671587e-05,
      "loss": 0.0407,
      "step": 186
    },
    {
      "epoch": 1.3563636363636364,
      "grad_norm": 0.2660326063632965,
      "learning_rate": 6.642066420664207e-05,
      "loss": 0.1201,
      "step": 187
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.27028000354766846,
      "learning_rate": 6.568265682656827e-05,
      "loss": 0.1355,
      "step": 188
    },
    {
      "epoch": 1.3709090909090909,
      "grad_norm": 0.17649942636489868,
      "learning_rate": 6.494464944649446e-05,
      "loss": 0.0257,
      "step": 189
    },
    {
      "epoch": 1.3781818181818182,
      "grad_norm": 0.19975675642490387,
      "learning_rate": 6.420664206642066e-05,
      "loss": 0.0848,
      "step": 190
    },
    {
      "epoch": 1.3854545454545455,
      "grad_norm": 0.20331323146820068,
      "learning_rate": 6.346863468634686e-05,
      "loss": 0.0704,
      "step": 191
    },
    {
      "epoch": 1.3927272727272726,
      "grad_norm": 0.18338856101036072,
      "learning_rate": 6.273062730627307e-05,
      "loss": 0.0654,
      "step": 192
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.2776281237602234,
      "learning_rate": 6.199261992619927e-05,
      "loss": 0.0899,
      "step": 193
    },
    {
      "epoch": 1.4072727272727272,
      "grad_norm": 0.1963641345500946,
      "learning_rate": 6.125461254612547e-05,
      "loss": 0.0757,
      "step": 194
    },
    {
      "epoch": 1.4145454545454546,
      "grad_norm": 0.22008158266544342,
      "learning_rate": 6.0516605166051664e-05,
      "loss": 0.0611,
      "step": 195
    },
    {
      "epoch": 1.4218181818181819,
      "grad_norm": 0.29287928342819214,
      "learning_rate": 5.9778597785977866e-05,
      "loss": 0.0643,
      "step": 196
    },
    {
      "epoch": 1.4290909090909092,
      "grad_norm": 0.2204040288925171,
      "learning_rate": 5.904059040590406e-05,
      "loss": 0.0451,
      "step": 197
    },
    {
      "epoch": 1.4363636363636363,
      "grad_norm": 0.2895601689815521,
      "learning_rate": 5.830258302583026e-05,
      "loss": 0.1115,
      "step": 198
    },
    {
      "epoch": 1.4436363636363636,
      "grad_norm": 0.14693088829517365,
      "learning_rate": 5.756457564575646e-05,
      "loss": 0.0407,
      "step": 199
    },
    {
      "epoch": 1.450909090909091,
      "grad_norm": 0.19387979805469513,
      "learning_rate": 5.682656826568265e-05,
      "loss": 0.0559,
      "step": 200
    },
    {
      "epoch": 1.4581818181818182,
      "grad_norm": 0.36853745579719543,
      "learning_rate": 5.6088560885608855e-05,
      "loss": 0.1112,
      "step": 201
    },
    {
      "epoch": 1.4654545454545453,
      "grad_norm": 0.3001222312450409,
      "learning_rate": 5.535055350553506e-05,
      "loss": 0.1259,
      "step": 202
    },
    {
      "epoch": 1.4727272727272727,
      "grad_norm": 17.521526336669922,
      "learning_rate": 5.461254612546126e-05,
      "loss": 0.0589,
      "step": 203
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.18705137073993683,
      "learning_rate": 5.387453874538746e-05,
      "loss": 0.0498,
      "step": 204
    },
    {
      "epoch": 1.4872727272727273,
      "grad_norm": 0.2566661834716797,
      "learning_rate": 5.3136531365313655e-05,
      "loss": 0.0629,
      "step": 205
    },
    {
      "epoch": 1.4945454545454546,
      "grad_norm": 0.167724609375,
      "learning_rate": 5.239852398523986e-05,
      "loss": 0.0364,
      "step": 206
    },
    {
      "epoch": 1.501818181818182,
      "grad_norm": 0.16270428895950317,
      "learning_rate": 5.166051660516605e-05,
      "loss": 0.0542,
      "step": 207
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.24680288136005402,
      "learning_rate": 5.0922509225092254e-05,
      "loss": 0.0528,
      "step": 208
    },
    {
      "epoch": 1.5163636363636364,
      "grad_norm": 106.7393569946289,
      "learning_rate": 5.018450184501845e-05,
      "loss": 0.0867,
      "step": 209
    },
    {
      "epoch": 1.5236363636363637,
      "grad_norm": 0.2952440679073334,
      "learning_rate": 4.944649446494466e-05,
      "loss": 0.0841,
      "step": 210
    },
    {
      "epoch": 1.5309090909090908,
      "grad_norm": 0.20078758895397186,
      "learning_rate": 4.870848708487085e-05,
      "loss": 0.0605,
      "step": 211
    },
    {
      "epoch": 1.538181818181818,
      "grad_norm": 0.23668760061264038,
      "learning_rate": 4.797047970479705e-05,
      "loss": 0.0606,
      "step": 212
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.206594318151474,
      "learning_rate": 4.723247232472325e-05,
      "loss": 0.0477,
      "step": 213
    },
    {
      "epoch": 1.5527272727272727,
      "grad_norm": 27.19643211364746,
      "learning_rate": 4.6494464944649444e-05,
      "loss": 0.0504,
      "step": 214
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.16169126331806183,
      "learning_rate": 4.575645756457565e-05,
      "loss": 0.0584,
      "step": 215
    },
    {
      "epoch": 1.5672727272727274,
      "grad_norm": 0.3686952292919159,
      "learning_rate": 4.501845018450185e-05,
      "loss": 0.1176,
      "step": 216
    },
    {
      "epoch": 1.5745454545454547,
      "grad_norm": 0.1832357794046402,
      "learning_rate": 4.428044280442805e-05,
      "loss": 0.0843,
      "step": 217
    },
    {
      "epoch": 1.5818181818181818,
      "grad_norm": 0.16484767198562622,
      "learning_rate": 4.3542435424354244e-05,
      "loss": 0.0562,
      "step": 218
    },
    {
      "epoch": 1.589090909090909,
      "grad_norm": 0.20575366914272308,
      "learning_rate": 4.280442804428044e-05,
      "loss": 0.0939,
      "step": 219
    },
    {
      "epoch": 1.5963636363636362,
      "grad_norm": 0.1444646120071411,
      "learning_rate": 4.206642066420665e-05,
      "loss": 0.0439,
      "step": 220
    },
    {
      "epoch": 1.6036363636363635,
      "grad_norm": 0.11925865709781647,
      "learning_rate": 4.132841328413284e-05,
      "loss": 0.0312,
      "step": 221
    },
    {
      "epoch": 1.6109090909090908,
      "grad_norm": 0.16423381865024567,
      "learning_rate": 4.0590405904059045e-05,
      "loss": 0.0329,
      "step": 222
    },
    {
      "epoch": 1.6181818181818182,
      "grad_norm": 0.19606071710586548,
      "learning_rate": 3.985239852398524e-05,
      "loss": 0.0828,
      "step": 223
    },
    {
      "epoch": 1.6254545454545455,
      "grad_norm": 0.18227291107177734,
      "learning_rate": 3.911439114391144e-05,
      "loss": 0.0896,
      "step": 224
    },
    {
      "epoch": 1.6327272727272728,
      "grad_norm": 0.1948312371969223,
      "learning_rate": 3.837638376383764e-05,
      "loss": 0.0519,
      "step": 225
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.1453128606081009,
      "learning_rate": 3.763837638376384e-05,
      "loss": 0.0385,
      "step": 226
    },
    {
      "epoch": 1.6472727272727272,
      "grad_norm": 0.1424703449010849,
      "learning_rate": 3.690036900369004e-05,
      "loss": 0.0576,
      "step": 227
    },
    {
      "epoch": 1.6545454545454545,
      "grad_norm": 27.31722640991211,
      "learning_rate": 3.6162361623616235e-05,
      "loss": 0.0233,
      "step": 228
    },
    {
      "epoch": 1.6618181818181819,
      "grad_norm": 0.28261756896972656,
      "learning_rate": 3.542435424354244e-05,
      "loss": 0.1123,
      "step": 229
    },
    {
      "epoch": 1.669090909090909,
      "grad_norm": 0.17665013670921326,
      "learning_rate": 3.468634686346864e-05,
      "loss": 0.0329,
      "step": 230
    },
    {
      "epoch": 1.6763636363636363,
      "grad_norm": 50.82008361816406,
      "learning_rate": 3.3948339483394833e-05,
      "loss": 0.0108,
      "step": 231
    },
    {
      "epoch": 1.6836363636363636,
      "grad_norm": 0.18455766141414642,
      "learning_rate": 3.3210332103321035e-05,
      "loss": 0.1219,
      "step": 232
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.286193311214447,
      "learning_rate": 3.247232472324723e-05,
      "loss": 0.0781,
      "step": 233
    },
    {
      "epoch": 1.6981818181818182,
      "grad_norm": 2.081183910369873,
      "learning_rate": 3.173431734317343e-05,
      "loss": 0.0623,
      "step": 234
    },
    {
      "epoch": 1.7054545454545456,
      "grad_norm": 0.17622168362140656,
      "learning_rate": 3.0996309963099634e-05,
      "loss": 0.0761,
      "step": 235
    },
    {
      "epoch": 1.7127272727272729,
      "grad_norm": 0.21056142449378967,
      "learning_rate": 3.0258302583025832e-05,
      "loss": 0.11,
      "step": 236
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.2261078953742981,
      "learning_rate": 2.952029520295203e-05,
      "loss": 0.1188,
      "step": 237
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.30876731872558594,
      "learning_rate": 2.878228782287823e-05,
      "loss": 0.1334,
      "step": 238
    },
    {
      "epoch": 1.7345454545454544,
      "grad_norm": 0.3174894452095032,
      "learning_rate": 2.8044280442804427e-05,
      "loss": 0.0751,
      "step": 239
    },
    {
      "epoch": 1.7418181818181817,
      "grad_norm": 0.20830222964286804,
      "learning_rate": 2.730627306273063e-05,
      "loss": 0.0749,
      "step": 240
    },
    {
      "epoch": 1.749090909090909,
      "grad_norm": 0.17501622438430786,
      "learning_rate": 2.6568265682656828e-05,
      "loss": 0.0468,
      "step": 241
    },
    {
      "epoch": 1.7563636363636363,
      "grad_norm": 22.9272403717041,
      "learning_rate": 2.5830258302583026e-05,
      "loss": 0.074,
      "step": 242
    },
    {
      "epoch": 1.7636363636363637,
      "grad_norm": 0.20550084114074707,
      "learning_rate": 2.5092250922509224e-05,
      "loss": 0.0683,
      "step": 243
    },
    {
      "epoch": 1.770909090909091,
      "grad_norm": 13.073174476623535,
      "learning_rate": 2.4354243542435426e-05,
      "loss": 0.044,
      "step": 244
    },
    {
      "epoch": 1.7781818181818183,
      "grad_norm": 6.002513885498047,
      "learning_rate": 2.3616236162361624e-05,
      "loss": 0.0543,
      "step": 245
    },
    {
      "epoch": 1.7854545454545454,
      "grad_norm": 0.20209453999996185,
      "learning_rate": 2.2878228782287826e-05,
      "loss": 0.0392,
      "step": 246
    },
    {
      "epoch": 1.7927272727272727,
      "grad_norm": 5.517965316772461,
      "learning_rate": 2.2140221402214025e-05,
      "loss": 0.0862,
      "step": 247
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.4277420938014984,
      "learning_rate": 2.140221402214022e-05,
      "loss": 0.0214,
      "step": 248
    },
    {
      "epoch": 1.8072727272727271,
      "grad_norm": 0.24777650833129883,
      "learning_rate": 2.066420664206642e-05,
      "loss": 0.1346,
      "step": 249
    },
    {
      "epoch": 1.8145454545454545,
      "grad_norm": 0.27193576097488403,
      "learning_rate": 1.992619926199262e-05,
      "loss": 0.06,
      "step": 250
    },
    {
      "epoch": 1.8218181818181818,
      "grad_norm": 0.2186596393585205,
      "learning_rate": 1.918819188191882e-05,
      "loss": 0.0421,
      "step": 251
    },
    {
      "epoch": 1.829090909090909,
      "grad_norm": 0.4207431972026825,
      "learning_rate": 1.845018450184502e-05,
      "loss": 0.1896,
      "step": 252
    },
    {
      "epoch": 1.8363636363636364,
      "grad_norm": 0.2727193236351013,
      "learning_rate": 1.771217712177122e-05,
      "loss": 0.0668,
      "step": 253
    },
    {
      "epoch": 1.8436363636363637,
      "grad_norm": 0.28311946988105774,
      "learning_rate": 1.6974169741697417e-05,
      "loss": 0.1084,
      "step": 254
    },
    {
      "epoch": 1.850909090909091,
      "grad_norm": 0.3079265356063843,
      "learning_rate": 1.6236162361623615e-05,
      "loss": 0.0919,
      "step": 255
    },
    {
      "epoch": 1.8581818181818182,
      "grad_norm": 0.26461631059646606,
      "learning_rate": 1.5498154981549817e-05,
      "loss": 0.0717,
      "step": 256
    },
    {
      "epoch": 1.8654545454545455,
      "grad_norm": 0.2598989009857178,
      "learning_rate": 1.4760147601476015e-05,
      "loss": 0.0549,
      "step": 257
    },
    {
      "epoch": 1.8727272727272726,
      "grad_norm": 0.24650171399116516,
      "learning_rate": 1.4022140221402214e-05,
      "loss": 0.047,
      "step": 258
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.237873375415802,
      "learning_rate": 1.3284132841328414e-05,
      "loss": 0.0277,
      "step": 259
    },
    {
      "epoch": 1.8872727272727272,
      "grad_norm": 2.7281506061553955,
      "learning_rate": 1.2546125461254612e-05,
      "loss": 0.0877,
      "step": 260
    },
    {
      "epoch": 1.8945454545454545,
      "grad_norm": 0.2873205244541168,
      "learning_rate": 1.1808118081180812e-05,
      "loss": 0.1201,
      "step": 261
    },
    {
      "epoch": 1.9018181818181819,
      "grad_norm": 108.2063980102539,
      "learning_rate": 1.1070110701107012e-05,
      "loss": -0.0391,
      "step": 262
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.2730659544467926,
      "learning_rate": 1.033210332103321e-05,
      "loss": 0.066,
      "step": 263
    },
    {
      "epoch": 1.9163636363636365,
      "grad_norm": 0.3322694003582001,
      "learning_rate": 9.59409594095941e-06,
      "loss": 0.1071,
      "step": 264
    },
    {
      "epoch": 1.9236363636363636,
      "grad_norm": 0.2243436872959137,
      "learning_rate": 8.85608856088561e-06,
      "loss": 0.061,
      "step": 265
    },
    {
      "epoch": 1.930909090909091,
      "grad_norm": 0.23541520535945892,
      "learning_rate": 8.118081180811808e-06,
      "loss": 0.0417,
      "step": 266
    },
    {
      "epoch": 1.9381818181818182,
      "grad_norm": 0.3062206208705902,
      "learning_rate": 7.380073800738008e-06,
      "loss": 0.0355,
      "step": 267
    },
    {
      "epoch": 1.9454545454545453,
      "grad_norm": 2.571284294128418,
      "learning_rate": 6.642066420664207e-06,
      "loss": 0.1569,
      "step": 268
    },
    {
      "epoch": 1.9527272727272726,
      "grad_norm": 0.21017251908779144,
      "learning_rate": 5.904059040590406e-06,
      "loss": 0.0353,
      "step": 269
    },
    {
      "epoch": 1.96,
      "grad_norm": 6.671807765960693,
      "learning_rate": 5.166051660516605e-06,
      "loss": 0.0516,
      "step": 270
    },
    {
      "epoch": 1.9672727272727273,
      "grad_norm": 0.3296937048435211,
      "learning_rate": 4.428044280442805e-06,
      "loss": 0.0987,
      "step": 271
    },
    {
      "epoch": 1.9745454545454546,
      "grad_norm": 9.00086784362793,
      "learning_rate": 3.690036900369004e-06,
      "loss": 0.0382,
      "step": 272
    },
    {
      "epoch": 1.981818181818182,
      "grad_norm": 0.299243301153183,
      "learning_rate": 2.952029520295203e-06,
      "loss": 0.116,
      "step": 273
    },
    {
      "epoch": 1.9890909090909092,
      "grad_norm": 0.3207961618900299,
      "learning_rate": 2.2140221402214023e-06,
      "loss": 0.0798,
      "step": 274
    },
    {
      "epoch": 1.9963636363636363,
      "grad_norm": 0.226903036236763,
      "learning_rate": 1.4760147601476015e-06,
      "loss": 0.0737,
      "step": 275
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2944619655609131,
      "learning_rate": 7.380073800738008e-07,
      "loss": 0.0554,
      "step": 276
    }
  ],
  "logging_steps": 1,
  "max_steps": 276,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.077139494109184e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
