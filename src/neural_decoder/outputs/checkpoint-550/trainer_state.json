{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 50,
  "global_step": 550,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 2.0869061946868896,
      "learning_rate": 0.0,
      "loss": 0.3688,
      "step": 1
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 2.6834609508514404,
      "learning_rate": 4e-05,
      "loss": 0.3445,
      "step": 2
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 2.2297093868255615,
      "learning_rate": 8e-05,
      "loss": 0.2358,
      "step": 3
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 1.9685579538345337,
      "learning_rate": 0.00012,
      "loss": 0.2445,
      "step": 4
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.4767664670944214,
      "learning_rate": 0.00016,
      "loss": 0.1293,
      "step": 5
    },
    {
      "epoch": 0.01090909090909091,
      "grad_norm": 0.40503543615341187,
      "learning_rate": 0.0002,
      "loss": 0.1143,
      "step": 6
    },
    {
      "epoch": 0.012727272727272728,
      "grad_norm": 0.6441239714622498,
      "learning_rate": 0.0001996330275229358,
      "loss": 0.2103,
      "step": 7
    },
    {
      "epoch": 0.014545454545454545,
      "grad_norm": 0.32398363947868347,
      "learning_rate": 0.00019926605504587156,
      "loss": 0.083,
      "step": 8
    },
    {
      "epoch": 0.016363636363636365,
      "grad_norm": 0.4104377031326294,
      "learning_rate": 0.00019889908256880736,
      "loss": 0.1147,
      "step": 9
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.345976859331131,
      "learning_rate": 0.00019853211009174314,
      "loss": 0.0915,
      "step": 10
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.7613843679428101,
      "learning_rate": 0.0001981651376146789,
      "loss": 0.0994,
      "step": 11
    },
    {
      "epoch": 0.02181818181818182,
      "grad_norm": 0.31534260511398315,
      "learning_rate": 0.0001977981651376147,
      "loss": 0.0658,
      "step": 12
    },
    {
      "epoch": 0.023636363636363636,
      "grad_norm": 0.19248786568641663,
      "learning_rate": 0.0001974311926605505,
      "loss": 0.0381,
      "step": 13
    },
    {
      "epoch": 0.025454545454545455,
      "grad_norm": 0.29620349407196045,
      "learning_rate": 0.00019706422018348624,
      "loss": 0.0451,
      "step": 14
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 0.375815212726593,
      "learning_rate": 0.00019669724770642204,
      "loss": 0.1324,
      "step": 15
    },
    {
      "epoch": 0.02909090909090909,
      "grad_norm": 0.2894703447818756,
      "learning_rate": 0.00019633027522935782,
      "loss": 0.1193,
      "step": 16
    },
    {
      "epoch": 0.03090909090909091,
      "grad_norm": 0.33762264251708984,
      "learning_rate": 0.0001959633027522936,
      "loss": 0.0709,
      "step": 17
    },
    {
      "epoch": 0.03272727272727273,
      "grad_norm": 0.31585726141929626,
      "learning_rate": 0.00019559633027522937,
      "loss": 0.0758,
      "step": 18
    },
    {
      "epoch": 0.034545454545454546,
      "grad_norm": 0.3653563857078552,
      "learning_rate": 0.00019522935779816517,
      "loss": 0.0765,
      "step": 19
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.3419817090034485,
      "learning_rate": 0.00019486238532110092,
      "loss": 0.0584,
      "step": 20
    },
    {
      "epoch": 0.038181818181818185,
      "grad_norm": 0.3716219961643219,
      "learning_rate": 0.00019449541284403672,
      "loss": 0.0378,
      "step": 21
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.3473185896873474,
      "learning_rate": 0.0001941284403669725,
      "loss": 0.0644,
      "step": 22
    },
    {
      "epoch": 0.04181818181818182,
      "grad_norm": 0.3160959780216217,
      "learning_rate": 0.00019376146788990827,
      "loss": 0.0633,
      "step": 23
    },
    {
      "epoch": 0.04363636363636364,
      "grad_norm": 0.18515537679195404,
      "learning_rate": 0.00019339449541284405,
      "loss": 0.0339,
      "step": 24
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.33434492349624634,
      "learning_rate": 0.00019302752293577982,
      "loss": 0.1094,
      "step": 25
    },
    {
      "epoch": 0.04727272727272727,
      "grad_norm": 0.4507327377796173,
      "learning_rate": 0.0001926605504587156,
      "loss": 0.0991,
      "step": 26
    },
    {
      "epoch": 0.04909090909090909,
      "grad_norm": 0.34536468982696533,
      "learning_rate": 0.0001922935779816514,
      "loss": 0.0866,
      "step": 27
    },
    {
      "epoch": 0.05090909090909091,
      "grad_norm": 0.6454206109046936,
      "learning_rate": 0.00019192660550458718,
      "loss": 0.2123,
      "step": 28
    },
    {
      "epoch": 0.05272727272727273,
      "grad_norm": 0.41845160722732544,
      "learning_rate": 0.00019155963302752295,
      "loss": 0.091,
      "step": 29
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.33017948269844055,
      "learning_rate": 0.00019119266055045873,
      "loss": 0.0872,
      "step": 30
    },
    {
      "epoch": 0.056363636363636366,
      "grad_norm": 0.25655391812324524,
      "learning_rate": 0.0001908256880733945,
      "loss": 0.0733,
      "step": 31
    },
    {
      "epoch": 0.05818181818181818,
      "grad_norm": 0.40614551305770874,
      "learning_rate": 0.00019045871559633028,
      "loss": 0.0834,
      "step": 32
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.1643538624048233,
      "learning_rate": 0.00019009174311926608,
      "loss": 0.0224,
      "step": 33
    },
    {
      "epoch": 0.06181818181818182,
      "grad_norm": 0.18973717093467712,
      "learning_rate": 0.00018972477064220186,
      "loss": 0.0397,
      "step": 34
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.2707992494106293,
      "learning_rate": 0.00018935779816513763,
      "loss": 0.0408,
      "step": 35
    },
    {
      "epoch": 0.06545454545454546,
      "grad_norm": 0.23991160094738007,
      "learning_rate": 0.0001889908256880734,
      "loss": 0.051,
      "step": 36
    },
    {
      "epoch": 0.06727272727272728,
      "grad_norm": 0.5553834438323975,
      "learning_rate": 0.00018862385321100918,
      "loss": 0.1568,
      "step": 37
    },
    {
      "epoch": 0.06909090909090909,
      "grad_norm": 0.2763439118862152,
      "learning_rate": 0.00018825688073394496,
      "loss": 0.0785,
      "step": 38
    },
    {
      "epoch": 0.07090909090909091,
      "grad_norm": 0.2927410304546356,
      "learning_rate": 0.00018788990825688076,
      "loss": 0.0475,
      "step": 39
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.26740092039108276,
      "learning_rate": 0.00018752293577981653,
      "loss": 0.0533,
      "step": 40
    },
    {
      "epoch": 0.07454545454545454,
      "grad_norm": 0.19216686487197876,
      "learning_rate": 0.0001871559633027523,
      "loss": 0.0379,
      "step": 41
    },
    {
      "epoch": 0.07636363636363637,
      "grad_norm": 0.22445735335350037,
      "learning_rate": 0.00018678899082568809,
      "loss": 0.0404,
      "step": 42
    },
    {
      "epoch": 0.07818181818181819,
      "grad_norm": 0.19338995218276978,
      "learning_rate": 0.00018642201834862386,
      "loss": 0.0417,
      "step": 43
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.21048107743263245,
      "learning_rate": 0.00018605504587155964,
      "loss": 0.0217,
      "step": 44
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.19640257954597473,
      "learning_rate": 0.00018568807339449544,
      "loss": 0.028,
      "step": 45
    },
    {
      "epoch": 0.08363636363636363,
      "grad_norm": 0.2928834557533264,
      "learning_rate": 0.00018532110091743121,
      "loss": 0.047,
      "step": 46
    },
    {
      "epoch": 0.08545454545454545,
      "grad_norm": 0.4213186800479889,
      "learning_rate": 0.000184954128440367,
      "loss": 0.1178,
      "step": 47
    },
    {
      "epoch": 0.08727272727272728,
      "grad_norm": 0.32410702109336853,
      "learning_rate": 0.00018458715596330276,
      "loss": 0.0854,
      "step": 48
    },
    {
      "epoch": 0.0890909090909091,
      "grad_norm": 0.5060970187187195,
      "learning_rate": 0.00018422018348623854,
      "loss": 0.1693,
      "step": 49
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.21000853180885315,
      "learning_rate": 0.00018385321100917432,
      "loss": 0.0534,
      "step": 50
    },
    {
      "epoch": 0.09272727272727273,
      "grad_norm": 0.17272070050239563,
      "learning_rate": 0.00018348623853211012,
      "loss": 0.0345,
      "step": 51
    },
    {
      "epoch": 0.09454545454545454,
      "grad_norm": 0.18573574721813202,
      "learning_rate": 0.00018311926605504587,
      "loss": 0.0331,
      "step": 52
    },
    {
      "epoch": 0.09636363636363636,
      "grad_norm": 0.27810192108154297,
      "learning_rate": 0.00018275229357798167,
      "loss": 0.1122,
      "step": 53
    },
    {
      "epoch": 0.09818181818181818,
      "grad_norm": 0.27994105219841003,
      "learning_rate": 0.00018238532110091744,
      "loss": 0.0824,
      "step": 54
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.443622350692749,
      "learning_rate": 0.00018201834862385322,
      "loss": 0.1211,
      "step": 55
    },
    {
      "epoch": 0.10181818181818182,
      "grad_norm": 0.19131623208522797,
      "learning_rate": 0.000181651376146789,
      "loss": 0.0418,
      "step": 56
    },
    {
      "epoch": 0.10363636363636364,
      "grad_norm": 0.20418068766593933,
      "learning_rate": 0.0001812844036697248,
      "loss": 0.0526,
      "step": 57
    },
    {
      "epoch": 0.10545454545454545,
      "grad_norm": 0.1769232600927353,
      "learning_rate": 0.00018091743119266055,
      "loss": 0.0251,
      "step": 58
    },
    {
      "epoch": 0.10727272727272727,
      "grad_norm": 0.13870593905448914,
      "learning_rate": 0.00018055045871559635,
      "loss": 0.0295,
      "step": 59
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.30556154251098633,
      "learning_rate": 0.00018018348623853212,
      "loss": 0.0731,
      "step": 60
    },
    {
      "epoch": 0.11090909090909092,
      "grad_norm": 0.2005482167005539,
      "learning_rate": 0.0001798165137614679,
      "loss": 0.0288,
      "step": 61
    },
    {
      "epoch": 0.11272727272727273,
      "grad_norm": 0.44977623224258423,
      "learning_rate": 0.00017944954128440367,
      "loss": 0.133,
      "step": 62
    },
    {
      "epoch": 0.11454545454545455,
      "grad_norm": 0.16486051678657532,
      "learning_rate": 0.00017908256880733948,
      "loss": 0.0184,
      "step": 63
    },
    {
      "epoch": 0.11636363636363636,
      "grad_norm": 0.22145166993141174,
      "learning_rate": 0.00017871559633027523,
      "loss": 0.0603,
      "step": 64
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 0.6885732412338257,
      "learning_rate": 0.00017834862385321103,
      "loss": 0.1403,
      "step": 65
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.3044857978820801,
      "learning_rate": 0.0001779816513761468,
      "loss": 0.0827,
      "step": 66
    },
    {
      "epoch": 0.12181818181818181,
      "grad_norm": 0.36428403854370117,
      "learning_rate": 0.00017761467889908258,
      "loss": 0.1272,
      "step": 67
    },
    {
      "epoch": 0.12363636363636364,
      "grad_norm": 0.23941704630851746,
      "learning_rate": 0.00017724770642201835,
      "loss": 0.0564,
      "step": 68
    },
    {
      "epoch": 0.12545454545454546,
      "grad_norm": 0.17875781655311584,
      "learning_rate": 0.00017688073394495416,
      "loss": 0.039,
      "step": 69
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.15405087172985077,
      "learning_rate": 0.0001765137614678899,
      "loss": 0.0177,
      "step": 70
    },
    {
      "epoch": 0.1290909090909091,
      "grad_norm": 0.15403228998184204,
      "learning_rate": 0.0001761467889908257,
      "loss": 0.0266,
      "step": 71
    },
    {
      "epoch": 0.13090909090909092,
      "grad_norm": 0.3792799711227417,
      "learning_rate": 0.00017577981651376148,
      "loss": 0.0809,
      "step": 72
    },
    {
      "epoch": 0.13272727272727272,
      "grad_norm": 0.4156974256038666,
      "learning_rate": 0.00017541284403669726,
      "loss": 0.1481,
      "step": 73
    },
    {
      "epoch": 0.13454545454545455,
      "grad_norm": 0.23599794507026672,
      "learning_rate": 0.00017504587155963303,
      "loss": 0.0425,
      "step": 74
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.24781592190265656,
      "learning_rate": 0.00017467889908256884,
      "loss": 0.0595,
      "step": 75
    },
    {
      "epoch": 0.13818181818181818,
      "grad_norm": 0.192385733127594,
      "learning_rate": 0.00017431192660550458,
      "loss": 0.0289,
      "step": 76
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.230913907289505,
      "learning_rate": 0.00017394495412844039,
      "loss": 0.0798,
      "step": 77
    },
    {
      "epoch": 0.14181818181818182,
      "grad_norm": 0.17412491142749786,
      "learning_rate": 0.00017357798165137616,
      "loss": 0.0324,
      "step": 78
    },
    {
      "epoch": 0.14363636363636365,
      "grad_norm": 0.12514536082744598,
      "learning_rate": 0.00017321100917431194,
      "loss": 0.0226,
      "step": 79
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.16890059411525726,
      "learning_rate": 0.0001728440366972477,
      "loss": 0.0181,
      "step": 80
    },
    {
      "epoch": 0.14727272727272728,
      "grad_norm": 0.18429145216941833,
      "learning_rate": 0.00017247706422018351,
      "loss": 0.0444,
      "step": 81
    },
    {
      "epoch": 0.14909090909090908,
      "grad_norm": 0.3096857964992523,
      "learning_rate": 0.00017211009174311926,
      "loss": 0.0887,
      "step": 82
    },
    {
      "epoch": 0.1509090909090909,
      "grad_norm": 0.30409005284309387,
      "learning_rate": 0.00017174311926605507,
      "loss": 0.0659,
      "step": 83
    },
    {
      "epoch": 0.15272727272727274,
      "grad_norm": 0.31909048557281494,
      "learning_rate": 0.00017137614678899084,
      "loss": 0.1004,
      "step": 84
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.30601418018341064,
      "learning_rate": 0.00017100917431192662,
      "loss": 0.0638,
      "step": 85
    },
    {
      "epoch": 0.15636363636363637,
      "grad_norm": 0.3242199420928955,
      "learning_rate": 0.0001706422018348624,
      "loss": 0.0378,
      "step": 86
    },
    {
      "epoch": 0.15818181818181817,
      "grad_norm": 0.2478545904159546,
      "learning_rate": 0.0001702752293577982,
      "loss": 0.0398,
      "step": 87
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.3039764165878296,
      "learning_rate": 0.00016990825688073394,
      "loss": 0.0828,
      "step": 88
    },
    {
      "epoch": 0.1618181818181818,
      "grad_norm": 0.09244807064533234,
      "learning_rate": 0.00016954128440366974,
      "loss": 0.0089,
      "step": 89
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.20528951287269592,
      "learning_rate": 0.00016917431192660552,
      "loss": 0.0139,
      "step": 90
    },
    {
      "epoch": 0.16545454545454547,
      "grad_norm": 0.218203604221344,
      "learning_rate": 0.0001688073394495413,
      "loss": 0.037,
      "step": 91
    },
    {
      "epoch": 0.16727272727272727,
      "grad_norm": 0.2663852572441101,
      "learning_rate": 0.00016844036697247707,
      "loss": 0.0517,
      "step": 92
    },
    {
      "epoch": 0.1690909090909091,
      "grad_norm": 0.2080344259738922,
      "learning_rate": 0.00016807339449541287,
      "loss": 0.0385,
      "step": 93
    },
    {
      "epoch": 0.1709090909090909,
      "grad_norm": 0.2833997309207916,
      "learning_rate": 0.00016770642201834862,
      "loss": 0.0699,
      "step": 94
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.3992193639278412,
      "learning_rate": 0.00016733944954128442,
      "loss": 0.1188,
      "step": 95
    },
    {
      "epoch": 0.17454545454545456,
      "grad_norm": 0.14206862449645996,
      "learning_rate": 0.0001669724770642202,
      "loss": 0.024,
      "step": 96
    },
    {
      "epoch": 0.17636363636363636,
      "grad_norm": 0.2353571206331253,
      "learning_rate": 0.00016660550458715598,
      "loss": 0.0518,
      "step": 97
    },
    {
      "epoch": 0.1781818181818182,
      "grad_norm": 0.2478833645582199,
      "learning_rate": 0.00016623853211009175,
      "loss": 0.0352,
      "step": 98
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.1590869128704071,
      "learning_rate": 0.00016587155963302755,
      "loss": 0.0263,
      "step": 99
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.21372579038143158,
      "learning_rate": 0.0001655045871559633,
      "loss": 0.0213,
      "step": 100
    },
    {
      "epoch": 0.18363636363636363,
      "grad_norm": 0.19274669885635376,
      "learning_rate": 0.0001651376146788991,
      "loss": 0.0291,
      "step": 101
    },
    {
      "epoch": 0.18545454545454546,
      "grad_norm": 0.08096074312925339,
      "learning_rate": 0.00016477064220183488,
      "loss": 0.0086,
      "step": 102
    },
    {
      "epoch": 0.18727272727272729,
      "grad_norm": 0.445581316947937,
      "learning_rate": 0.00016440366972477065,
      "loss": 0.0618,
      "step": 103
    },
    {
      "epoch": 0.1890909090909091,
      "grad_norm": 0.16287586092948914,
      "learning_rate": 0.00016403669724770643,
      "loss": 0.0061,
      "step": 104
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.20321398973464966,
      "learning_rate": 0.00016366972477064223,
      "loss": 0.028,
      "step": 105
    },
    {
      "epoch": 0.19272727272727272,
      "grad_norm": 0.3386825919151306,
      "learning_rate": 0.00016330275229357798,
      "loss": 0.0768,
      "step": 106
    },
    {
      "epoch": 0.19454545454545455,
      "grad_norm": 0.3784842789173126,
      "learning_rate": 0.00016293577981651378,
      "loss": 0.0869,
      "step": 107
    },
    {
      "epoch": 0.19636363636363635,
      "grad_norm": 0.5484545826911926,
      "learning_rate": 0.00016256880733944956,
      "loss": 0.1582,
      "step": 108
    },
    {
      "epoch": 0.19818181818181818,
      "grad_norm": 0.09672052413225174,
      "learning_rate": 0.00016220183486238533,
      "loss": 0.011,
      "step": 109
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.11172603070735931,
      "learning_rate": 0.0001618348623853211,
      "loss": 0.0067,
      "step": 110
    },
    {
      "epoch": 0.2018181818181818,
      "grad_norm": 0.122329480946064,
      "learning_rate": 0.00016146788990825688,
      "loss": 0.025,
      "step": 111
    },
    {
      "epoch": 0.20363636363636364,
      "grad_norm": 0.19339945912361145,
      "learning_rate": 0.00016110091743119266,
      "loss": 0.0466,
      "step": 112
    },
    {
      "epoch": 0.20545454545454545,
      "grad_norm": 0.07514356821775436,
      "learning_rate": 0.00016073394495412846,
      "loss": 0.006,
      "step": 113
    },
    {
      "epoch": 0.20727272727272728,
      "grad_norm": 0.3539903461933136,
      "learning_rate": 0.00016036697247706424,
      "loss": 0.085,
      "step": 114
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.41104811429977417,
      "learning_rate": 0.00016,
      "loss": 0.0945,
      "step": 115
    },
    {
      "epoch": 0.2109090909090909,
      "grad_norm": 0.16618400812149048,
      "learning_rate": 0.0001596330275229358,
      "loss": 0.0326,
      "step": 116
    },
    {
      "epoch": 0.21272727272727274,
      "grad_norm": 0.35953792929649353,
      "learning_rate": 0.00015926605504587156,
      "loss": 0.078,
      "step": 117
    },
    {
      "epoch": 0.21454545454545454,
      "grad_norm": 0.28031599521636963,
      "learning_rate": 0.00015889908256880734,
      "loss": 0.0553,
      "step": 118
    },
    {
      "epoch": 0.21636363636363637,
      "grad_norm": 0.5716135501861572,
      "learning_rate": 0.00015853211009174314,
      "loss": 0.1933,
      "step": 119
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.12900812923908234,
      "learning_rate": 0.00015816513761467892,
      "loss": 0.01,
      "step": 120
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.11863625049591064,
      "learning_rate": 0.0001577981651376147,
      "loss": 0.0153,
      "step": 121
    },
    {
      "epoch": 0.22181818181818183,
      "grad_norm": 0.4609273374080658,
      "learning_rate": 0.00015743119266055047,
      "loss": 0.0792,
      "step": 122
    },
    {
      "epoch": 0.22363636363636363,
      "grad_norm": 0.3202100694179535,
      "learning_rate": 0.00015706422018348624,
      "loss": 0.0672,
      "step": 123
    },
    {
      "epoch": 0.22545454545454546,
      "grad_norm": 0.2103869765996933,
      "learning_rate": 0.00015669724770642202,
      "loss": 0.0303,
      "step": 124
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.2878802418708801,
      "learning_rate": 0.00015633027522935782,
      "loss": 0.0755,
      "step": 125
    },
    {
      "epoch": 0.2290909090909091,
      "grad_norm": 0.09580240398645401,
      "learning_rate": 0.0001559633027522936,
      "loss": 0.0102,
      "step": 126
    },
    {
      "epoch": 0.2309090909090909,
      "grad_norm": 0.20782455801963806,
      "learning_rate": 0.00015559633027522937,
      "loss": 0.0322,
      "step": 127
    },
    {
      "epoch": 0.23272727272727273,
      "grad_norm": 0.07897714525461197,
      "learning_rate": 0.00015522935779816515,
      "loss": 0.0081,
      "step": 128
    },
    {
      "epoch": 0.23454545454545456,
      "grad_norm": 0.32571089267730713,
      "learning_rate": 0.00015486238532110092,
      "loss": 0.1119,
      "step": 129
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.1985579878091812,
      "learning_rate": 0.0001544954128440367,
      "loss": 0.0283,
      "step": 130
    },
    {
      "epoch": 0.2381818181818182,
      "grad_norm": 0.2746596932411194,
      "learning_rate": 0.0001541284403669725,
      "loss": 0.0721,
      "step": 131
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.22848813235759735,
      "learning_rate": 0.00015376146788990825,
      "loss": 0.0658,
      "step": 132
    },
    {
      "epoch": 0.24181818181818182,
      "grad_norm": 0.2230565994977951,
      "learning_rate": 0.00015339449541284405,
      "loss": 0.0272,
      "step": 133
    },
    {
      "epoch": 0.24363636363636362,
      "grad_norm": 0.38632312417030334,
      "learning_rate": 0.00015302752293577983,
      "loss": 0.0374,
      "step": 134
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.12261049449443817,
      "learning_rate": 0.0001526605504587156,
      "loss": 0.0227,
      "step": 135
    },
    {
      "epoch": 0.24727272727272728,
      "grad_norm": 0.055476002395153046,
      "learning_rate": 0.00015229357798165138,
      "loss": 0.0047,
      "step": 136
    },
    {
      "epoch": 0.24909090909090909,
      "grad_norm": 0.17743389308452606,
      "learning_rate": 0.00015192660550458718,
      "loss": 0.0357,
      "step": 137
    },
    {
      "epoch": 0.2509090909090909,
      "grad_norm": 0.19711537659168243,
      "learning_rate": 0.00015155963302752293,
      "loss": 0.0459,
      "step": 138
    },
    {
      "epoch": 0.25272727272727274,
      "grad_norm": 0.1542416661977768,
      "learning_rate": 0.00015119266055045873,
      "loss": 0.0203,
      "step": 139
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.07047806680202484,
      "learning_rate": 0.0001508256880733945,
      "loss": 0.0079,
      "step": 140
    },
    {
      "epoch": 0.25636363636363635,
      "grad_norm": 0.4204016625881195,
      "learning_rate": 0.00015045871559633028,
      "loss": 0.1257,
      "step": 141
    },
    {
      "epoch": 0.2581818181818182,
      "grad_norm": 0.23667332530021667,
      "learning_rate": 0.00015009174311926606,
      "loss": 0.0481,
      "step": 142
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.14645835757255554,
      "learning_rate": 0.00014972477064220186,
      "loss": 0.0211,
      "step": 143
    },
    {
      "epoch": 0.26181818181818184,
      "grad_norm": 0.2518030107021332,
      "learning_rate": 0.0001493577981651376,
      "loss": 0.0498,
      "step": 144
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.35970959067344666,
      "learning_rate": 0.0001489908256880734,
      "loss": 0.109,
      "step": 145
    },
    {
      "epoch": 0.26545454545454544,
      "grad_norm": 0.12224166840314865,
      "learning_rate": 0.00014862385321100919,
      "loss": 0.0183,
      "step": 146
    },
    {
      "epoch": 0.2672727272727273,
      "grad_norm": 0.2057022899389267,
      "learning_rate": 0.00014825688073394496,
      "loss": 0.0524,
      "step": 147
    },
    {
      "epoch": 0.2690909090909091,
      "grad_norm": 0.34247761964797974,
      "learning_rate": 0.00014788990825688074,
      "loss": 0.1069,
      "step": 148
    },
    {
      "epoch": 0.27090909090909093,
      "grad_norm": 0.1157023087143898,
      "learning_rate": 0.00014752293577981654,
      "loss": 0.0279,
      "step": 149
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.18156766891479492,
      "learning_rate": 0.0001471559633027523,
      "loss": 0.0169,
      "step": 150
    },
    {
      "epoch": 0.27454545454545454,
      "grad_norm": 0.3152984380722046,
      "learning_rate": 0.0001467889908256881,
      "loss": 0.0556,
      "step": 151
    },
    {
      "epoch": 0.27636363636363637,
      "grad_norm": 0.2504279911518097,
      "learning_rate": 0.00014642201834862386,
      "loss": 0.0463,
      "step": 152
    },
    {
      "epoch": 0.2781818181818182,
      "grad_norm": 0.20481781661510468,
      "learning_rate": 0.00014605504587155964,
      "loss": 0.0232,
      "step": 153
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.26747217774391174,
      "learning_rate": 0.00014568807339449542,
      "loss": 0.0328,
      "step": 154
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.34794101119041443,
      "learning_rate": 0.00014532110091743122,
      "loss": 0.1025,
      "step": 155
    },
    {
      "epoch": 0.28363636363636363,
      "grad_norm": 0.24540485441684723,
      "learning_rate": 0.00014495412844036697,
      "loss": 0.0902,
      "step": 156
    },
    {
      "epoch": 0.28545454545454546,
      "grad_norm": 0.14496372640132904,
      "learning_rate": 0.00014458715596330277,
      "loss": 0.0133,
      "step": 157
    },
    {
      "epoch": 0.2872727272727273,
      "grad_norm": 0.2282831221818924,
      "learning_rate": 0.00014422018348623854,
      "loss": 0.0345,
      "step": 158
    },
    {
      "epoch": 0.28909090909090907,
      "grad_norm": 0.1997269093990326,
      "learning_rate": 0.00014385321100917432,
      "loss": 0.0392,
      "step": 159
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.3106130361557007,
      "learning_rate": 0.0001434862385321101,
      "loss": 0.049,
      "step": 160
    },
    {
      "epoch": 0.2927272727272727,
      "grad_norm": 0.2950758934020996,
      "learning_rate": 0.0001431192660550459,
      "loss": 0.0556,
      "step": 161
    },
    {
      "epoch": 0.29454545454545455,
      "grad_norm": 0.44740742444992065,
      "learning_rate": 0.00014275229357798165,
      "loss": 0.043,
      "step": 162
    },
    {
      "epoch": 0.2963636363636364,
      "grad_norm": 0.10613447427749634,
      "learning_rate": 0.00014238532110091745,
      "loss": 0.0095,
      "step": 163
    },
    {
      "epoch": 0.29818181818181816,
      "grad_norm": 0.20387057960033417,
      "learning_rate": 0.00014201834862385322,
      "loss": 0.0443,
      "step": 164
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.1807709038257599,
      "learning_rate": 0.000141651376146789,
      "loss": 0.0344,
      "step": 165
    },
    {
      "epoch": 0.3018181818181818,
      "grad_norm": 0.2719568610191345,
      "learning_rate": 0.00014128440366972477,
      "loss": 0.0535,
      "step": 166
    },
    {
      "epoch": 0.30363636363636365,
      "grad_norm": 0.4160782992839813,
      "learning_rate": 0.00014091743119266058,
      "loss": 0.0895,
      "step": 167
    },
    {
      "epoch": 0.3054545454545455,
      "grad_norm": 0.2572765052318573,
      "learning_rate": 0.00014055045871559632,
      "loss": 0.0358,
      "step": 168
    },
    {
      "epoch": 0.30727272727272725,
      "grad_norm": 0.3543051779270172,
      "learning_rate": 0.00014018348623853213,
      "loss": 0.0584,
      "step": 169
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.3320467472076416,
      "learning_rate": 0.0001398165137614679,
      "loss": 0.0573,
      "step": 170
    },
    {
      "epoch": 0.3109090909090909,
      "grad_norm": 0.3805478513240814,
      "learning_rate": 0.00013944954128440368,
      "loss": 0.0827,
      "step": 171
    },
    {
      "epoch": 0.31272727272727274,
      "grad_norm": 0.24161392450332642,
      "learning_rate": 0.00013908256880733945,
      "loss": 0.0389,
      "step": 172
    },
    {
      "epoch": 0.3145454545454546,
      "grad_norm": 0.2950664460659027,
      "learning_rate": 0.00013871559633027526,
      "loss": 0.0844,
      "step": 173
    },
    {
      "epoch": 0.31636363636363635,
      "grad_norm": 0.22431856393814087,
      "learning_rate": 0.000138348623853211,
      "loss": 0.0435,
      "step": 174
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.3362470865249634,
      "learning_rate": 0.0001379816513761468,
      "loss": 0.1657,
      "step": 175
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.22808608412742615,
      "learning_rate": 0.00013761467889908258,
      "loss": 0.0376,
      "step": 176
    },
    {
      "epoch": 0.32181818181818184,
      "grad_norm": 0.1217612698674202,
      "learning_rate": 0.00013724770642201836,
      "loss": 0.016,
      "step": 177
    },
    {
      "epoch": 0.3236363636363636,
      "grad_norm": 0.2989322543144226,
      "learning_rate": 0.00013688073394495413,
      "loss": 0.0414,
      "step": 178
    },
    {
      "epoch": 0.32545454545454544,
      "grad_norm": 0.17178432643413544,
      "learning_rate": 0.00013651376146788994,
      "loss": 0.0221,
      "step": 179
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.12059013545513153,
      "learning_rate": 0.00013614678899082568,
      "loss": 0.013,
      "step": 180
    },
    {
      "epoch": 0.3290909090909091,
      "grad_norm": 0.3035137951374054,
      "learning_rate": 0.00013577981651376149,
      "loss": 0.0827,
      "step": 181
    },
    {
      "epoch": 0.33090909090909093,
      "grad_norm": 0.1889035850763321,
      "learning_rate": 0.00013541284403669726,
      "loss": 0.0262,
      "step": 182
    },
    {
      "epoch": 0.3327272727272727,
      "grad_norm": 0.2603878080844879,
      "learning_rate": 0.00013504587155963304,
      "loss": 0.0797,
      "step": 183
    },
    {
      "epoch": 0.33454545454545453,
      "grad_norm": 0.16496305167675018,
      "learning_rate": 0.0001346788990825688,
      "loss": 0.049,
      "step": 184
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.3089204430580139,
      "learning_rate": 0.00013431192660550461,
      "loss": 0.0828,
      "step": 185
    },
    {
      "epoch": 0.3381818181818182,
      "grad_norm": 0.2277173101902008,
      "learning_rate": 0.00013394495412844036,
      "loss": 0.0383,
      "step": 186
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.3033851385116577,
      "learning_rate": 0.00013357798165137617,
      "loss": 0.0587,
      "step": 187
    },
    {
      "epoch": 0.3418181818181818,
      "grad_norm": 0.3340968191623688,
      "learning_rate": 0.00013321100917431194,
      "loss": 0.0239,
      "step": 188
    },
    {
      "epoch": 0.34363636363636363,
      "grad_norm": 0.17831824719905853,
      "learning_rate": 0.00013284403669724772,
      "loss": 0.0254,
      "step": 189
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.1496455818414688,
      "learning_rate": 0.0001324770642201835,
      "loss": 0.0106,
      "step": 190
    },
    {
      "epoch": 0.3472727272727273,
      "grad_norm": 0.3277512788772583,
      "learning_rate": 0.00013211009174311927,
      "loss": 0.1281,
      "step": 191
    },
    {
      "epoch": 0.3490909090909091,
      "grad_norm": 0.12951891124248505,
      "learning_rate": 0.00013174311926605504,
      "loss": 0.0105,
      "step": 192
    },
    {
      "epoch": 0.3509090909090909,
      "grad_norm": 0.08029940724372864,
      "learning_rate": 0.00013137614678899084,
      "loss": 0.0062,
      "step": 193
    },
    {
      "epoch": 0.3527272727272727,
      "grad_norm": 0.14023694396018982,
      "learning_rate": 0.00013100917431192662,
      "loss": 0.0186,
      "step": 194
    },
    {
      "epoch": 0.35454545454545455,
      "grad_norm": 0.2764906585216522,
      "learning_rate": 0.0001306422018348624,
      "loss": 0.0642,
      "step": 195
    },
    {
      "epoch": 0.3563636363636364,
      "grad_norm": 0.2713642418384552,
      "learning_rate": 0.00013027522935779817,
      "loss": 0.0903,
      "step": 196
    },
    {
      "epoch": 0.35818181818181816,
      "grad_norm": 0.28163740038871765,
      "learning_rate": 0.00012990825688073395,
      "loss": 0.0435,
      "step": 197
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.09425994008779526,
      "learning_rate": 0.00012954128440366972,
      "loss": 0.0112,
      "step": 198
    },
    {
      "epoch": 0.3618181818181818,
      "grad_norm": 0.2308140993118286,
      "learning_rate": 0.00012917431192660552,
      "loss": 0.0618,
      "step": 199
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.0682930275797844,
      "learning_rate": 0.0001288073394495413,
      "loss": 0.0081,
      "step": 200
    },
    {
      "epoch": 0.3654545454545455,
      "grad_norm": 0.2012118399143219,
      "learning_rate": 0.00012844036697247707,
      "loss": 0.0378,
      "step": 201
    },
    {
      "epoch": 0.36727272727272725,
      "grad_norm": 0.30303797125816345,
      "learning_rate": 0.00012807339449541285,
      "loss": 0.0959,
      "step": 202
    },
    {
      "epoch": 0.3690909090909091,
      "grad_norm": 0.2414141297340393,
      "learning_rate": 0.00012770642201834863,
      "loss": 0.0437,
      "step": 203
    },
    {
      "epoch": 0.3709090909090909,
      "grad_norm": 0.15533463656902313,
      "learning_rate": 0.0001273394495412844,
      "loss": 0.0152,
      "step": 204
    },
    {
      "epoch": 0.37272727272727274,
      "grad_norm": 0.36681056022644043,
      "learning_rate": 0.0001269724770642202,
      "loss": 0.1205,
      "step": 205
    },
    {
      "epoch": 0.37454545454545457,
      "grad_norm": 0.21522419154644012,
      "learning_rate": 0.00012660550458715598,
      "loss": 0.0186,
      "step": 206
    },
    {
      "epoch": 0.37636363636363634,
      "grad_norm": 0.22591646015644073,
      "learning_rate": 0.00012623853211009175,
      "loss": 0.0572,
      "step": 207
    },
    {
      "epoch": 0.3781818181818182,
      "grad_norm": 0.18938377499580383,
      "learning_rate": 0.00012587155963302753,
      "loss": 0.053,
      "step": 208
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.1790306270122528,
      "learning_rate": 0.0001255045871559633,
      "loss": 0.0303,
      "step": 209
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.11157228797674179,
      "learning_rate": 0.00012513761467889908,
      "loss": 0.0119,
      "step": 210
    },
    {
      "epoch": 0.3836363636363636,
      "grad_norm": 0.2933838665485382,
      "learning_rate": 0.00012477064220183488,
      "loss": 0.0613,
      "step": 211
    },
    {
      "epoch": 0.38545454545454544,
      "grad_norm": 0.18928205966949463,
      "learning_rate": 0.00012440366972477063,
      "loss": 0.0306,
      "step": 212
    },
    {
      "epoch": 0.38727272727272727,
      "grad_norm": 0.27651023864746094,
      "learning_rate": 0.00012403669724770643,
      "loss": 0.0676,
      "step": 213
    },
    {
      "epoch": 0.3890909090909091,
      "grad_norm": 0.07661617547273636,
      "learning_rate": 0.0001236697247706422,
      "loss": 0.0046,
      "step": 214
    },
    {
      "epoch": 0.39090909090909093,
      "grad_norm": 0.15713803470134735,
      "learning_rate": 0.00012330275229357798,
      "loss": 0.014,
      "step": 215
    },
    {
      "epoch": 0.3927272727272727,
      "grad_norm": 0.11521978676319122,
      "learning_rate": 0.00012293577981651376,
      "loss": 0.0175,
      "step": 216
    },
    {
      "epoch": 0.39454545454545453,
      "grad_norm": 0.14849445223808289,
      "learning_rate": 0.00012256880733944956,
      "loss": 0.0105,
      "step": 217
    },
    {
      "epoch": 0.39636363636363636,
      "grad_norm": 0.21549250185489655,
      "learning_rate": 0.0001222018348623853,
      "loss": 0.0485,
      "step": 218
    },
    {
      "epoch": 0.3981818181818182,
      "grad_norm": 0.15855491161346436,
      "learning_rate": 0.00012183486238532111,
      "loss": 0.0399,
      "step": 219
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07281143963336945,
      "learning_rate": 0.00012146788990825689,
      "loss": 0.0065,
      "step": 220
    },
    {
      "epoch": 0.4018181818181818,
      "grad_norm": 0.1321536749601364,
      "learning_rate": 0.00012110091743119268,
      "loss": 0.0059,
      "step": 221
    },
    {
      "epoch": 0.4036363636363636,
      "grad_norm": 0.10267098993062973,
      "learning_rate": 0.00012073394495412844,
      "loss": 0.0091,
      "step": 222
    },
    {
      "epoch": 0.40545454545454546,
      "grad_norm": 0.07245607674121857,
      "learning_rate": 0.00012036697247706423,
      "loss": 0.0065,
      "step": 223
    },
    {
      "epoch": 0.4072727272727273,
      "grad_norm": 0.2683417499065399,
      "learning_rate": 0.00012,
      "loss": 0.0694,
      "step": 224
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.15490002930164337,
      "learning_rate": 0.00011963302752293579,
      "loss": 0.0115,
      "step": 225
    },
    {
      "epoch": 0.4109090909090909,
      "grad_norm": 0.34287065267562866,
      "learning_rate": 0.00011926605504587157,
      "loss": 0.0691,
      "step": 226
    },
    {
      "epoch": 0.4127272727272727,
      "grad_norm": 0.29358357191085815,
      "learning_rate": 0.00011889908256880736,
      "loss": 0.0786,
      "step": 227
    },
    {
      "epoch": 0.41454545454545455,
      "grad_norm": 0.31253522634506226,
      "learning_rate": 0.00011853211009174312,
      "loss": 0.0826,
      "step": 228
    },
    {
      "epoch": 0.4163636363636364,
      "grad_norm": 0.20093311369419098,
      "learning_rate": 0.00011816513761467891,
      "loss": 0.0341,
      "step": 229
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.23711763322353363,
      "learning_rate": 0.00011779816513761468,
      "loss": 0.0331,
      "step": 230
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.1702217310667038,
      "learning_rate": 0.00011743119266055047,
      "loss": 0.0146,
      "step": 231
    },
    {
      "epoch": 0.4218181818181818,
      "grad_norm": 0.34525927901268005,
      "learning_rate": 0.00011706422018348623,
      "loss": 0.1054,
      "step": 232
    },
    {
      "epoch": 0.42363636363636364,
      "grad_norm": 0.1165570318698883,
      "learning_rate": 0.00011669724770642204,
      "loss": 0.0154,
      "step": 233
    },
    {
      "epoch": 0.4254545454545455,
      "grad_norm": 0.3414374589920044,
      "learning_rate": 0.0001163302752293578,
      "loss": 0.0855,
      "step": 234
    },
    {
      "epoch": 0.42727272727272725,
      "grad_norm": 0.11132565885782242,
      "learning_rate": 0.00011596330275229359,
      "loss": 0.0096,
      "step": 235
    },
    {
      "epoch": 0.4290909090909091,
      "grad_norm": 0.2893137037754059,
      "learning_rate": 0.00011559633027522936,
      "loss": 0.0915,
      "step": 236
    },
    {
      "epoch": 0.4309090909090909,
      "grad_norm": 0.13451042771339417,
      "learning_rate": 0.00011522935779816515,
      "loss": 0.0205,
      "step": 237
    },
    {
      "epoch": 0.43272727272727274,
      "grad_norm": 0.3251959979534149,
      "learning_rate": 0.00011486238532110091,
      "loss": 0.0646,
      "step": 238
    },
    {
      "epoch": 0.43454545454545457,
      "grad_norm": 0.10631631314754486,
      "learning_rate": 0.00011449541284403671,
      "loss": 0.0167,
      "step": 239
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.04021703824400902,
      "learning_rate": 0.00011412844036697248,
      "loss": 0.0036,
      "step": 240
    },
    {
      "epoch": 0.4381818181818182,
      "grad_norm": 0.10531014204025269,
      "learning_rate": 0.00011376146788990827,
      "loss": 0.0245,
      "step": 241
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.26570403575897217,
      "learning_rate": 0.00011339449541284404,
      "loss": 0.0769,
      "step": 242
    },
    {
      "epoch": 0.44181818181818183,
      "grad_norm": 0.36088356375694275,
      "learning_rate": 0.00011302752293577983,
      "loss": 0.1395,
      "step": 243
    },
    {
      "epoch": 0.44363636363636366,
      "grad_norm": 0.20344924926757812,
      "learning_rate": 0.00011266055045871559,
      "loss": 0.0333,
      "step": 244
    },
    {
      "epoch": 0.44545454545454544,
      "grad_norm": 0.18568557500839233,
      "learning_rate": 0.0001122935779816514,
      "loss": 0.0424,
      "step": 245
    },
    {
      "epoch": 0.44727272727272727,
      "grad_norm": 0.2459505796432495,
      "learning_rate": 0.00011192660550458716,
      "loss": 0.0547,
      "step": 246
    },
    {
      "epoch": 0.4490909090909091,
      "grad_norm": 0.2183993011713028,
      "learning_rate": 0.00011155963302752295,
      "loss": 0.0566,
      "step": 247
    },
    {
      "epoch": 0.4509090909090909,
      "grad_norm": 0.2011561542749405,
      "learning_rate": 0.00011119266055045872,
      "loss": 0.037,
      "step": 248
    },
    {
      "epoch": 0.4527272727272727,
      "grad_norm": 0.15336290001869202,
      "learning_rate": 0.00011082568807339451,
      "loss": 0.0157,
      "step": 249
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.2835693061351776,
      "learning_rate": 0.00011045871559633027,
      "loss": 0.0649,
      "step": 250
    },
    {
      "epoch": 0.45636363636363636,
      "grad_norm": 0.09609012305736542,
      "learning_rate": 0.00011009174311926606,
      "loss": 0.0078,
      "step": 251
    },
    {
      "epoch": 0.4581818181818182,
      "grad_norm": 0.23164023458957672,
      "learning_rate": 0.00010972477064220184,
      "loss": 0.0175,
      "step": 252
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.42857855558395386,
      "learning_rate": 0.00010935779816513762,
      "loss": 0.0328,
      "step": 253
    },
    {
      "epoch": 0.4618181818181818,
      "grad_norm": 0.2374146431684494,
      "learning_rate": 0.0001089908256880734,
      "loss": 0.0422,
      "step": 254
    },
    {
      "epoch": 0.4636363636363636,
      "grad_norm": 0.1602640151977539,
      "learning_rate": 0.00010862385321100919,
      "loss": 0.0148,
      "step": 255
    },
    {
      "epoch": 0.46545454545454545,
      "grad_norm": 0.29289597272872925,
      "learning_rate": 0.00010825688073394495,
      "loss": 0.041,
      "step": 256
    },
    {
      "epoch": 0.4672727272727273,
      "grad_norm": 0.20515526831150055,
      "learning_rate": 0.00010788990825688074,
      "loss": 0.0303,
      "step": 257
    },
    {
      "epoch": 0.4690909090909091,
      "grad_norm": 0.17917919158935547,
      "learning_rate": 0.00010752293577981651,
      "loss": 0.0147,
      "step": 258
    },
    {
      "epoch": 0.4709090909090909,
      "grad_norm": 0.2543739378452301,
      "learning_rate": 0.0001071559633027523,
      "loss": 0.025,
      "step": 259
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.1772337555885315,
      "learning_rate": 0.00010678899082568808,
      "loss": 0.0251,
      "step": 260
    },
    {
      "epoch": 0.47454545454545455,
      "grad_norm": 0.10138455033302307,
      "learning_rate": 0.00010642201834862387,
      "loss": 0.0087,
      "step": 261
    },
    {
      "epoch": 0.4763636363636364,
      "grad_norm": 0.31928548216819763,
      "learning_rate": 0.00010605504587155963,
      "loss": 0.0644,
      "step": 262
    },
    {
      "epoch": 0.4781818181818182,
      "grad_norm": 0.13221454620361328,
      "learning_rate": 0.00010568807339449542,
      "loss": 0.0164,
      "step": 263
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.14834004640579224,
      "learning_rate": 0.0001053211009174312,
      "loss": 0.0289,
      "step": 264
    },
    {
      "epoch": 0.4818181818181818,
      "grad_norm": 0.18197152018547058,
      "learning_rate": 0.00010495412844036698,
      "loss": 0.0478,
      "step": 265
    },
    {
      "epoch": 0.48363636363636364,
      "grad_norm": 0.2058606594800949,
      "learning_rate": 0.00010458715596330276,
      "loss": 0.039,
      "step": 266
    },
    {
      "epoch": 0.48545454545454547,
      "grad_norm": 0.29321739077568054,
      "learning_rate": 0.00010422018348623855,
      "loss": 0.0971,
      "step": 267
    },
    {
      "epoch": 0.48727272727272725,
      "grad_norm": 0.06209462136030197,
      "learning_rate": 0.00010385321100917431,
      "loss": 0.0056,
      "step": 268
    },
    {
      "epoch": 0.4890909090909091,
      "grad_norm": 0.2108938843011856,
      "learning_rate": 0.0001034862385321101,
      "loss": 0.0553,
      "step": 269
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.18645384907722473,
      "learning_rate": 0.00010311926605504587,
      "loss": 0.0678,
      "step": 270
    },
    {
      "epoch": 0.49272727272727274,
      "grad_norm": 0.2248598337173462,
      "learning_rate": 0.00010275229357798166,
      "loss": 0.0789,
      "step": 271
    },
    {
      "epoch": 0.49454545454545457,
      "grad_norm": 0.15062375366687775,
      "learning_rate": 0.00010238532110091742,
      "loss": 0.0549,
      "step": 272
    },
    {
      "epoch": 0.49636363636363634,
      "grad_norm": 0.2501031458377838,
      "learning_rate": 0.00010201834862385323,
      "loss": 0.0867,
      "step": 273
    },
    {
      "epoch": 0.49818181818181817,
      "grad_norm": 0.14842958748340607,
      "learning_rate": 0.00010165137614678899,
      "loss": 0.0131,
      "step": 274
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.10269279032945633,
      "learning_rate": 0.00010128440366972478,
      "loss": 0.0136,
      "step": 275
    },
    {
      "epoch": 0.5018181818181818,
      "grad_norm": 0.25051435828208923,
      "learning_rate": 0.00010091743119266055,
      "loss": 0.0621,
      "step": 276
    },
    {
      "epoch": 0.5036363636363637,
      "grad_norm": 0.3267698585987091,
      "learning_rate": 0.00010055045871559634,
      "loss": 0.0809,
      "step": 277
    },
    {
      "epoch": 0.5054545454545455,
      "grad_norm": 0.21237437427043915,
      "learning_rate": 0.0001001834862385321,
      "loss": 0.0582,
      "step": 278
    },
    {
      "epoch": 0.5072727272727273,
      "grad_norm": 0.20965559780597687,
      "learning_rate": 9.98165137614679e-05,
      "loss": 0.0401,
      "step": 279
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.04950452595949173,
      "learning_rate": 9.944954128440368e-05,
      "loss": 0.0038,
      "step": 280
    },
    {
      "epoch": 0.5109090909090909,
      "grad_norm": 0.14870235323905945,
      "learning_rate": 9.908256880733946e-05,
      "loss": 0.0174,
      "step": 281
    },
    {
      "epoch": 0.5127272727272727,
      "grad_norm": 0.11732164025306702,
      "learning_rate": 9.871559633027525e-05,
      "loss": 0.0159,
      "step": 282
    },
    {
      "epoch": 0.5145454545454545,
      "grad_norm": 0.1275234967470169,
      "learning_rate": 9.834862385321102e-05,
      "loss": 0.0188,
      "step": 283
    },
    {
      "epoch": 0.5163636363636364,
      "grad_norm": 0.23614616692066193,
      "learning_rate": 9.79816513761468e-05,
      "loss": 0.0208,
      "step": 284
    },
    {
      "epoch": 0.5181818181818182,
      "grad_norm": 0.11457470804452896,
      "learning_rate": 9.761467889908259e-05,
      "loss": 0.0156,
      "step": 285
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.19866754114627838,
      "learning_rate": 9.724770642201836e-05,
      "loss": 0.0192,
      "step": 286
    },
    {
      "epoch": 0.5218181818181818,
      "grad_norm": 0.1600734144449234,
      "learning_rate": 9.688073394495414e-05,
      "loss": 0.0107,
      "step": 287
    },
    {
      "epoch": 0.5236363636363637,
      "grad_norm": 0.43116310238838196,
      "learning_rate": 9.651376146788991e-05,
      "loss": 0.0804,
      "step": 288
    },
    {
      "epoch": 0.5254545454545455,
      "grad_norm": 0.2132592499256134,
      "learning_rate": 9.61467889908257e-05,
      "loss": 0.0268,
      "step": 289
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.7480737566947937,
      "learning_rate": 9.577981651376148e-05,
      "loss": 0.1129,
      "step": 290
    },
    {
      "epoch": 0.5290909090909091,
      "grad_norm": 0.29835423827171326,
      "learning_rate": 9.541284403669725e-05,
      "loss": 0.0297,
      "step": 291
    },
    {
      "epoch": 0.5309090909090909,
      "grad_norm": 0.3059428036212921,
      "learning_rate": 9.504587155963304e-05,
      "loss": 0.0599,
      "step": 292
    },
    {
      "epoch": 0.5327272727272727,
      "grad_norm": 0.15361103415489197,
      "learning_rate": 9.467889908256882e-05,
      "loss": 0.0268,
      "step": 293
    },
    {
      "epoch": 0.5345454545454545,
      "grad_norm": 0.20340104401111603,
      "learning_rate": 9.431192660550459e-05,
      "loss": 0.0112,
      "step": 294
    },
    {
      "epoch": 0.5363636363636364,
      "grad_norm": 0.10969298332929611,
      "learning_rate": 9.394495412844038e-05,
      "loss": 0.0123,
      "step": 295
    },
    {
      "epoch": 0.5381818181818182,
      "grad_norm": 0.035945307463407516,
      "learning_rate": 9.357798165137616e-05,
      "loss": 0.0025,
      "step": 296
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.18416500091552734,
      "learning_rate": 9.321100917431193e-05,
      "loss": 0.0322,
      "step": 297
    },
    {
      "epoch": 0.5418181818181819,
      "grad_norm": 0.14212673902511597,
      "learning_rate": 9.284403669724772e-05,
      "loss": 0.0265,
      "step": 298
    },
    {
      "epoch": 0.5436363636363636,
      "grad_norm": 0.2547135651111603,
      "learning_rate": 9.24770642201835e-05,
      "loss": 0.0274,
      "step": 299
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.21091969311237335,
      "learning_rate": 9.211009174311927e-05,
      "loss": 0.0537,
      "step": 300
    },
    {
      "epoch": 0.5472727272727272,
      "grad_norm": 0.5228303670883179,
      "learning_rate": 9.174311926605506e-05,
      "loss": 0.1755,
      "step": 301
    },
    {
      "epoch": 0.5490909090909091,
      "grad_norm": 0.37613236904144287,
      "learning_rate": 9.137614678899083e-05,
      "loss": 0.0228,
      "step": 302
    },
    {
      "epoch": 0.5509090909090909,
      "grad_norm": 0.27742740511894226,
      "learning_rate": 9.100917431192661e-05,
      "loss": 0.0831,
      "step": 303
    },
    {
      "epoch": 0.5527272727272727,
      "grad_norm": 0.2569931447505951,
      "learning_rate": 9.06422018348624e-05,
      "loss": 0.0594,
      "step": 304
    },
    {
      "epoch": 0.5545454545454546,
      "grad_norm": 0.2936508059501648,
      "learning_rate": 9.027522935779817e-05,
      "loss": 0.0526,
      "step": 305
    },
    {
      "epoch": 0.5563636363636364,
      "grad_norm": 0.2110864371061325,
      "learning_rate": 8.990825688073395e-05,
      "loss": 0.0447,
      "step": 306
    },
    {
      "epoch": 0.5581818181818182,
      "grad_norm": 0.13315409421920776,
      "learning_rate": 8.954128440366974e-05,
      "loss": 0.0188,
      "step": 307
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.2241198569536209,
      "learning_rate": 8.917431192660551e-05,
      "loss": 0.0486,
      "step": 308
    },
    {
      "epoch": 0.5618181818181818,
      "grad_norm": 0.18960708379745483,
      "learning_rate": 8.880733944954129e-05,
      "loss": 0.0277,
      "step": 309
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.3623030483722687,
      "learning_rate": 8.844036697247708e-05,
      "loss": 0.0161,
      "step": 310
    },
    {
      "epoch": 0.5654545454545454,
      "grad_norm": 0.057494908571243286,
      "learning_rate": 8.807339449541285e-05,
      "loss": 0.0049,
      "step": 311
    },
    {
      "epoch": 0.5672727272727273,
      "grad_norm": 0.2021004557609558,
      "learning_rate": 8.770642201834863e-05,
      "loss": 0.0301,
      "step": 312
    },
    {
      "epoch": 0.5690909090909091,
      "grad_norm": 0.238887220621109,
      "learning_rate": 8.733944954128442e-05,
      "loss": 0.0345,
      "step": 313
    },
    {
      "epoch": 0.5709090909090909,
      "grad_norm": 0.33597105741500854,
      "learning_rate": 8.697247706422019e-05,
      "loss": 0.1311,
      "step": 314
    },
    {
      "epoch": 0.5727272727272728,
      "grad_norm": 0.05955353006720543,
      "learning_rate": 8.660550458715597e-05,
      "loss": 0.0056,
      "step": 315
    },
    {
      "epoch": 0.5745454545454546,
      "grad_norm": 0.3128560185432434,
      "learning_rate": 8.623853211009176e-05,
      "loss": 0.0915,
      "step": 316
    },
    {
      "epoch": 0.5763636363636364,
      "grad_norm": 0.06347023695707321,
      "learning_rate": 8.587155963302753e-05,
      "loss": 0.0053,
      "step": 317
    },
    {
      "epoch": 0.5781818181818181,
      "grad_norm": 0.2543555498123169,
      "learning_rate": 8.550458715596331e-05,
      "loss": 0.07,
      "step": 318
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.3387523889541626,
      "learning_rate": 8.51376146788991e-05,
      "loss": 0.0764,
      "step": 319
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.16244131326675415,
      "learning_rate": 8.477064220183487e-05,
      "loss": 0.0257,
      "step": 320
    },
    {
      "epoch": 0.5836363636363636,
      "grad_norm": 0.2802168130874634,
      "learning_rate": 8.440366972477065e-05,
      "loss": 0.0472,
      "step": 321
    },
    {
      "epoch": 0.5854545454545454,
      "grad_norm": 0.24284270405769348,
      "learning_rate": 8.403669724770644e-05,
      "loss": 0.0469,
      "step": 322
    },
    {
      "epoch": 0.5872727272727273,
      "grad_norm": 0.1438303142786026,
      "learning_rate": 8.366972477064221e-05,
      "loss": 0.0135,
      "step": 323
    },
    {
      "epoch": 0.5890909090909091,
      "grad_norm": 0.13390104472637177,
      "learning_rate": 8.330275229357799e-05,
      "loss": 0.0199,
      "step": 324
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.23094873130321503,
      "learning_rate": 8.293577981651378e-05,
      "loss": 0.0195,
      "step": 325
    },
    {
      "epoch": 0.5927272727272728,
      "grad_norm": 0.18290144205093384,
      "learning_rate": 8.256880733944955e-05,
      "loss": 0.0244,
      "step": 326
    },
    {
      "epoch": 0.5945454545454546,
      "grad_norm": 0.36614346504211426,
      "learning_rate": 8.220183486238533e-05,
      "loss": 0.1044,
      "step": 327
    },
    {
      "epoch": 0.5963636363636363,
      "grad_norm": 0.23880508542060852,
      "learning_rate": 8.183486238532112e-05,
      "loss": 0.0391,
      "step": 328
    },
    {
      "epoch": 0.5981818181818181,
      "grad_norm": 0.2666987478733063,
      "learning_rate": 8.146788990825689e-05,
      "loss": 0.0567,
      "step": 329
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.20394960045814514,
      "learning_rate": 8.110091743119267e-05,
      "loss": 0.0291,
      "step": 330
    },
    {
      "epoch": 0.6018181818181818,
      "grad_norm": 0.21837151050567627,
      "learning_rate": 8.073394495412844e-05,
      "loss": 0.0556,
      "step": 331
    },
    {
      "epoch": 0.6036363636363636,
      "grad_norm": 0.20277650654315948,
      "learning_rate": 8.036697247706423e-05,
      "loss": 0.046,
      "step": 332
    },
    {
      "epoch": 0.6054545454545455,
      "grad_norm": 0.20363359153270721,
      "learning_rate": 8e-05,
      "loss": 0.0232,
      "step": 333
    },
    {
      "epoch": 0.6072727272727273,
      "grad_norm": 0.1777394711971283,
      "learning_rate": 7.963302752293578e-05,
      "loss": 0.0187,
      "step": 334
    },
    {
      "epoch": 0.6090909090909091,
      "grad_norm": 0.2224687933921814,
      "learning_rate": 7.926605504587157e-05,
      "loss": 0.0391,
      "step": 335
    },
    {
      "epoch": 0.610909090909091,
      "grad_norm": 0.1101943850517273,
      "learning_rate": 7.889908256880735e-05,
      "loss": 0.0178,
      "step": 336
    },
    {
      "epoch": 0.6127272727272727,
      "grad_norm": 0.09276539087295532,
      "learning_rate": 7.853211009174312e-05,
      "loss": 0.0071,
      "step": 337
    },
    {
      "epoch": 0.6145454545454545,
      "grad_norm": 0.14569474756717682,
      "learning_rate": 7.816513761467891e-05,
      "loss": 0.0246,
      "step": 338
    },
    {
      "epoch": 0.6163636363636363,
      "grad_norm": 0.10715912282466888,
      "learning_rate": 7.779816513761469e-05,
      "loss": 0.0098,
      "step": 339
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.11459314078092575,
      "learning_rate": 7.743119266055046e-05,
      "loss": 0.0075,
      "step": 340
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.24471762776374817,
      "learning_rate": 7.706422018348625e-05,
      "loss": 0.0301,
      "step": 341
    },
    {
      "epoch": 0.6218181818181818,
      "grad_norm": 0.18010400235652924,
      "learning_rate": 7.669724770642203e-05,
      "loss": 0.0167,
      "step": 342
    },
    {
      "epoch": 0.6236363636363637,
      "grad_norm": 0.09419813752174377,
      "learning_rate": 7.63302752293578e-05,
      "loss": 0.009,
      "step": 343
    },
    {
      "epoch": 0.6254545454545455,
      "grad_norm": 0.17763681709766388,
      "learning_rate": 7.596330275229359e-05,
      "loss": 0.0275,
      "step": 344
    },
    {
      "epoch": 0.6272727272727273,
      "grad_norm": 0.033711664378643036,
      "learning_rate": 7.559633027522937e-05,
      "loss": 0.0038,
      "step": 345
    },
    {
      "epoch": 0.6290909090909091,
      "grad_norm": 0.16520404815673828,
      "learning_rate": 7.522935779816514e-05,
      "loss": 0.0187,
      "step": 346
    },
    {
      "epoch": 0.6309090909090909,
      "grad_norm": 0.1324990838766098,
      "learning_rate": 7.486238532110093e-05,
      "loss": 0.0164,
      "step": 347
    },
    {
      "epoch": 0.6327272727272727,
      "grad_norm": 0.25636276602745056,
      "learning_rate": 7.44954128440367e-05,
      "loss": 0.0591,
      "step": 348
    },
    {
      "epoch": 0.6345454545454545,
      "grad_norm": 0.26519617438316345,
      "learning_rate": 7.412844036697248e-05,
      "loss": 0.0627,
      "step": 349
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.31316718459129333,
      "learning_rate": 7.376146788990827e-05,
      "loss": 0.1006,
      "step": 350
    },
    {
      "epoch": 0.6381818181818182,
      "grad_norm": 0.1575094759464264,
      "learning_rate": 7.339449541284404e-05,
      "loss": 0.0093,
      "step": 351
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.21517153084278107,
      "learning_rate": 7.302752293577982e-05,
      "loss": 0.0285,
      "step": 352
    },
    {
      "epoch": 0.6418181818181818,
      "grad_norm": 0.0851864442229271,
      "learning_rate": 7.266055045871561e-05,
      "loss": 0.0062,
      "step": 353
    },
    {
      "epoch": 0.6436363636363637,
      "grad_norm": 0.14787112176418304,
      "learning_rate": 7.229357798165138e-05,
      "loss": 0.0171,
      "step": 354
    },
    {
      "epoch": 0.6454545454545455,
      "grad_norm": 0.24049009382724762,
      "learning_rate": 7.192660550458716e-05,
      "loss": 0.0669,
      "step": 355
    },
    {
      "epoch": 0.6472727272727272,
      "grad_norm": 0.3295653164386749,
      "learning_rate": 7.155963302752295e-05,
      "loss": 0.1002,
      "step": 356
    },
    {
      "epoch": 0.649090909090909,
      "grad_norm": 0.047577571123838425,
      "learning_rate": 7.119266055045872e-05,
      "loss": 0.0038,
      "step": 357
    },
    {
      "epoch": 0.6509090909090909,
      "grad_norm": 0.13617758452892303,
      "learning_rate": 7.08256880733945e-05,
      "loss": 0.0199,
      "step": 358
    },
    {
      "epoch": 0.6527272727272727,
      "grad_norm": 0.11588389426469803,
      "learning_rate": 7.045871559633029e-05,
      "loss": 0.0213,
      "step": 359
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.1466895192861557,
      "learning_rate": 7.009174311926606e-05,
      "loss": 0.0146,
      "step": 360
    },
    {
      "epoch": 0.6563636363636364,
      "grad_norm": 0.07562890648841858,
      "learning_rate": 6.972477064220184e-05,
      "loss": 0.0042,
      "step": 361
    },
    {
      "epoch": 0.6581818181818182,
      "grad_norm": 0.15290555357933044,
      "learning_rate": 6.935779816513763e-05,
      "loss": 0.0252,
      "step": 362
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.18271584808826447,
      "learning_rate": 6.89908256880734e-05,
      "loss": 0.0361,
      "step": 363
    },
    {
      "epoch": 0.6618181818181819,
      "grad_norm": 0.10512940585613251,
      "learning_rate": 6.862385321100918e-05,
      "loss": 0.0128,
      "step": 364
    },
    {
      "epoch": 0.6636363636363637,
      "grad_norm": 0.29588669538497925,
      "learning_rate": 6.825688073394497e-05,
      "loss": 0.0701,
      "step": 365
    },
    {
      "epoch": 0.6654545454545454,
      "grad_norm": 0.18322311341762543,
      "learning_rate": 6.788990825688074e-05,
      "loss": 0.0381,
      "step": 366
    },
    {
      "epoch": 0.6672727272727272,
      "grad_norm": 0.127156063914299,
      "learning_rate": 6.752293577981652e-05,
      "loss": 0.0127,
      "step": 367
    },
    {
      "epoch": 0.6690909090909091,
      "grad_norm": 0.2596765458583832,
      "learning_rate": 6.715596330275231e-05,
      "loss": 0.1038,
      "step": 368
    },
    {
      "epoch": 0.6709090909090909,
      "grad_norm": 0.19962285459041595,
      "learning_rate": 6.678899082568808e-05,
      "loss": 0.0124,
      "step": 369
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.2840002179145813,
      "learning_rate": 6.642201834862386e-05,
      "loss": 0.0702,
      "step": 370
    },
    {
      "epoch": 0.6745454545454546,
      "grad_norm": 0.3482336401939392,
      "learning_rate": 6.605504587155963e-05,
      "loss": 0.0859,
      "step": 371
    },
    {
      "epoch": 0.6763636363636364,
      "grad_norm": 0.10975788533687592,
      "learning_rate": 6.568807339449542e-05,
      "loss": 0.0134,
      "step": 372
    },
    {
      "epoch": 0.6781818181818182,
      "grad_norm": 0.11577411741018295,
      "learning_rate": 6.53211009174312e-05,
      "loss": 0.009,
      "step": 373
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.14999978244304657,
      "learning_rate": 6.495412844036697e-05,
      "loss": 0.0138,
      "step": 374
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.25761744379997253,
      "learning_rate": 6.458715596330276e-05,
      "loss": 0.0673,
      "step": 375
    },
    {
      "epoch": 0.6836363636363636,
      "grad_norm": 0.0834006741642952,
      "learning_rate": 6.422018348623854e-05,
      "loss": 0.0053,
      "step": 376
    },
    {
      "epoch": 0.6854545454545454,
      "grad_norm": 0.16704687476158142,
      "learning_rate": 6.385321100917431e-05,
      "loss": 0.0149,
      "step": 377
    },
    {
      "epoch": 0.6872727272727273,
      "grad_norm": 0.03615481033921242,
      "learning_rate": 6.34862385321101e-05,
      "loss": 0.0027,
      "step": 378
    },
    {
      "epoch": 0.6890909090909091,
      "grad_norm": 0.049690600484609604,
      "learning_rate": 6.311926605504588e-05,
      "loss": 0.0046,
      "step": 379
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.43006888031959534,
      "learning_rate": 6.275229357798165e-05,
      "loss": 0.0215,
      "step": 380
    },
    {
      "epoch": 0.6927272727272727,
      "grad_norm": 0.2452259063720703,
      "learning_rate": 6.238532110091744e-05,
      "loss": 0.0608,
      "step": 381
    },
    {
      "epoch": 0.6945454545454546,
      "grad_norm": 0.23529578745365143,
      "learning_rate": 6.201834862385322e-05,
      "loss": 0.0258,
      "step": 382
    },
    {
      "epoch": 0.6963636363636364,
      "grad_norm": 0.1499873548746109,
      "learning_rate": 6.165137614678899e-05,
      "loss": 0.0175,
      "step": 383
    },
    {
      "epoch": 0.6981818181818182,
      "grad_norm": 0.2500057816505432,
      "learning_rate": 6.128440366972478e-05,
      "loss": 0.0806,
      "step": 384
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.20496748387813568,
      "learning_rate": 6.0917431192660556e-05,
      "loss": 0.0146,
      "step": 385
    },
    {
      "epoch": 0.7018181818181818,
      "grad_norm": 0.2875692546367645,
      "learning_rate": 6.055045871559634e-05,
      "loss": 0.0776,
      "step": 386
    },
    {
      "epoch": 0.7036363636363636,
      "grad_norm": 0.18551045656204224,
      "learning_rate": 6.0183486238532114e-05,
      "loss": 0.0203,
      "step": 387
    },
    {
      "epoch": 0.7054545454545454,
      "grad_norm": 0.11167962104082108,
      "learning_rate": 5.9816513761467896e-05,
      "loss": 0.0133,
      "step": 388
    },
    {
      "epoch": 0.7072727272727273,
      "grad_norm": 0.10871255397796631,
      "learning_rate": 5.944954128440368e-05,
      "loss": 0.0088,
      "step": 389
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.19860148429870605,
      "learning_rate": 5.9082568807339454e-05,
      "loss": 0.0258,
      "step": 390
    },
    {
      "epoch": 0.7109090909090909,
      "grad_norm": 0.1790824681520462,
      "learning_rate": 5.8715596330275236e-05,
      "loss": 0.0257,
      "step": 391
    },
    {
      "epoch": 0.7127272727272728,
      "grad_norm": 0.2501089870929718,
      "learning_rate": 5.834862385321102e-05,
      "loss": 0.0811,
      "step": 392
    },
    {
      "epoch": 0.7145454545454546,
      "grad_norm": 0.1924431174993515,
      "learning_rate": 5.798165137614679e-05,
      "loss": 0.0258,
      "step": 393
    },
    {
      "epoch": 0.7163636363636363,
      "grad_norm": 0.19332647323608398,
      "learning_rate": 5.7614678899082575e-05,
      "loss": 0.0328,
      "step": 394
    },
    {
      "epoch": 0.7181818181818181,
      "grad_norm": 0.17516012489795685,
      "learning_rate": 5.724770642201836e-05,
      "loss": 0.0257,
      "step": 395
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.07635791599750519,
      "learning_rate": 5.688073394495413e-05,
      "loss": 0.0073,
      "step": 396
    },
    {
      "epoch": 0.7218181818181818,
      "grad_norm": 0.13736312091350555,
      "learning_rate": 5.6513761467889915e-05,
      "loss": 0.021,
      "step": 397
    },
    {
      "epoch": 0.7236363636363636,
      "grad_norm": 0.0169016532599926,
      "learning_rate": 5.61467889908257e-05,
      "loss": 0.0016,
      "step": 398
    },
    {
      "epoch": 0.7254545454545455,
      "grad_norm": 0.1259976178407669,
      "learning_rate": 5.577981651376147e-05,
      "loss": 0.027,
      "step": 399
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.3251633644104004,
      "learning_rate": 5.5412844036697255e-05,
      "loss": 0.0594,
      "step": 400
    },
    {
      "epoch": 0.7290909090909091,
      "grad_norm": 0.2306991070508957,
      "learning_rate": 5.504587155963303e-05,
      "loss": 0.046,
      "step": 401
    },
    {
      "epoch": 0.730909090909091,
      "grad_norm": 0.15657246112823486,
      "learning_rate": 5.467889908256881e-05,
      "loss": 0.0198,
      "step": 402
    },
    {
      "epoch": 0.7327272727272728,
      "grad_norm": 0.2366553246974945,
      "learning_rate": 5.4311926605504594e-05,
      "loss": 0.0232,
      "step": 403
    },
    {
      "epoch": 0.7345454545454545,
      "grad_norm": 0.15471138060092926,
      "learning_rate": 5.394495412844037e-05,
      "loss": 0.0154,
      "step": 404
    },
    {
      "epoch": 0.7363636363636363,
      "grad_norm": 0.2181294858455658,
      "learning_rate": 5.357798165137615e-05,
      "loss": 0.0508,
      "step": 405
    },
    {
      "epoch": 0.7381818181818182,
      "grad_norm": 0.15842187404632568,
      "learning_rate": 5.3211009174311934e-05,
      "loss": 0.0256,
      "step": 406
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.03308369219303131,
      "learning_rate": 5.284403669724771e-05,
      "loss": 0.0033,
      "step": 407
    },
    {
      "epoch": 0.7418181818181818,
      "grad_norm": 0.20179669559001923,
      "learning_rate": 5.247706422018349e-05,
      "loss": 0.0573,
      "step": 408
    },
    {
      "epoch": 0.7436363636363637,
      "grad_norm": 0.10476170480251312,
      "learning_rate": 5.2110091743119274e-05,
      "loss": 0.0071,
      "step": 409
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.22118569910526276,
      "learning_rate": 5.174311926605505e-05,
      "loss": 0.0122,
      "step": 410
    },
    {
      "epoch": 0.7472727272727273,
      "grad_norm": 0.29258519411087036,
      "learning_rate": 5.137614678899083e-05,
      "loss": 0.0238,
      "step": 411
    },
    {
      "epoch": 0.7490909090909091,
      "grad_norm": 0.22007977962493896,
      "learning_rate": 5.100917431192661e-05,
      "loss": 0.0483,
      "step": 412
    },
    {
      "epoch": 0.7509090909090909,
      "grad_norm": 0.18731705844402313,
      "learning_rate": 5.064220183486239e-05,
      "loss": 0.0252,
      "step": 413
    },
    {
      "epoch": 0.7527272727272727,
      "grad_norm": 0.36532407999038696,
      "learning_rate": 5.027522935779817e-05,
      "loss": 0.1158,
      "step": 414
    },
    {
      "epoch": 0.7545454545454545,
      "grad_norm": 0.17780007421970367,
      "learning_rate": 4.990825688073395e-05,
      "loss": 0.0318,
      "step": 415
    },
    {
      "epoch": 0.7563636363636363,
      "grad_norm": 0.20649568736553192,
      "learning_rate": 4.954128440366973e-05,
      "loss": 0.0415,
      "step": 416
    },
    {
      "epoch": 0.7581818181818182,
      "grad_norm": 0.09066277742385864,
      "learning_rate": 4.917431192660551e-05,
      "loss": 0.0071,
      "step": 417
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.25135958194732666,
      "learning_rate": 4.880733944954129e-05,
      "loss": 0.0436,
      "step": 418
    },
    {
      "epoch": 0.7618181818181818,
      "grad_norm": 0.16416287422180176,
      "learning_rate": 4.844036697247707e-05,
      "loss": 0.0338,
      "step": 419
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.33346468210220337,
      "learning_rate": 4.807339449541285e-05,
      "loss": 0.0575,
      "step": 420
    },
    {
      "epoch": 0.7654545454545455,
      "grad_norm": 0.3062519431114197,
      "learning_rate": 4.7706422018348626e-05,
      "loss": 0.0416,
      "step": 421
    },
    {
      "epoch": 0.7672727272727272,
      "grad_norm": 0.14600563049316406,
      "learning_rate": 4.733944954128441e-05,
      "loss": 0.0205,
      "step": 422
    },
    {
      "epoch": 0.769090909090909,
      "grad_norm": 0.20663271844387054,
      "learning_rate": 4.697247706422019e-05,
      "loss": 0.0209,
      "step": 423
    },
    {
      "epoch": 0.7709090909090909,
      "grad_norm": 0.33247870206832886,
      "learning_rate": 4.6605504587155965e-05,
      "loss": 0.0641,
      "step": 424
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.29924529790878296,
      "learning_rate": 4.623853211009175e-05,
      "loss": 0.0798,
      "step": 425
    },
    {
      "epoch": 0.7745454545454545,
      "grad_norm": 0.23131918907165527,
      "learning_rate": 4.587155963302753e-05,
      "loss": 0.0358,
      "step": 426
    },
    {
      "epoch": 0.7763636363636364,
      "grad_norm": 0.28837069869041443,
      "learning_rate": 4.5504587155963305e-05,
      "loss": 0.011,
      "step": 427
    },
    {
      "epoch": 0.7781818181818182,
      "grad_norm": 0.13393470644950867,
      "learning_rate": 4.513761467889909e-05,
      "loss": 0.0213,
      "step": 428
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.2652676999568939,
      "learning_rate": 4.477064220183487e-05,
      "loss": 0.0793,
      "step": 429
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.23593273758888245,
      "learning_rate": 4.4403669724770645e-05,
      "loss": 0.039,
      "step": 430
    },
    {
      "epoch": 0.7836363636363637,
      "grad_norm": 0.03157651424407959,
      "learning_rate": 4.403669724770643e-05,
      "loss": 0.0033,
      "step": 431
    },
    {
      "epoch": 0.7854545454545454,
      "grad_norm": 0.14107102155685425,
      "learning_rate": 4.366972477064221e-05,
      "loss": 0.0212,
      "step": 432
    },
    {
      "epoch": 0.7872727272727272,
      "grad_norm": 0.18101412057876587,
      "learning_rate": 4.3302752293577984e-05,
      "loss": 0.0309,
      "step": 433
    },
    {
      "epoch": 0.7890909090909091,
      "grad_norm": 0.14601176977157593,
      "learning_rate": 4.2935779816513766e-05,
      "loss": 0.0299,
      "step": 434
    },
    {
      "epoch": 0.7909090909090909,
      "grad_norm": 0.10073903948068619,
      "learning_rate": 4.256880733944955e-05,
      "loss": 0.0163,
      "step": 435
    },
    {
      "epoch": 0.7927272727272727,
      "grad_norm": 0.19026274979114532,
      "learning_rate": 4.2201834862385324e-05,
      "loss": 0.0301,
      "step": 436
    },
    {
      "epoch": 0.7945454545454546,
      "grad_norm": 0.11796621978282928,
      "learning_rate": 4.1834862385321106e-05,
      "loss": 0.0233,
      "step": 437
    },
    {
      "epoch": 0.7963636363636364,
      "grad_norm": 0.032002732157707214,
      "learning_rate": 4.146788990825689e-05,
      "loss": 0.0029,
      "step": 438
    },
    {
      "epoch": 0.7981818181818182,
      "grad_norm": 0.09133130311965942,
      "learning_rate": 4.1100917431192664e-05,
      "loss": 0.008,
      "step": 439
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.10926232486963272,
      "learning_rate": 4.0733944954128446e-05,
      "loss": 0.0051,
      "step": 440
    },
    {
      "epoch": 0.8018181818181818,
      "grad_norm": 0.08797521144151688,
      "learning_rate": 4.036697247706422e-05,
      "loss": 0.0071,
      "step": 441
    },
    {
      "epoch": 0.8036363636363636,
      "grad_norm": 0.1461970955133438,
      "learning_rate": 4e-05,
      "loss": 0.0185,
      "step": 442
    },
    {
      "epoch": 0.8054545454545454,
      "grad_norm": 0.1971481889486313,
      "learning_rate": 3.9633027522935785e-05,
      "loss": 0.0434,
      "step": 443
    },
    {
      "epoch": 0.8072727272727273,
      "grad_norm": 0.06447146832942963,
      "learning_rate": 3.926605504587156e-05,
      "loss": 0.0062,
      "step": 444
    },
    {
      "epoch": 0.8090909090909091,
      "grad_norm": 0.18071244657039642,
      "learning_rate": 3.889908256880734e-05,
      "loss": 0.0291,
      "step": 445
    },
    {
      "epoch": 0.8109090909090909,
      "grad_norm": 0.21332411468029022,
      "learning_rate": 3.8532110091743125e-05,
      "loss": 0.038,
      "step": 446
    },
    {
      "epoch": 0.8127272727272727,
      "grad_norm": 0.19486309587955475,
      "learning_rate": 3.81651376146789e-05,
      "loss": 0.0201,
      "step": 447
    },
    {
      "epoch": 0.8145454545454546,
      "grad_norm": 0.25246721506118774,
      "learning_rate": 3.779816513761468e-05,
      "loss": 0.0434,
      "step": 448
    },
    {
      "epoch": 0.8163636363636364,
      "grad_norm": 0.35398614406585693,
      "learning_rate": 3.7431192660550465e-05,
      "loss": 0.0879,
      "step": 449
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.11694852262735367,
      "learning_rate": 3.706422018348624e-05,
      "loss": 0.0108,
      "step": 450
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.11130383610725403,
      "learning_rate": 3.669724770642202e-05,
      "loss": 0.0209,
      "step": 451
    },
    {
      "epoch": 0.8218181818181818,
      "grad_norm": 0.07239571958780289,
      "learning_rate": 3.6330275229357804e-05,
      "loss": 0.0061,
      "step": 452
    },
    {
      "epoch": 0.8236363636363636,
      "grad_norm": 0.16864953935146332,
      "learning_rate": 3.596330275229358e-05,
      "loss": 0.0164,
      "step": 453
    },
    {
      "epoch": 0.8254545454545454,
      "grad_norm": 0.1347959041595459,
      "learning_rate": 3.559633027522936e-05,
      "loss": 0.009,
      "step": 454
    },
    {
      "epoch": 0.8272727272727273,
      "grad_norm": 0.16752712428569794,
      "learning_rate": 3.5229357798165144e-05,
      "loss": 0.0178,
      "step": 455
    },
    {
      "epoch": 0.8290909090909091,
      "grad_norm": 0.1415048986673355,
      "learning_rate": 3.486238532110092e-05,
      "loss": 0.0167,
      "step": 456
    },
    {
      "epoch": 0.8309090909090909,
      "grad_norm": 0.25138893723487854,
      "learning_rate": 3.44954128440367e-05,
      "loss": 0.0164,
      "step": 457
    },
    {
      "epoch": 0.8327272727272728,
      "grad_norm": 0.08080172538757324,
      "learning_rate": 3.4128440366972484e-05,
      "loss": 0.0107,
      "step": 458
    },
    {
      "epoch": 0.8345454545454546,
      "grad_norm": 0.12583664059638977,
      "learning_rate": 3.376146788990826e-05,
      "loss": 0.0162,
      "step": 459
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.20288221538066864,
      "learning_rate": 3.339449541284404e-05,
      "loss": 0.0342,
      "step": 460
    },
    {
      "epoch": 0.8381818181818181,
      "grad_norm": 0.02835817262530327,
      "learning_rate": 3.302752293577982e-05,
      "loss": 0.0022,
      "step": 461
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.016300085932016373,
      "learning_rate": 3.26605504587156e-05,
      "loss": 0.0016,
      "step": 462
    },
    {
      "epoch": 0.8418181818181818,
      "grad_norm": 0.23102638125419617,
      "learning_rate": 3.229357798165138e-05,
      "loss": 0.0165,
      "step": 463
    },
    {
      "epoch": 0.8436363636363636,
      "grad_norm": 0.2312186360359192,
      "learning_rate": 3.1926605504587156e-05,
      "loss": 0.0375,
      "step": 464
    },
    {
      "epoch": 0.8454545454545455,
      "grad_norm": 0.12822115421295166,
      "learning_rate": 3.155963302752294e-05,
      "loss": 0.0112,
      "step": 465
    },
    {
      "epoch": 0.8472727272727273,
      "grad_norm": 0.21223919093608856,
      "learning_rate": 3.119266055045872e-05,
      "loss": 0.0109,
      "step": 466
    },
    {
      "epoch": 0.8490909090909091,
      "grad_norm": 0.15735121071338654,
      "learning_rate": 3.0825688073394496e-05,
      "loss": 0.0202,
      "step": 467
    },
    {
      "epoch": 0.850909090909091,
      "grad_norm": 0.1728348433971405,
      "learning_rate": 3.0458715596330278e-05,
      "loss": 0.0168,
      "step": 468
    },
    {
      "epoch": 0.8527272727272728,
      "grad_norm": 0.11521036177873611,
      "learning_rate": 3.0091743119266057e-05,
      "loss": 0.0098,
      "step": 469
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.1642901450395584,
      "learning_rate": 2.972477064220184e-05,
      "loss": 0.0165,
      "step": 470
    },
    {
      "epoch": 0.8563636363636363,
      "grad_norm": 0.14384980499744415,
      "learning_rate": 2.9357798165137618e-05,
      "loss": 0.0183,
      "step": 471
    },
    {
      "epoch": 0.8581818181818182,
      "grad_norm": 0.2731875479221344,
      "learning_rate": 2.8990825688073397e-05,
      "loss": 0.0218,
      "step": 472
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.04424485191702843,
      "learning_rate": 2.862385321100918e-05,
      "loss": 0.0029,
      "step": 473
    },
    {
      "epoch": 0.8618181818181818,
      "grad_norm": 0.15878728032112122,
      "learning_rate": 2.8256880733944957e-05,
      "loss": 0.0122,
      "step": 474
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.18424800038337708,
      "learning_rate": 2.7889908256880736e-05,
      "loss": 0.0172,
      "step": 475
    },
    {
      "epoch": 0.8654545454545455,
      "grad_norm": 0.07480369508266449,
      "learning_rate": 2.7522935779816515e-05,
      "loss": 0.0064,
      "step": 476
    },
    {
      "epoch": 0.8672727272727273,
      "grad_norm": 0.10563183575868607,
      "learning_rate": 2.7155963302752297e-05,
      "loss": 0.0084,
      "step": 477
    },
    {
      "epoch": 0.8690909090909091,
      "grad_norm": 0.3786635100841522,
      "learning_rate": 2.6788990825688076e-05,
      "loss": 0.0819,
      "step": 478
    },
    {
      "epoch": 0.8709090909090909,
      "grad_norm": 0.2753799557685852,
      "learning_rate": 2.6422018348623855e-05,
      "loss": 0.0371,
      "step": 479
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.22636578977108002,
      "learning_rate": 2.6055045871559637e-05,
      "loss": 0.0308,
      "step": 480
    },
    {
      "epoch": 0.8745454545454545,
      "grad_norm": 0.14083132147789001,
      "learning_rate": 2.5688073394495416e-05,
      "loss": 0.0129,
      "step": 481
    },
    {
      "epoch": 0.8763636363636363,
      "grad_norm": 0.1564892679452896,
      "learning_rate": 2.5321100917431194e-05,
      "loss": 0.0182,
      "step": 482
    },
    {
      "epoch": 0.8781818181818182,
      "grad_norm": 0.30798789858818054,
      "learning_rate": 2.4954128440366977e-05,
      "loss": 0.0545,
      "step": 483
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.16281114518642426,
      "learning_rate": 2.4587155963302755e-05,
      "loss": 0.021,
      "step": 484
    },
    {
      "epoch": 0.8818181818181818,
      "grad_norm": 0.12024199217557907,
      "learning_rate": 2.4220183486238534e-05,
      "loss": 0.0061,
      "step": 485
    },
    {
      "epoch": 0.8836363636363637,
      "grad_norm": 0.20718863606452942,
      "learning_rate": 2.3853211009174313e-05,
      "loss": 0.022,
      "step": 486
    },
    {
      "epoch": 0.8854545454545455,
      "grad_norm": 0.08765348792076111,
      "learning_rate": 2.3486238532110095e-05,
      "loss": 0.0052,
      "step": 487
    },
    {
      "epoch": 0.8872727272727273,
      "grad_norm": 0.483629435300827,
      "learning_rate": 2.3119266055045874e-05,
      "loss": 0.0527,
      "step": 488
    },
    {
      "epoch": 0.889090909090909,
      "grad_norm": 0.21134799718856812,
      "learning_rate": 2.2752293577981652e-05,
      "loss": 0.0293,
      "step": 489
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.11641409248113632,
      "learning_rate": 2.2385321100917435e-05,
      "loss": 0.0066,
      "step": 490
    },
    {
      "epoch": 0.8927272727272727,
      "grad_norm": 0.244451105594635,
      "learning_rate": 2.2018348623853213e-05,
      "loss": 0.0386,
      "step": 491
    },
    {
      "epoch": 0.8945454545454545,
      "grad_norm": 0.27394944429397583,
      "learning_rate": 2.1651376146788992e-05,
      "loss": 0.0649,
      "step": 492
    },
    {
      "epoch": 0.8963636363636364,
      "grad_norm": 0.3319663405418396,
      "learning_rate": 2.1284403669724774e-05,
      "loss": 0.0429,
      "step": 493
    },
    {
      "epoch": 0.8981818181818182,
      "grad_norm": 0.23026172816753387,
      "learning_rate": 2.0917431192660553e-05,
      "loss": 0.0267,
      "step": 494
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.15952320396900177,
      "learning_rate": 2.0550458715596332e-05,
      "loss": 0.008,
      "step": 495
    },
    {
      "epoch": 0.9018181818181819,
      "grad_norm": 0.2832353413105011,
      "learning_rate": 2.018348623853211e-05,
      "loss": 0.0667,
      "step": 496
    },
    {
      "epoch": 0.9036363636363637,
      "grad_norm": 0.29508069157600403,
      "learning_rate": 1.9816513761467893e-05,
      "loss": 0.024,
      "step": 497
    },
    {
      "epoch": 0.9054545454545454,
      "grad_norm": 0.1372382789850235,
      "learning_rate": 1.944954128440367e-05,
      "loss": 0.0109,
      "step": 498
    },
    {
      "epoch": 0.9072727272727272,
      "grad_norm": 0.1254330575466156,
      "learning_rate": 1.908256880733945e-05,
      "loss": 0.0091,
      "step": 499
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.046702779829502106,
      "learning_rate": 1.8715596330275232e-05,
      "loss": 0.0032,
      "step": 500
    },
    {
      "epoch": 0.9109090909090909,
      "grad_norm": 0.12440414726734161,
      "learning_rate": 1.834862385321101e-05,
      "loss": 0.0126,
      "step": 501
    },
    {
      "epoch": 0.9127272727272727,
      "grad_norm": 0.303941011428833,
      "learning_rate": 1.798165137614679e-05,
      "loss": 0.0491,
      "step": 502
    },
    {
      "epoch": 0.9145454545454546,
      "grad_norm": 0.3837968409061432,
      "learning_rate": 1.7614678899082572e-05,
      "loss": 0.0446,
      "step": 503
    },
    {
      "epoch": 0.9163636363636364,
      "grad_norm": 0.1391468048095703,
      "learning_rate": 1.724770642201835e-05,
      "loss": 0.0222,
      "step": 504
    },
    {
      "epoch": 0.9181818181818182,
      "grad_norm": 0.20629771053791046,
      "learning_rate": 1.688073394495413e-05,
      "loss": 0.0245,
      "step": 505
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.4433112144470215,
      "learning_rate": 1.651376146788991e-05,
      "loss": 0.0594,
      "step": 506
    },
    {
      "epoch": 0.9218181818181819,
      "grad_norm": 0.1912693828344345,
      "learning_rate": 1.614678899082569e-05,
      "loss": 0.0524,
      "step": 507
    },
    {
      "epoch": 0.9236363636363636,
      "grad_norm": 0.2432246208190918,
      "learning_rate": 1.577981651376147e-05,
      "loss": 0.0403,
      "step": 508
    },
    {
      "epoch": 0.9254545454545454,
      "grad_norm": 0.2384028136730194,
      "learning_rate": 1.5412844036697248e-05,
      "loss": 0.0413,
      "step": 509
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.34047985076904297,
      "learning_rate": 1.5045871559633028e-05,
      "loss": 0.0471,
      "step": 510
    },
    {
      "epoch": 0.9290909090909091,
      "grad_norm": 0.12733031809329987,
      "learning_rate": 1.4678899082568809e-05,
      "loss": 0.0088,
      "step": 511
    },
    {
      "epoch": 0.9309090909090909,
      "grad_norm": 0.16392409801483154,
      "learning_rate": 1.431192660550459e-05,
      "loss": 0.0173,
      "step": 512
    },
    {
      "epoch": 0.9327272727272727,
      "grad_norm": 0.21244138479232788,
      "learning_rate": 1.3944954128440368e-05,
      "loss": 0.017,
      "step": 513
    },
    {
      "epoch": 0.9345454545454546,
      "grad_norm": 0.3079732060432434,
      "learning_rate": 1.3577981651376149e-05,
      "loss": 0.0667,
      "step": 514
    },
    {
      "epoch": 0.9363636363636364,
      "grad_norm": 0.17854014039039612,
      "learning_rate": 1.3211009174311927e-05,
      "loss": 0.0344,
      "step": 515
    },
    {
      "epoch": 0.9381818181818182,
      "grad_norm": 0.31165027618408203,
      "learning_rate": 1.2844036697247708e-05,
      "loss": 0.082,
      "step": 516
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.5011227130889893,
      "learning_rate": 1.2477064220183488e-05,
      "loss": 0.2446,
      "step": 517
    },
    {
      "epoch": 0.9418181818181818,
      "grad_norm": 0.16109666228294373,
      "learning_rate": 1.2110091743119267e-05,
      "loss": 0.0295,
      "step": 518
    },
    {
      "epoch": 0.9436363636363636,
      "grad_norm": 0.194817453622818,
      "learning_rate": 1.1743119266055047e-05,
      "loss": 0.0151,
      "step": 519
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.15853719413280487,
      "learning_rate": 1.1376146788990826e-05,
      "loss": 0.0212,
      "step": 520
    },
    {
      "epoch": 0.9472727272727273,
      "grad_norm": 0.10337577760219574,
      "learning_rate": 1.1009174311926607e-05,
      "loss": 0.0108,
      "step": 521
    },
    {
      "epoch": 0.9490909090909091,
      "grad_norm": 0.34139883518218994,
      "learning_rate": 1.0642201834862387e-05,
      "loss": 0.1001,
      "step": 522
    },
    {
      "epoch": 0.9509090909090909,
      "grad_norm": 0.17268864810466766,
      "learning_rate": 1.0275229357798166e-05,
      "loss": 0.0335,
      "step": 523
    },
    {
      "epoch": 0.9527272727272728,
      "grad_norm": 0.24859726428985596,
      "learning_rate": 9.908256880733946e-06,
      "loss": 0.0415,
      "step": 524
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.3757864832878113,
      "learning_rate": 9.541284403669725e-06,
      "loss": 0.1257,
      "step": 525
    },
    {
      "epoch": 0.9563636363636364,
      "grad_norm": 0.3081084191799164,
      "learning_rate": 9.174311926605506e-06,
      "loss": 0.074,
      "step": 526
    },
    {
      "epoch": 0.9581818181818181,
      "grad_norm": 0.18394875526428223,
      "learning_rate": 8.807339449541286e-06,
      "loss": 0.0236,
      "step": 527
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.07017572224140167,
      "learning_rate": 8.440366972477065e-06,
      "loss": 0.0054,
      "step": 528
    },
    {
      "epoch": 0.9618181818181818,
      "grad_norm": 0.2929394245147705,
      "learning_rate": 8.073394495412845e-06,
      "loss": 0.085,
      "step": 529
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.4189014136791229,
      "learning_rate": 7.706422018348624e-06,
      "loss": 0.0515,
      "step": 530
    },
    {
      "epoch": 0.9654545454545455,
      "grad_norm": 0.5744812488555908,
      "learning_rate": 7.3394495412844045e-06,
      "loss": 0.0832,
      "step": 531
    },
    {
      "epoch": 0.9672727272727273,
      "grad_norm": 0.09619303047657013,
      "learning_rate": 6.972477064220184e-06,
      "loss": 0.0083,
      "step": 532
    },
    {
      "epoch": 0.9690909090909091,
      "grad_norm": 0.1365353763103485,
      "learning_rate": 6.605504587155964e-06,
      "loss": 0.0225,
      "step": 533
    },
    {
      "epoch": 0.9709090909090909,
      "grad_norm": 0.1599094718694687,
      "learning_rate": 6.238532110091744e-06,
      "loss": 0.0219,
      "step": 534
    },
    {
      "epoch": 0.9727272727272728,
      "grad_norm": 0.16075393557548523,
      "learning_rate": 5.871559633027524e-06,
      "loss": 0.0202,
      "step": 535
    },
    {
      "epoch": 0.9745454545454545,
      "grad_norm": 0.44821134209632874,
      "learning_rate": 5.504587155963303e-06,
      "loss": 0.1493,
      "step": 536
    },
    {
      "epoch": 0.9763636363636363,
      "grad_norm": 0.15099748969078064,
      "learning_rate": 5.137614678899083e-06,
      "loss": 0.0068,
      "step": 537
    },
    {
      "epoch": 0.9781818181818182,
      "grad_norm": 0.030339067801833153,
      "learning_rate": 4.7706422018348626e-06,
      "loss": 0.0033,
      "step": 538
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.2457229495048523,
      "learning_rate": 4.403669724770643e-06,
      "loss": 0.0518,
      "step": 539
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.33485302329063416,
      "learning_rate": 4.036697247706423e-06,
      "loss": 0.0277,
      "step": 540
    },
    {
      "epoch": 0.9836363636363636,
      "grad_norm": 0.09638965129852295,
      "learning_rate": 3.6697247706422022e-06,
      "loss": 0.0073,
      "step": 541
    },
    {
      "epoch": 0.9854545454545455,
      "grad_norm": 0.1941184401512146,
      "learning_rate": 3.302752293577982e-06,
      "loss": 0.017,
      "step": 542
    },
    {
      "epoch": 0.9872727272727273,
      "grad_norm": 0.05182204768061638,
      "learning_rate": 2.935779816513762e-06,
      "loss": 0.004,
      "step": 543
    },
    {
      "epoch": 0.9890909090909091,
      "grad_norm": 0.10093339532613754,
      "learning_rate": 2.5688073394495415e-06,
      "loss": 0.019,
      "step": 544
    },
    {
      "epoch": 0.990909090909091,
      "grad_norm": 0.15293647348880768,
      "learning_rate": 2.2018348623853215e-06,
      "loss": 0.0239,
      "step": 545
    },
    {
      "epoch": 0.9927272727272727,
      "grad_norm": 0.09899584949016571,
      "learning_rate": 1.8348623853211011e-06,
      "loss": 0.0086,
      "step": 546
    },
    {
      "epoch": 0.9945454545454545,
      "grad_norm": 0.1165822297334671,
      "learning_rate": 1.467889908256881e-06,
      "loss": 0.0071,
      "step": 547
    },
    {
      "epoch": 0.9963636363636363,
      "grad_norm": 0.09996416419744492,
      "learning_rate": 1.1009174311926608e-06,
      "loss": 0.0131,
      "step": 548
    },
    {
      "epoch": 0.9981818181818182,
      "grad_norm": 0.11236351728439331,
      "learning_rate": 7.339449541284405e-07,
      "loss": 0.0089,
      "step": 549
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1914595663547516,
      "learning_rate": 3.6697247706422023e-07,
      "loss": 0.0266,
      "step": 550
    }
  ],
  "logging_steps": 1,
  "max_steps": 550,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.149330657996964e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
