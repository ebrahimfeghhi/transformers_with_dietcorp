{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 50,
  "global_step": 629,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001589825119236884,
      "grad_norm": 1.4262330532073975,
      "learning_rate": 0.0,
      "loss": 0.5049,
      "step": 1
    },
    {
      "epoch": 0.003179650238473768,
      "grad_norm": 2.3885185718536377,
      "learning_rate": 4e-05,
      "loss": 0.3356,
      "step": 2
    },
    {
      "epoch": 0.0047694753577106515,
      "grad_norm": 1.8109177350997925,
      "learning_rate": 8e-05,
      "loss": 0.1745,
      "step": 3
    },
    {
      "epoch": 0.006359300476947536,
      "grad_norm": 1.6046630144119263,
      "learning_rate": 0.00012,
      "loss": 0.1749,
      "step": 4
    },
    {
      "epoch": 0.00794912559618442,
      "grad_norm": 0.5475466251373291,
      "learning_rate": 0.00016,
      "loss": 0.3157,
      "step": 5
    },
    {
      "epoch": 0.009538950715421303,
      "grad_norm": 0.00028233087505213916,
      "learning_rate": 0.0002,
      "loss": 0.0005,
      "step": 6
    },
    {
      "epoch": 0.011128775834658187,
      "grad_norm": 0.4783531725406647,
      "learning_rate": 0.00019967948717948718,
      "loss": 0.0083,
      "step": 7
    },
    {
      "epoch": 0.012718600953895072,
      "grad_norm": 0.871662437915802,
      "learning_rate": 0.00019935897435897437,
      "loss": 0.1595,
      "step": 8
    },
    {
      "epoch": 0.014308426073131956,
      "grad_norm": 0.23768723011016846,
      "learning_rate": 0.00019903846153846154,
      "loss": 0.0096,
      "step": 9
    },
    {
      "epoch": 0.01589825119236884,
      "grad_norm": 0.4152223765850067,
      "learning_rate": 0.00019871794871794874,
      "loss": 0.1683,
      "step": 10
    },
    {
      "epoch": 0.017488076311605722,
      "grad_norm": 0.38095682859420776,
      "learning_rate": 0.0001983974358974359,
      "loss": 0.1067,
      "step": 11
    },
    {
      "epoch": 0.019077901430842606,
      "grad_norm": 0.4597535729408264,
      "learning_rate": 0.0001980769230769231,
      "loss": 0.1693,
      "step": 12
    },
    {
      "epoch": 0.02066772655007949,
      "grad_norm": 0.2678505778312683,
      "learning_rate": 0.00019775641025641027,
      "loss": 0.047,
      "step": 13
    },
    {
      "epoch": 0.022257551669316374,
      "grad_norm": 0.30955255031585693,
      "learning_rate": 0.00019743589743589744,
      "loss": 0.1232,
      "step": 14
    },
    {
      "epoch": 0.02384737678855326,
      "grad_norm": 0.22422584891319275,
      "learning_rate": 0.00019711538461538464,
      "loss": 0.0161,
      "step": 15
    },
    {
      "epoch": 0.025437201907790145,
      "grad_norm": 0.39825260639190674,
      "learning_rate": 0.00019679487179487178,
      "loss": 0.0637,
      "step": 16
    },
    {
      "epoch": 0.02702702702702703,
      "grad_norm": 0.4597500264644623,
      "learning_rate": 0.00019647435897435898,
      "loss": 0.1987,
      "step": 17
    },
    {
      "epoch": 0.028616852146263912,
      "grad_norm": 0.5145430564880371,
      "learning_rate": 0.00019615384615384615,
      "loss": 0.1318,
      "step": 18
    },
    {
      "epoch": 0.030206677265500796,
      "grad_norm": 0.36839836835861206,
      "learning_rate": 0.00019583333333333334,
      "loss": 0.1584,
      "step": 19
    },
    {
      "epoch": 0.03179650238473768,
      "grad_norm": 0.2272891104221344,
      "learning_rate": 0.0001955128205128205,
      "loss": 0.0908,
      "step": 20
    },
    {
      "epoch": 0.033386327503974564,
      "grad_norm": 0.31373631954193115,
      "learning_rate": 0.0001951923076923077,
      "loss": 0.1327,
      "step": 21
    },
    {
      "epoch": 0.034976152623211444,
      "grad_norm": 0.143495112657547,
      "learning_rate": 0.00019487179487179487,
      "loss": 0.0205,
      "step": 22
    },
    {
      "epoch": 0.03656597774244833,
      "grad_norm": 0.17150747776031494,
      "learning_rate": 0.00019455128205128207,
      "loss": 0.0726,
      "step": 23
    },
    {
      "epoch": 0.03815580286168521,
      "grad_norm": 0.16530999541282654,
      "learning_rate": 0.00019423076923076924,
      "loss": 0.033,
      "step": 24
    },
    {
      "epoch": 0.0397456279809221,
      "grad_norm": 0.27525508403778076,
      "learning_rate": 0.0001939102564102564,
      "loss": 0.1003,
      "step": 25
    },
    {
      "epoch": 0.04133545310015898,
      "grad_norm": 0.27830928564071655,
      "learning_rate": 0.0001935897435897436,
      "loss": 0.1243,
      "step": 26
    },
    {
      "epoch": 0.04292527821939587,
      "grad_norm": 18.95360565185547,
      "learning_rate": 0.00019326923076923077,
      "loss": 0.0223,
      "step": 27
    },
    {
      "epoch": 0.04451510333863275,
      "grad_norm": 0.33224785327911377,
      "learning_rate": 0.00019294871794871797,
      "loss": 0.1903,
      "step": 28
    },
    {
      "epoch": 0.046104928457869634,
      "grad_norm": 0.23611438274383545,
      "learning_rate": 0.00019262820512820514,
      "loss": 0.0525,
      "step": 29
    },
    {
      "epoch": 0.04769475357710652,
      "grad_norm": 0.4285670518875122,
      "learning_rate": 0.00019230769230769233,
      "loss": 0.1859,
      "step": 30
    },
    {
      "epoch": 0.0492845786963434,
      "grad_norm": 0.4020407795906067,
      "learning_rate": 0.0001919871794871795,
      "loss": 0.1932,
      "step": 31
    },
    {
      "epoch": 0.05087440381558029,
      "grad_norm": 0.6257483959197998,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.4301,
      "step": 32
    },
    {
      "epoch": 0.05246422893481717,
      "grad_norm": 0.2771381139755249,
      "learning_rate": 0.00019134615384615387,
      "loss": 0.0679,
      "step": 33
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": 0.33565187454223633,
      "learning_rate": 0.00019102564102564104,
      "loss": 0.2,
      "step": 34
    },
    {
      "epoch": 0.05564387917329094,
      "grad_norm": 0.19069908559322357,
      "learning_rate": 0.00019070512820512823,
      "loss": 0.0803,
      "step": 35
    },
    {
      "epoch": 0.057233704292527825,
      "grad_norm": 0.23087917268276215,
      "learning_rate": 0.00019038461538461538,
      "loss": 0.063,
      "step": 36
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 0.22795426845550537,
      "learning_rate": 0.00019006410256410257,
      "loss": 0.0898,
      "step": 37
    },
    {
      "epoch": 0.06041335453100159,
      "grad_norm": 0.039017193019390106,
      "learning_rate": 0.00018974358974358974,
      "loss": 0.0047,
      "step": 38
    },
    {
      "epoch": 0.06200317965023847,
      "grad_norm": 0.190594881772995,
      "learning_rate": 0.00018942307692307694,
      "loss": 0.0434,
      "step": 39
    },
    {
      "epoch": 0.06359300476947535,
      "grad_norm": 0.0254549290984869,
      "learning_rate": 0.0001891025641025641,
      "loss": 0.0044,
      "step": 40
    },
    {
      "epoch": 0.06518282988871224,
      "grad_norm": 0.13332459330558777,
      "learning_rate": 0.00018878205128205127,
      "loss": 0.0355,
      "step": 41
    },
    {
      "epoch": 0.06677265500794913,
      "grad_norm": 0.4952179789543152,
      "learning_rate": 0.00018846153846153847,
      "loss": 0.2445,
      "step": 42
    },
    {
      "epoch": 0.06836248012718601,
      "grad_norm": 0.33445829153060913,
      "learning_rate": 0.00018814102564102564,
      "loss": 0.1786,
      "step": 43
    },
    {
      "epoch": 0.06995230524642289,
      "grad_norm": 0.29577013850212097,
      "learning_rate": 0.00018782051282051283,
      "loss": 0.0443,
      "step": 44
    },
    {
      "epoch": 0.07154213036565978,
      "grad_norm": 0.7259592413902283,
      "learning_rate": 0.0001875,
      "loss": 0.0229,
      "step": 45
    },
    {
      "epoch": 0.07313195548489666,
      "grad_norm": 0.20380666851997375,
      "learning_rate": 0.0001871794871794872,
      "loss": 0.0812,
      "step": 46
    },
    {
      "epoch": 0.07472178060413355,
      "grad_norm": 0.27101045846939087,
      "learning_rate": 0.00018685897435897437,
      "loss": 0.1131,
      "step": 47
    },
    {
      "epoch": 0.07631160572337042,
      "grad_norm": 0.0161459781229496,
      "learning_rate": 0.00018653846153846154,
      "loss": 0.0031,
      "step": 48
    },
    {
      "epoch": 0.07790143084260731,
      "grad_norm": 0.40508928894996643,
      "learning_rate": 0.00018621794871794873,
      "loss": 0.0989,
      "step": 49
    },
    {
      "epoch": 0.0794912559618442,
      "grad_norm": 0.11442727595567703,
      "learning_rate": 0.0001858974358974359,
      "loss": 0.0123,
      "step": 50
    },
    {
      "epoch": 0.08108108108108109,
      "grad_norm": 0.016786497086286545,
      "learning_rate": 0.0001855769230769231,
      "loss": 0.0029,
      "step": 51
    },
    {
      "epoch": 0.08267090620031796,
      "grad_norm": 0.027301492169499397,
      "learning_rate": 0.00018525641025641027,
      "loss": 0.0029,
      "step": 52
    },
    {
      "epoch": 0.08426073131955485,
      "grad_norm": 0.8909986615180969,
      "learning_rate": 0.00018493589743589746,
      "loss": 0.0338,
      "step": 53
    },
    {
      "epoch": 0.08585055643879173,
      "grad_norm": 0.5908395051956177,
      "learning_rate": 0.00018461538461538463,
      "loss": 0.363,
      "step": 54
    },
    {
      "epoch": 0.08744038155802862,
      "grad_norm": 0.26042112708091736,
      "learning_rate": 0.0001842948717948718,
      "loss": 0.0958,
      "step": 55
    },
    {
      "epoch": 0.0890302066772655,
      "grad_norm": 0.5254289507865906,
      "learning_rate": 0.00018397435897435897,
      "loss": 0.0325,
      "step": 56
    },
    {
      "epoch": 0.09062003179650238,
      "grad_norm": 0.3558183014392853,
      "learning_rate": 0.00018365384615384617,
      "loss": 0.0379,
      "step": 57
    },
    {
      "epoch": 0.09220985691573927,
      "grad_norm": 0.38113075494766235,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.1197,
      "step": 58
    },
    {
      "epoch": 0.09379968203497616,
      "grad_norm": 0.322793185710907,
      "learning_rate": 0.0001830128205128205,
      "loss": 0.0351,
      "step": 59
    },
    {
      "epoch": 0.09538950715421304,
      "grad_norm": 0.5246855616569519,
      "learning_rate": 0.0001826923076923077,
      "loss": 0.1244,
      "step": 60
    },
    {
      "epoch": 0.09697933227344992,
      "grad_norm": 0.5532249212265015,
      "learning_rate": 0.00018237179487179487,
      "loss": 0.1193,
      "step": 61
    },
    {
      "epoch": 0.0985691573926868,
      "grad_norm": 0.06948810070753098,
      "learning_rate": 0.00018205128205128207,
      "loss": 0.0103,
      "step": 62
    },
    {
      "epoch": 0.10015898251192369,
      "grad_norm": 0.6311110854148865,
      "learning_rate": 0.00018173076923076923,
      "loss": 0.2796,
      "step": 63
    },
    {
      "epoch": 0.10174880763116058,
      "grad_norm": 0.4108875095844269,
      "learning_rate": 0.00018141025641025643,
      "loss": 0.0695,
      "step": 64
    },
    {
      "epoch": 0.10333863275039745,
      "grad_norm": 0.28577694296836853,
      "learning_rate": 0.0001810897435897436,
      "loss": 0.1164,
      "step": 65
    },
    {
      "epoch": 0.10492845786963434,
      "grad_norm": 0.1612689346075058,
      "learning_rate": 0.00018076923076923077,
      "loss": 0.0244,
      "step": 66
    },
    {
      "epoch": 0.10651828298887123,
      "grad_norm": 0.053874701261520386,
      "learning_rate": 0.00018044871794871796,
      "loss": 0.0088,
      "step": 67
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 0.2818957567214966,
      "learning_rate": 0.00018012820512820513,
      "loss": 0.0506,
      "step": 68
    },
    {
      "epoch": 0.10969793322734499,
      "grad_norm": 0.16873519122600555,
      "learning_rate": 0.00017980769230769233,
      "loss": 0.0329,
      "step": 69
    },
    {
      "epoch": 0.11128775834658187,
      "grad_norm": 0.12725022435188293,
      "learning_rate": 0.0001794871794871795,
      "loss": 0.0202,
      "step": 70
    },
    {
      "epoch": 0.11287758346581876,
      "grad_norm": 0.3488382399082184,
      "learning_rate": 0.0001791666666666667,
      "loss": 0.1081,
      "step": 71
    },
    {
      "epoch": 0.11446740858505565,
      "grad_norm": 0.11585051566362381,
      "learning_rate": 0.00017884615384615386,
      "loss": 0.0162,
      "step": 72
    },
    {
      "epoch": 0.11605723370429252,
      "grad_norm": 0.5479907393455505,
      "learning_rate": 0.00017852564102564103,
      "loss": 0.1652,
      "step": 73
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.861056923866272,
      "learning_rate": 0.00017820512820512823,
      "loss": 0.2215,
      "step": 74
    },
    {
      "epoch": 0.1192368839427663,
      "grad_norm": 0.31831133365631104,
      "learning_rate": 0.00017788461538461537,
      "loss": 0.1105,
      "step": 75
    },
    {
      "epoch": 0.12082670906200318,
      "grad_norm": 0.23487530648708344,
      "learning_rate": 0.00017756410256410257,
      "loss": 0.0275,
      "step": 76
    },
    {
      "epoch": 0.12241653418124006,
      "grad_norm": 0.7985079288482666,
      "learning_rate": 0.00017724358974358973,
      "loss": 0.4167,
      "step": 77
    },
    {
      "epoch": 0.12400635930047695,
      "grad_norm": 0.307547926902771,
      "learning_rate": 0.00017692307692307693,
      "loss": 0.0691,
      "step": 78
    },
    {
      "epoch": 0.12559618441971382,
      "grad_norm": 0.24701562523841858,
      "learning_rate": 0.0001766025641025641,
      "loss": 0.0347,
      "step": 79
    },
    {
      "epoch": 0.1271860095389507,
      "grad_norm": 0.1826268434524536,
      "learning_rate": 0.0001762820512820513,
      "loss": 0.0288,
      "step": 80
    },
    {
      "epoch": 0.1287758346581876,
      "grad_norm": 0.2271430641412735,
      "learning_rate": 0.00017596153846153846,
      "loss": 0.0802,
      "step": 81
    },
    {
      "epoch": 0.13036565977742448,
      "grad_norm": 0.5056482553482056,
      "learning_rate": 0.00017564102564102566,
      "loss": 0.3233,
      "step": 82
    },
    {
      "epoch": 0.13195548489666137,
      "grad_norm": 0.5126563310623169,
      "learning_rate": 0.00017532051282051283,
      "loss": 0.1988,
      "step": 83
    },
    {
      "epoch": 0.13354531001589826,
      "grad_norm": 0.36526066064834595,
      "learning_rate": 0.000175,
      "loss": 0.1119,
      "step": 84
    },
    {
      "epoch": 0.13513513513513514,
      "grad_norm": 0.31568992137908936,
      "learning_rate": 0.0001746794871794872,
      "loss": 0.0549,
      "step": 85
    },
    {
      "epoch": 0.13672496025437203,
      "grad_norm": 0.27643805742263794,
      "learning_rate": 0.00017435897435897436,
      "loss": 0.0529,
      "step": 86
    },
    {
      "epoch": 0.1383147853736089,
      "grad_norm": 0.10486604273319244,
      "learning_rate": 0.00017403846153846156,
      "loss": 0.0108,
      "step": 87
    },
    {
      "epoch": 0.13990461049284578,
      "grad_norm": 0.469613641500473,
      "learning_rate": 0.00017371794871794873,
      "loss": 0.0818,
      "step": 88
    },
    {
      "epoch": 0.14149443561208266,
      "grad_norm": 0.3125797510147095,
      "learning_rate": 0.00017339743589743592,
      "loss": 0.102,
      "step": 89
    },
    {
      "epoch": 0.14308426073131955,
      "grad_norm": 0.19230173528194427,
      "learning_rate": 0.0001730769230769231,
      "loss": 0.0957,
      "step": 90
    },
    {
      "epoch": 0.14467408585055644,
      "grad_norm": 0.05057496950030327,
      "learning_rate": 0.00017275641025641026,
      "loss": 0.0061,
      "step": 91
    },
    {
      "epoch": 0.14626391096979333,
      "grad_norm": 0.12305531650781631,
      "learning_rate": 0.00017243589743589746,
      "loss": 0.0097,
      "step": 92
    },
    {
      "epoch": 0.1478537360890302,
      "grad_norm": 0.5133407711982727,
      "learning_rate": 0.00017211538461538463,
      "loss": 0.1353,
      "step": 93
    },
    {
      "epoch": 0.1494435612082671,
      "grad_norm": 0.5778212547302246,
      "learning_rate": 0.0001717948717948718,
      "loss": 0.2159,
      "step": 94
    },
    {
      "epoch": 0.151033386327504,
      "grad_norm": 0.2372937798500061,
      "learning_rate": 0.00017147435897435896,
      "loss": 0.0808,
      "step": 95
    },
    {
      "epoch": 0.15262321144674085,
      "grad_norm": 0.3155849874019623,
      "learning_rate": 0.00017115384615384616,
      "loss": 0.0896,
      "step": 96
    },
    {
      "epoch": 0.15421303656597773,
      "grad_norm": 0.2555392384529114,
      "learning_rate": 0.00017083333333333333,
      "loss": 0.0564,
      "step": 97
    },
    {
      "epoch": 0.15580286168521462,
      "grad_norm": 0.5136285424232483,
      "learning_rate": 0.00017051282051282053,
      "loss": 0.0644,
      "step": 98
    },
    {
      "epoch": 0.1573926868044515,
      "grad_norm": 0.240817129611969,
      "learning_rate": 0.0001701923076923077,
      "loss": 0.1134,
      "step": 99
    },
    {
      "epoch": 0.1589825119236884,
      "grad_norm": 0.2676571011543274,
      "learning_rate": 0.00016987179487179486,
      "loss": 0.0938,
      "step": 100
    },
    {
      "epoch": 0.16057233704292528,
      "grad_norm": 0.10100515931844711,
      "learning_rate": 0.00016955128205128206,
      "loss": 0.0127,
      "step": 101
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 0.07447593659162521,
      "learning_rate": 0.00016923076923076923,
      "loss": 0.01,
      "step": 102
    },
    {
      "epoch": 0.16375198728139906,
      "grad_norm": 0.0933547094464302,
      "learning_rate": 0.00016891025641025642,
      "loss": 0.0138,
      "step": 103
    },
    {
      "epoch": 0.16534181240063592,
      "grad_norm": 0.1554928869009018,
      "learning_rate": 0.0001685897435897436,
      "loss": 0.0244,
      "step": 104
    },
    {
      "epoch": 0.1669316375198728,
      "grad_norm": 0.18091414868831635,
      "learning_rate": 0.0001682692307692308,
      "loss": 0.0462,
      "step": 105
    },
    {
      "epoch": 0.1685214626391097,
      "grad_norm": 0.11257199198007584,
      "learning_rate": 0.00016794871794871796,
      "loss": 0.0171,
      "step": 106
    },
    {
      "epoch": 0.17011128775834658,
      "grad_norm": 0.1391802728176117,
      "learning_rate": 0.00016762820512820513,
      "loss": 0.0221,
      "step": 107
    },
    {
      "epoch": 0.17170111287758347,
      "grad_norm": 0.4835328459739685,
      "learning_rate": 0.00016730769230769232,
      "loss": 0.1553,
      "step": 108
    },
    {
      "epoch": 0.17329093799682035,
      "grad_norm": 0.22395801544189453,
      "learning_rate": 0.0001669871794871795,
      "loss": 0.0722,
      "step": 109
    },
    {
      "epoch": 0.17488076311605724,
      "grad_norm": 0.09505224972963333,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.0115,
      "step": 110
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.2222518026828766,
      "learning_rate": 0.00016634615384615386,
      "loss": 0.0557,
      "step": 111
    },
    {
      "epoch": 0.178060413354531,
      "grad_norm": 0.22806501388549805,
      "learning_rate": 0.00016602564102564105,
      "loss": 0.0554,
      "step": 112
    },
    {
      "epoch": 0.17965023847376788,
      "grad_norm": 0.046130627393722534,
      "learning_rate": 0.00016570512820512822,
      "loss": 0.0056,
      "step": 113
    },
    {
      "epoch": 0.18124006359300476,
      "grad_norm": 0.26400595903396606,
      "learning_rate": 0.0001653846153846154,
      "loss": 0.0813,
      "step": 114
    },
    {
      "epoch": 0.18282988871224165,
      "grad_norm": 0.1652650088071823,
      "learning_rate": 0.00016506410256410256,
      "loss": 0.0262,
      "step": 115
    },
    {
      "epoch": 0.18441971383147854,
      "grad_norm": 0.025349829345941544,
      "learning_rate": 0.00016474358974358976,
      "loss": 0.0038,
      "step": 116
    },
    {
      "epoch": 0.18600953895071543,
      "grad_norm": 0.04871387407183647,
      "learning_rate": 0.00016442307692307692,
      "loss": 0.0044,
      "step": 117
    },
    {
      "epoch": 0.1875993640699523,
      "grad_norm": 0.6301618218421936,
      "learning_rate": 0.0001641025641025641,
      "loss": 0.1429,
      "step": 118
    },
    {
      "epoch": 0.1891891891891892,
      "grad_norm": 0.05472850427031517,
      "learning_rate": 0.0001637820512820513,
      "loss": 0.0053,
      "step": 119
    },
    {
      "epoch": 0.1907790143084261,
      "grad_norm": 0.057183727622032166,
      "learning_rate": 0.00016346153846153846,
      "loss": 0.0068,
      "step": 120
    },
    {
      "epoch": 0.19236883942766295,
      "grad_norm": 0.13320955634117126,
      "learning_rate": 0.00016314102564102565,
      "loss": 0.0247,
      "step": 121
    },
    {
      "epoch": 0.19395866454689983,
      "grad_norm": 0.6101327538490295,
      "learning_rate": 0.00016282051282051282,
      "loss": 0.1346,
      "step": 122
    },
    {
      "epoch": 0.19554848966613672,
      "grad_norm": 0.47875115275382996,
      "learning_rate": 0.00016250000000000002,
      "loss": 0.1105,
      "step": 123
    },
    {
      "epoch": 0.1971383147853736,
      "grad_norm": 1.3784394264221191,
      "learning_rate": 0.0001621794871794872,
      "loss": 0.7509,
      "step": 124
    },
    {
      "epoch": 0.1987281399046105,
      "grad_norm": 0.07454273849725723,
      "learning_rate": 0.00016185897435897436,
      "loss": 0.0108,
      "step": 125
    },
    {
      "epoch": 0.20031796502384738,
      "grad_norm": 0.281345009803772,
      "learning_rate": 0.00016153846153846155,
      "loss": 0.0734,
      "step": 126
    },
    {
      "epoch": 0.20190779014308427,
      "grad_norm": 0.4562978446483612,
      "learning_rate": 0.00016121794871794872,
      "loss": 0.0726,
      "step": 127
    },
    {
      "epoch": 0.20349761526232116,
      "grad_norm": 0.1783936470746994,
      "learning_rate": 0.00016089743589743592,
      "loss": 0.015,
      "step": 128
    },
    {
      "epoch": 0.20508744038155802,
      "grad_norm": 0.0973133072257042,
      "learning_rate": 0.0001605769230769231,
      "loss": 0.0108,
      "step": 129
    },
    {
      "epoch": 0.2066772655007949,
      "grad_norm": 0.39501118659973145,
      "learning_rate": 0.00016025641025641028,
      "loss": 0.0864,
      "step": 130
    },
    {
      "epoch": 0.2082670906200318,
      "grad_norm": 0.5723370909690857,
      "learning_rate": 0.00015993589743589745,
      "loss": 0.1319,
      "step": 131
    },
    {
      "epoch": 0.20985691573926868,
      "grad_norm": 0.46184465289115906,
      "learning_rate": 0.00015961538461538462,
      "loss": 0.112,
      "step": 132
    },
    {
      "epoch": 0.21144674085850557,
      "grad_norm": 0.0755009651184082,
      "learning_rate": 0.0001592948717948718,
      "loss": 0.0102,
      "step": 133
    },
    {
      "epoch": 0.21303656597774245,
      "grad_norm": 0.44719627499580383,
      "learning_rate": 0.00015897435897435896,
      "loss": 0.1124,
      "step": 134
    },
    {
      "epoch": 0.21462639109697934,
      "grad_norm": 0.28039753437042236,
      "learning_rate": 0.00015865384615384616,
      "loss": 0.059,
      "step": 135
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 0.7138969302177429,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.4132,
      "step": 136
    },
    {
      "epoch": 0.2178060413354531,
      "grad_norm": 0.10978276282548904,
      "learning_rate": 0.00015801282051282052,
      "loss": 0.0069,
      "step": 137
    },
    {
      "epoch": 0.21939586645468998,
      "grad_norm": 0.15510642528533936,
      "learning_rate": 0.0001576923076923077,
      "loss": 0.0064,
      "step": 138
    },
    {
      "epoch": 0.22098569157392686,
      "grad_norm": 0.7322877645492554,
      "learning_rate": 0.00015737179487179488,
      "loss": 0.1376,
      "step": 139
    },
    {
      "epoch": 0.22257551669316375,
      "grad_norm": 0.6868706345558167,
      "learning_rate": 0.00015705128205128205,
      "loss": 0.123,
      "step": 140
    },
    {
      "epoch": 0.22416534181240064,
      "grad_norm": 0.253219336271286,
      "learning_rate": 0.00015673076923076925,
      "loss": 0.0324,
      "step": 141
    },
    {
      "epoch": 0.22575516693163752,
      "grad_norm": 0.09452369064092636,
      "learning_rate": 0.00015641025641025642,
      "loss": 0.0099,
      "step": 142
    },
    {
      "epoch": 0.2273449920508744,
      "grad_norm": 0.1280839741230011,
      "learning_rate": 0.0001560897435897436,
      "loss": 0.0214,
      "step": 143
    },
    {
      "epoch": 0.2289348171701113,
      "grad_norm": 0.2901113033294678,
      "learning_rate": 0.00015576923076923078,
      "loss": 0.0668,
      "step": 144
    },
    {
      "epoch": 0.23052464228934816,
      "grad_norm": 0.37370768189430237,
      "learning_rate": 0.00015544871794871795,
      "loss": 0.1455,
      "step": 145
    },
    {
      "epoch": 0.23211446740858505,
      "grad_norm": 0.028219183906912804,
      "learning_rate": 0.00015512820512820515,
      "loss": 0.0043,
      "step": 146
    },
    {
      "epoch": 0.23370429252782193,
      "grad_norm": 0.4764983654022217,
      "learning_rate": 0.00015480769230769232,
      "loss": 0.1652,
      "step": 147
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.04360749572515488,
      "learning_rate": 0.00015448717948717951,
      "loss": 0.0064,
      "step": 148
    },
    {
      "epoch": 0.2368839427662957,
      "grad_norm": 0.23304075002670288,
      "learning_rate": 0.00015416666666666668,
      "loss": 0.0371,
      "step": 149
    },
    {
      "epoch": 0.2384737678855326,
      "grad_norm": 0.5001382231712341,
      "learning_rate": 0.00015384615384615385,
      "loss": 0.1187,
      "step": 150
    },
    {
      "epoch": 0.24006359300476948,
      "grad_norm": 0.25231361389160156,
      "learning_rate": 0.00015352564102564105,
      "loss": 0.0768,
      "step": 151
    },
    {
      "epoch": 0.24165341812400637,
      "grad_norm": 0.09585005044937134,
      "learning_rate": 0.00015320512820512822,
      "loss": 0.006,
      "step": 152
    },
    {
      "epoch": 0.24324324324324326,
      "grad_norm": 0.2419535517692566,
      "learning_rate": 0.00015288461538461539,
      "loss": 0.0977,
      "step": 153
    },
    {
      "epoch": 0.24483306836248012,
      "grad_norm": 0.146329864859581,
      "learning_rate": 0.00015256410256410255,
      "loss": 0.0536,
      "step": 154
    },
    {
      "epoch": 0.246422893481717,
      "grad_norm": 0.18240629136562347,
      "learning_rate": 0.00015224358974358975,
      "loss": 0.017,
      "step": 155
    },
    {
      "epoch": 0.2480127186009539,
      "grad_norm": 0.05926453694701195,
      "learning_rate": 0.00015192307692307692,
      "loss": 0.0064,
      "step": 156
    },
    {
      "epoch": 0.24960254372019078,
      "grad_norm": 0.3880820870399475,
      "learning_rate": 0.00015160256410256412,
      "loss": 0.1601,
      "step": 157
    },
    {
      "epoch": 0.25119236883942764,
      "grad_norm": 0.2482868731021881,
      "learning_rate": 0.00015128205128205128,
      "loss": 0.0411,
      "step": 158
    },
    {
      "epoch": 0.2527821939586645,
      "grad_norm": 0.06078463792800903,
      "learning_rate": 0.00015096153846153845,
      "loss": 0.0064,
      "step": 159
    },
    {
      "epoch": 0.2543720190779014,
      "grad_norm": 0.17962682247161865,
      "learning_rate": 0.00015064102564102565,
      "loss": 0.0265,
      "step": 160
    },
    {
      "epoch": 0.2559618441971383,
      "grad_norm": 0.7806020975112915,
      "learning_rate": 0.00015032051282051282,
      "loss": 0.3072,
      "step": 161
    },
    {
      "epoch": 0.2575516693163752,
      "grad_norm": 0.15985456109046936,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.0757,
      "step": 162
    },
    {
      "epoch": 0.2591414944356121,
      "grad_norm": 0.16290226578712463,
      "learning_rate": 0.00014967948717948718,
      "loss": 0.0507,
      "step": 163
    },
    {
      "epoch": 0.26073131955484896,
      "grad_norm": 0.10111305117607117,
      "learning_rate": 0.00014935897435897438,
      "loss": 0.0114,
      "step": 164
    },
    {
      "epoch": 0.26232114467408585,
      "grad_norm": 0.05879606679081917,
      "learning_rate": 0.00014903846153846155,
      "loss": 0.01,
      "step": 165
    },
    {
      "epoch": 0.26391096979332274,
      "grad_norm": 0.5051523447036743,
      "learning_rate": 0.00014871794871794872,
      "loss": 0.1601,
      "step": 166
    },
    {
      "epoch": 0.2655007949125596,
      "grad_norm": 0.03471995145082474,
      "learning_rate": 0.0001483974358974359,
      "loss": 0.0055,
      "step": 167
    },
    {
      "epoch": 0.2670906200317965,
      "grad_norm": 0.3056786060333252,
      "learning_rate": 0.00014807692307692308,
      "loss": 0.1231,
      "step": 168
    },
    {
      "epoch": 0.2686804451510334,
      "grad_norm": 0.42161616683006287,
      "learning_rate": 0.00014775641025641028,
      "loss": 0.25,
      "step": 169
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 0.06922491639852524,
      "learning_rate": 0.00014743589743589745,
      "loss": 0.011,
      "step": 170
    },
    {
      "epoch": 0.2718600953895072,
      "grad_norm": 0.20071479678153992,
      "learning_rate": 0.00014711538461538464,
      "loss": 0.0105,
      "step": 171
    },
    {
      "epoch": 0.27344992050874406,
      "grad_norm": 0.20506203174591064,
      "learning_rate": 0.00014679487179487178,
      "loss": 0.0315,
      "step": 172
    },
    {
      "epoch": 0.27503974562798095,
      "grad_norm": 0.2805284559726715,
      "learning_rate": 0.00014647435897435898,
      "loss": 0.0842,
      "step": 173
    },
    {
      "epoch": 0.2766295707472178,
      "grad_norm": 0.3971136212348938,
      "learning_rate": 0.00014615384615384615,
      "loss": 0.0619,
      "step": 174
    },
    {
      "epoch": 0.27821939586645467,
      "grad_norm": 0.15673407912254333,
      "learning_rate": 0.00014583333333333335,
      "loss": 0.0241,
      "step": 175
    },
    {
      "epoch": 0.27980922098569155,
      "grad_norm": 0.3629809617996216,
      "learning_rate": 0.00014551282051282051,
      "loss": 0.1297,
      "step": 176
    },
    {
      "epoch": 0.28139904610492844,
      "grad_norm": 0.4430636167526245,
      "learning_rate": 0.00014519230769230768,
      "loss": 0.2182,
      "step": 177
    },
    {
      "epoch": 0.28298887122416533,
      "grad_norm": 0.2839743196964264,
      "learning_rate": 0.00014487179487179488,
      "loss": 0.1216,
      "step": 178
    },
    {
      "epoch": 0.2845786963434022,
      "grad_norm": 0.12506307661533356,
      "learning_rate": 0.00014455128205128205,
      "loss": 0.0172,
      "step": 179
    },
    {
      "epoch": 0.2861685214626391,
      "grad_norm": 0.08133542537689209,
      "learning_rate": 0.00014423076923076924,
      "loss": 0.0081,
      "step": 180
    },
    {
      "epoch": 0.287758346581876,
      "grad_norm": 0.28072476387023926,
      "learning_rate": 0.0001439102564102564,
      "loss": 0.0334,
      "step": 181
    },
    {
      "epoch": 0.2893481717011129,
      "grad_norm": 0.34719398617744446,
      "learning_rate": 0.0001435897435897436,
      "loss": 0.1212,
      "step": 182
    },
    {
      "epoch": 0.29093799682034976,
      "grad_norm": 0.6279427409172058,
      "learning_rate": 0.00014326923076923078,
      "loss": 0.1061,
      "step": 183
    },
    {
      "epoch": 0.29252782193958665,
      "grad_norm": 0.4898602366447449,
      "learning_rate": 0.00014294871794871795,
      "loss": 0.1108,
      "step": 184
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.2711282968521118,
      "learning_rate": 0.00014262820512820514,
      "loss": 0.0397,
      "step": 185
    },
    {
      "epoch": 0.2957074721780604,
      "grad_norm": 0.09525412321090698,
      "learning_rate": 0.0001423076923076923,
      "loss": 0.0102,
      "step": 186
    },
    {
      "epoch": 0.2972972972972973,
      "grad_norm": 0.5395241975784302,
      "learning_rate": 0.0001419871794871795,
      "loss": 0.1352,
      "step": 187
    },
    {
      "epoch": 0.2988871224165342,
      "grad_norm": 0.4148273468017578,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.0879,
      "step": 188
    },
    {
      "epoch": 0.3004769475357711,
      "grad_norm": 0.257284551858902,
      "learning_rate": 0.00014134615384615387,
      "loss": 0.0794,
      "step": 189
    },
    {
      "epoch": 0.302066772655008,
      "grad_norm": 19.08846092224121,
      "learning_rate": 0.00014102564102564104,
      "loss": 0.0659,
      "step": 190
    },
    {
      "epoch": 0.3036565977742448,
      "grad_norm": 0.7303067445755005,
      "learning_rate": 0.0001407051282051282,
      "loss": 0.1265,
      "step": 191
    },
    {
      "epoch": 0.3052464228934817,
      "grad_norm": 0.12368550896644592,
      "learning_rate": 0.00014038461538461538,
      "loss": 0.0198,
      "step": 192
    },
    {
      "epoch": 0.3068362480127186,
      "grad_norm": 0.3169589638710022,
      "learning_rate": 0.00014006410256410255,
      "loss": 0.1496,
      "step": 193
    },
    {
      "epoch": 0.30842607313195547,
      "grad_norm": 0.21957504749298096,
      "learning_rate": 0.00013974358974358974,
      "loss": 0.0658,
      "step": 194
    },
    {
      "epoch": 0.31001589825119236,
      "grad_norm": 0.3794187903404236,
      "learning_rate": 0.0001394230769230769,
      "loss": 0.1782,
      "step": 195
    },
    {
      "epoch": 0.31160572337042924,
      "grad_norm": 0.08718869835138321,
      "learning_rate": 0.0001391025641025641,
      "loss": 0.014,
      "step": 196
    },
    {
      "epoch": 0.31319554848966613,
      "grad_norm": 0.6954455375671387,
      "learning_rate": 0.00013878205128205128,
      "loss": 0.0692,
      "step": 197
    },
    {
      "epoch": 0.314785373608903,
      "grad_norm": 0.6140972375869751,
      "learning_rate": 0.00013846153846153847,
      "loss": 0.12,
      "step": 198
    },
    {
      "epoch": 0.3163751987281399,
      "grad_norm": 0.4191308617591858,
      "learning_rate": 0.00013814102564102564,
      "loss": 0.0517,
      "step": 199
    },
    {
      "epoch": 0.3179650238473768,
      "grad_norm": 0.3832559585571289,
      "learning_rate": 0.00013782051282051284,
      "loss": 0.1843,
      "step": 200
    },
    {
      "epoch": 0.3195548489666137,
      "grad_norm": 0.11778168380260468,
      "learning_rate": 0.0001375,
      "loss": 0.0302,
      "step": 201
    },
    {
      "epoch": 0.32114467408585057,
      "grad_norm": 0.21141177415847778,
      "learning_rate": 0.00013717948717948718,
      "loss": 0.0229,
      "step": 202
    },
    {
      "epoch": 0.32273449920508746,
      "grad_norm": 0.10329002141952515,
      "learning_rate": 0.00013685897435897437,
      "loss": 0.011,
      "step": 203
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 0.4054304361343384,
      "learning_rate": 0.00013653846153846154,
      "loss": 0.0332,
      "step": 204
    },
    {
      "epoch": 0.32591414944356123,
      "grad_norm": 0.3176099956035614,
      "learning_rate": 0.00013621794871794874,
      "loss": 0.0609,
      "step": 205
    },
    {
      "epoch": 0.3275039745627981,
      "grad_norm": 0.17443755269050598,
      "learning_rate": 0.0001358974358974359,
      "loss": 0.0042,
      "step": 206
    },
    {
      "epoch": 0.32909379968203495,
      "grad_norm": 0.7067334651947021,
      "learning_rate": 0.0001355769230769231,
      "loss": 0.183,
      "step": 207
    },
    {
      "epoch": 0.33068362480127184,
      "grad_norm": 0.6094589233398438,
      "learning_rate": 0.00013525641025641027,
      "loss": 0.0398,
      "step": 208
    },
    {
      "epoch": 0.3322734499205087,
      "grad_norm": 0.6415721774101257,
      "learning_rate": 0.00013493589743589744,
      "loss": 0.1555,
      "step": 209
    },
    {
      "epoch": 0.3338632750397456,
      "grad_norm": 0.6520565748214722,
      "learning_rate": 0.00013461538461538464,
      "loss": 0.0243,
      "step": 210
    },
    {
      "epoch": 0.3354531001589825,
      "grad_norm": 0.8035753965377808,
      "learning_rate": 0.00013429487179487178,
      "loss": 0.1317,
      "step": 211
    },
    {
      "epoch": 0.3370429252782194,
      "grad_norm": 0.7319831252098083,
      "learning_rate": 0.00013397435897435897,
      "loss": 0.1178,
      "step": 212
    },
    {
      "epoch": 0.3386327503974563,
      "grad_norm": 0.8388110399246216,
      "learning_rate": 0.00013365384615384614,
      "loss": 0.1858,
      "step": 213
    },
    {
      "epoch": 0.34022257551669316,
      "grad_norm": 0.1044360101222992,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.0093,
      "step": 214
    },
    {
      "epoch": 0.34181240063593005,
      "grad_norm": 0.27868157625198364,
      "learning_rate": 0.0001330128205128205,
      "loss": 0.016,
      "step": 215
    },
    {
      "epoch": 0.34340222575516693,
      "grad_norm": 0.3134378492832184,
      "learning_rate": 0.0001326923076923077,
      "loss": 0.0263,
      "step": 216
    },
    {
      "epoch": 0.3449920508744038,
      "grad_norm": 0.068832628428936,
      "learning_rate": 0.00013237179487179487,
      "loss": 0.0065,
      "step": 217
    },
    {
      "epoch": 0.3465818759936407,
      "grad_norm": 0.5243362188339233,
      "learning_rate": 0.00013205128205128204,
      "loss": 0.1942,
      "step": 218
    },
    {
      "epoch": 0.3481717011128776,
      "grad_norm": 0.5906464457511902,
      "learning_rate": 0.00013173076923076924,
      "loss": 0.1522,
      "step": 219
    },
    {
      "epoch": 0.3497615262321145,
      "grad_norm": 0.16527941823005676,
      "learning_rate": 0.0001314102564102564,
      "loss": 0.0137,
      "step": 220
    },
    {
      "epoch": 0.35135135135135137,
      "grad_norm": 0.2257087379693985,
      "learning_rate": 0.0001310897435897436,
      "loss": 0.0182,
      "step": 221
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.33586767315864563,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.044,
      "step": 222
    },
    {
      "epoch": 0.35453100158982515,
      "grad_norm": 0.591204822063446,
      "learning_rate": 0.00013044871794871797,
      "loss": 0.2423,
      "step": 223
    },
    {
      "epoch": 0.356120826709062,
      "grad_norm": 0.409347265958786,
      "learning_rate": 0.00013012820512820514,
      "loss": 0.1148,
      "step": 224
    },
    {
      "epoch": 0.35771065182829886,
      "grad_norm": 0.3585527241230011,
      "learning_rate": 0.0001298076923076923,
      "loss": 0.0458,
      "step": 225
    },
    {
      "epoch": 0.35930047694753575,
      "grad_norm": 4.985270977020264,
      "learning_rate": 0.0001294871794871795,
      "loss": 0.0084,
      "step": 226
    },
    {
      "epoch": 0.36089030206677264,
      "grad_norm": 0.4792949855327606,
      "learning_rate": 0.00012916666666666667,
      "loss": 0.0918,
      "step": 227
    },
    {
      "epoch": 0.3624801271860095,
      "grad_norm": 0.1643398404121399,
      "learning_rate": 0.00012884615384615387,
      "loss": 0.0567,
      "step": 228
    },
    {
      "epoch": 0.3640699523052464,
      "grad_norm": 0.2612013816833496,
      "learning_rate": 0.00012852564102564104,
      "loss": 0.0377,
      "step": 229
    },
    {
      "epoch": 0.3656597774244833,
      "grad_norm": 0.46685922145843506,
      "learning_rate": 0.00012820512820512823,
      "loss": 0.0838,
      "step": 230
    },
    {
      "epoch": 0.3672496025437202,
      "grad_norm": 0.9093119502067566,
      "learning_rate": 0.00012788461538461537,
      "loss": 0.3713,
      "step": 231
    },
    {
      "epoch": 0.3688394276629571,
      "grad_norm": 0.37559229135513306,
      "learning_rate": 0.00012756410256410257,
      "loss": 0.1446,
      "step": 232
    },
    {
      "epoch": 0.37042925278219396,
      "grad_norm": 0.5033674836158752,
      "learning_rate": 0.00012724358974358974,
      "loss": 0.0453,
      "step": 233
    },
    {
      "epoch": 0.37201907790143085,
      "grad_norm": 0.7343152165412903,
      "learning_rate": 0.00012692307692307693,
      "loss": 0.2654,
      "step": 234
    },
    {
      "epoch": 0.37360890302066774,
      "grad_norm": 0.24636968970298767,
      "learning_rate": 0.0001266025641025641,
      "loss": 0.0458,
      "step": 235
    },
    {
      "epoch": 0.3751987281399046,
      "grad_norm": 0.24548473954200745,
      "learning_rate": 0.00012628205128205127,
      "loss": 0.0372,
      "step": 236
    },
    {
      "epoch": 0.3767885532591415,
      "grad_norm": 0.20967845618724823,
      "learning_rate": 0.00012596153846153847,
      "loss": 0.0219,
      "step": 237
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 0.2484554499387741,
      "learning_rate": 0.00012564102564102564,
      "loss": 0.101,
      "step": 238
    },
    {
      "epoch": 0.3799682034976153,
      "grad_norm": 0.12015972286462784,
      "learning_rate": 0.00012532051282051283,
      "loss": 0.0102,
      "step": 239
    },
    {
      "epoch": 0.3815580286168522,
      "grad_norm": 0.44661808013916016,
      "learning_rate": 0.000125,
      "loss": 0.1073,
      "step": 240
    },
    {
      "epoch": 0.383147853736089,
      "grad_norm": 0.1510990560054779,
      "learning_rate": 0.0001246794871794872,
      "loss": 0.017,
      "step": 241
    },
    {
      "epoch": 0.3847376788553259,
      "grad_norm": 0.0454753041267395,
      "learning_rate": 0.00012435897435897437,
      "loss": 0.0079,
      "step": 242
    },
    {
      "epoch": 0.3863275039745628,
      "grad_norm": 0.4062851667404175,
      "learning_rate": 0.00012403846153846154,
      "loss": 0.14,
      "step": 243
    },
    {
      "epoch": 0.38791732909379967,
      "grad_norm": 0.3378550708293915,
      "learning_rate": 0.00012371794871794873,
      "loss": 0.1767,
      "step": 244
    },
    {
      "epoch": 0.38950715421303655,
      "grad_norm": 0.07868903130292892,
      "learning_rate": 0.0001233974358974359,
      "loss": 0.0083,
      "step": 245
    },
    {
      "epoch": 0.39109697933227344,
      "grad_norm": 0.22338472306728363,
      "learning_rate": 0.0001230769230769231,
      "loss": 0.0233,
      "step": 246
    },
    {
      "epoch": 0.39268680445151033,
      "grad_norm": 0.0792534202337265,
      "learning_rate": 0.00012275641025641027,
      "loss": 0.0099,
      "step": 247
    },
    {
      "epoch": 0.3942766295707472,
      "grad_norm": 0.13687282800674438,
      "learning_rate": 0.00012243589743589746,
      "loss": 0.0369,
      "step": 248
    },
    {
      "epoch": 0.3958664546899841,
      "grad_norm": 0.02982896938920021,
      "learning_rate": 0.00012211538461538463,
      "loss": 0.0048,
      "step": 249
    },
    {
      "epoch": 0.397456279809221,
      "grad_norm": 0.16309936344623566,
      "learning_rate": 0.00012179487179487179,
      "loss": 0.0166,
      "step": 250
    },
    {
      "epoch": 0.3990461049284579,
      "grad_norm": 0.10924453288316727,
      "learning_rate": 0.00012147435897435897,
      "loss": 0.0124,
      "step": 251
    },
    {
      "epoch": 0.40063593004769477,
      "grad_norm": 0.1163051649928093,
      "learning_rate": 0.00012115384615384615,
      "loss": 0.0179,
      "step": 252
    },
    {
      "epoch": 0.40222575516693165,
      "grad_norm": 0.052949920296669006,
      "learning_rate": 0.00012083333333333333,
      "loss": 0.0065,
      "step": 253
    },
    {
      "epoch": 0.40381558028616854,
      "grad_norm": 0.017243890091776848,
      "learning_rate": 0.00012051282051282052,
      "loss": 0.002,
      "step": 254
    },
    {
      "epoch": 0.40540540540540543,
      "grad_norm": 0.06571174412965775,
      "learning_rate": 0.0001201923076923077,
      "loss": 0.0072,
      "step": 255
    },
    {
      "epoch": 0.4069952305246423,
      "grad_norm": 1.240970492362976,
      "learning_rate": 0.00011987179487179487,
      "loss": 0.1064,
      "step": 256
    },
    {
      "epoch": 0.40858505564387915,
      "grad_norm": 0.007118861656636,
      "learning_rate": 0.00011955128205128205,
      "loss": 0.0012,
      "step": 257
    },
    {
      "epoch": 0.41017488076311603,
      "grad_norm": 0.520529568195343,
      "learning_rate": 0.00011923076923076923,
      "loss": 0.1964,
      "step": 258
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.5938122868537903,
      "learning_rate": 0.00011891025641025642,
      "loss": 0.2524,
      "step": 259
    },
    {
      "epoch": 0.4133545310015898,
      "grad_norm": 0.6393735408782959,
      "learning_rate": 0.0001185897435897436,
      "loss": 0.2211,
      "step": 260
    },
    {
      "epoch": 0.4149443561208267,
      "grad_norm": 0.01591532863676548,
      "learning_rate": 0.00011826923076923078,
      "loss": 0.0023,
      "step": 261
    },
    {
      "epoch": 0.4165341812400636,
      "grad_norm": 110.45150756835938,
      "learning_rate": 0.00011794871794871796,
      "loss": -0.0506,
      "step": 262
    },
    {
      "epoch": 0.41812400635930047,
      "grad_norm": 0.025396550074219704,
      "learning_rate": 0.00011762820512820513,
      "loss": 0.0048,
      "step": 263
    },
    {
      "epoch": 0.41971383147853736,
      "grad_norm": 0.043962396681308746,
      "learning_rate": 0.00011730769230769231,
      "loss": 0.0075,
      "step": 264
    },
    {
      "epoch": 0.42130365659777425,
      "grad_norm": 0.4529667794704437,
      "learning_rate": 0.0001169871794871795,
      "loss": 0.1052,
      "step": 265
    },
    {
      "epoch": 0.42289348171701113,
      "grad_norm": 0.10411544144153595,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.0137,
      "step": 266
    },
    {
      "epoch": 0.424483306836248,
      "grad_norm": 0.19697844982147217,
      "learning_rate": 0.00011634615384615386,
      "loss": 0.0388,
      "step": 267
    },
    {
      "epoch": 0.4260731319554849,
      "grad_norm": 0.6809730529785156,
      "learning_rate": 0.00011602564102564104,
      "loss": 0.1925,
      "step": 268
    },
    {
      "epoch": 0.4276629570747218,
      "grad_norm": 0.20524252951145172,
      "learning_rate": 0.00011570512820512823,
      "loss": 0.0333,
      "step": 269
    },
    {
      "epoch": 0.4292527821939587,
      "grad_norm": 0.46487146615982056,
      "learning_rate": 0.00011538461538461538,
      "loss": 0.2307,
      "step": 270
    },
    {
      "epoch": 0.43084260731319557,
      "grad_norm": 0.25698384642601013,
      "learning_rate": 0.00011506410256410256,
      "loss": 0.0461,
      "step": 271
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 0.3483901917934418,
      "learning_rate": 0.00011474358974358975,
      "loss": 0.1837,
      "step": 272
    },
    {
      "epoch": 0.43402225755166934,
      "grad_norm": 0.06720589101314545,
      "learning_rate": 0.00011442307692307692,
      "loss": 0.0126,
      "step": 273
    },
    {
      "epoch": 0.4356120826709062,
      "grad_norm": 0.1251189261674881,
      "learning_rate": 0.0001141025641025641,
      "loss": 0.0189,
      "step": 274
    },
    {
      "epoch": 0.43720190779014306,
      "grad_norm": 0.15908971428871155,
      "learning_rate": 0.00011378205128205128,
      "loss": 0.0276,
      "step": 275
    },
    {
      "epoch": 0.43879173290937995,
      "grad_norm": 0.45688340067863464,
      "learning_rate": 0.00011346153846153846,
      "loss": 0.1925,
      "step": 276
    },
    {
      "epoch": 0.44038155802861684,
      "grad_norm": 0.3661150634288788,
      "learning_rate": 0.00011314102564102565,
      "loss": 0.0495,
      "step": 277
    },
    {
      "epoch": 0.4419713831478537,
      "grad_norm": 2120.263916015625,
      "learning_rate": 0.00011282051282051283,
      "loss": -0.2419,
      "step": 278
    },
    {
      "epoch": 0.4435612082670906,
      "grad_norm": 0.029478272423148155,
      "learning_rate": 0.00011250000000000001,
      "loss": 0.0056,
      "step": 279
    },
    {
      "epoch": 0.4451510333863275,
      "grad_norm": 0.17034417390823364,
      "learning_rate": 0.00011217948717948718,
      "loss": 0.0563,
      "step": 280
    },
    {
      "epoch": 0.4467408585055644,
      "grad_norm": 0.22419473528862,
      "learning_rate": 0.00011185897435897436,
      "loss": 0.0417,
      "step": 281
    },
    {
      "epoch": 0.4483306836248013,
      "grad_norm": 0.22315055131912231,
      "learning_rate": 0.00011153846153846154,
      "loss": 0.0594,
      "step": 282
    },
    {
      "epoch": 0.44992050874403816,
      "grad_norm": 0.4232434332370758,
      "learning_rate": 0.00011121794871794873,
      "loss": 0.0803,
      "step": 283
    },
    {
      "epoch": 0.45151033386327505,
      "grad_norm": 0.18342652916908264,
      "learning_rate": 0.00011089743589743591,
      "loss": 0.0357,
      "step": 284
    },
    {
      "epoch": 0.45310015898251194,
      "grad_norm": 0.21490100026130676,
      "learning_rate": 0.00011057692307692309,
      "loss": 0.0453,
      "step": 285
    },
    {
      "epoch": 0.4546899841017488,
      "grad_norm": 0.4675140976905823,
      "learning_rate": 0.00011025641025641027,
      "loss": 0.0512,
      "step": 286
    },
    {
      "epoch": 0.4562798092209857,
      "grad_norm": 0.06284596025943756,
      "learning_rate": 0.00010993589743589746,
      "loss": 0.0115,
      "step": 287
    },
    {
      "epoch": 0.4578696343402226,
      "grad_norm": 0.1315043419599533,
      "learning_rate": 0.00010961538461538463,
      "loss": 0.0179,
      "step": 288
    },
    {
      "epoch": 0.4594594594594595,
      "grad_norm": 0.2960908114910126,
      "learning_rate": 0.0001092948717948718,
      "loss": 0.1164,
      "step": 289
    },
    {
      "epoch": 0.4610492845786963,
      "grad_norm": 0.786170244216919,
      "learning_rate": 0.00010897435897435896,
      "loss": 0.0349,
      "step": 290
    },
    {
      "epoch": 0.4626391096979332,
      "grad_norm": 0.13254639506340027,
      "learning_rate": 0.00010865384615384615,
      "loss": 0.0405,
      "step": 291
    },
    {
      "epoch": 0.4642289348171701,
      "grad_norm": 0.05349672585725784,
      "learning_rate": 0.00010833333333333333,
      "loss": 0.0095,
      "step": 292
    },
    {
      "epoch": 0.465818759936407,
      "grad_norm": 0.4262657165527344,
      "learning_rate": 0.00010801282051282051,
      "loss": 0.076,
      "step": 293
    },
    {
      "epoch": 0.46740858505564387,
      "grad_norm": 0.14729416370391846,
      "learning_rate": 0.0001076923076923077,
      "loss": 0.0097,
      "step": 294
    },
    {
      "epoch": 0.46899841017488075,
      "grad_norm": 0.023249955847859383,
      "learning_rate": 0.00010737179487179488,
      "loss": 0.0053,
      "step": 295
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.22135311365127563,
      "learning_rate": 0.00010705128205128206,
      "loss": 0.0483,
      "step": 296
    },
    {
      "epoch": 0.47217806041335453,
      "grad_norm": 0.1529567688703537,
      "learning_rate": 0.00010673076923076924,
      "loss": 0.043,
      "step": 297
    },
    {
      "epoch": 0.4737678855325914,
      "grad_norm": 0.25659769773483276,
      "learning_rate": 0.00010641025641025641,
      "loss": 0.0563,
      "step": 298
    },
    {
      "epoch": 0.4753577106518283,
      "grad_norm": 0.26171502470970154,
      "learning_rate": 0.00010608974358974359,
      "loss": 0.0279,
      "step": 299
    },
    {
      "epoch": 0.4769475357710652,
      "grad_norm": 0.21543999016284943,
      "learning_rate": 0.00010576923076923077,
      "loss": 0.0546,
      "step": 300
    },
    {
      "epoch": 0.4785373608903021,
      "grad_norm": 0.02175227180123329,
      "learning_rate": 0.00010544871794871796,
      "loss": 0.0039,
      "step": 301
    },
    {
      "epoch": 0.48012718600953896,
      "grad_norm": 0.15471650660037994,
      "learning_rate": 0.00010512820512820514,
      "loss": 0.0771,
      "step": 302
    },
    {
      "epoch": 0.48171701112877585,
      "grad_norm": 0.16482192277908325,
      "learning_rate": 0.00010480769230769232,
      "loss": 0.0117,
      "step": 303
    },
    {
      "epoch": 0.48330683624801274,
      "grad_norm": 0.31149548292160034,
      "learning_rate": 0.0001044871794871795,
      "loss": 0.1199,
      "step": 304
    },
    {
      "epoch": 0.4848966613672496,
      "grad_norm": 0.5017039179801941,
      "learning_rate": 0.00010416666666666667,
      "loss": 0.1597,
      "step": 305
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 0.02880878373980522,
      "learning_rate": 0.00010384615384615386,
      "loss": 0.0042,
      "step": 306
    },
    {
      "epoch": 0.48807631160572335,
      "grad_norm": 0.17396098375320435,
      "learning_rate": 0.00010352564102564104,
      "loss": 0.0567,
      "step": 307
    },
    {
      "epoch": 0.48966613672496023,
      "grad_norm": 0.3302692770957947,
      "learning_rate": 0.00010320512820512822,
      "loss": 0.1082,
      "step": 308
    },
    {
      "epoch": 0.4912559618441971,
      "grad_norm": 0.5635743737220764,
      "learning_rate": 0.00010288461538461538,
      "loss": 0.1866,
      "step": 309
    },
    {
      "epoch": 0.492845786963434,
      "grad_norm": 0.8801028728485107,
      "learning_rate": 0.00010256410256410256,
      "loss": 0.2049,
      "step": 310
    },
    {
      "epoch": 0.4944356120826709,
      "grad_norm": 0.4140753746032715,
      "learning_rate": 0.00010224358974358974,
      "loss": 0.1696,
      "step": 311
    },
    {
      "epoch": 0.4960254372019078,
      "grad_norm": 0.5245272517204285,
      "learning_rate": 0.00010192307692307692,
      "loss": 0.1282,
      "step": 312
    },
    {
      "epoch": 0.49761526232114467,
      "grad_norm": 0.042833779007196426,
      "learning_rate": 0.0001016025641025641,
      "loss": 0.0077,
      "step": 313
    },
    {
      "epoch": 0.49920508744038156,
      "grad_norm": 0.15593135356903076,
      "learning_rate": 0.00010128205128205129,
      "loss": 0.0215,
      "step": 314
    },
    {
      "epoch": 0.5007949125596184,
      "grad_norm": 0.4478940963745117,
      "learning_rate": 0.00010096153846153846,
      "loss": 0.0815,
      "step": 315
    },
    {
      "epoch": 0.5023847376788553,
      "grad_norm": 0.1999501883983612,
      "learning_rate": 0.00010064102564102564,
      "loss": 0.1064,
      "step": 316
    },
    {
      "epoch": 0.5039745627980922,
      "grad_norm": 0.4019070565700531,
      "learning_rate": 0.00010032051282051282,
      "loss": 0.1601,
      "step": 317
    },
    {
      "epoch": 0.505564387917329,
      "grad_norm": 0.1148209348320961,
      "learning_rate": 0.0001,
      "loss": 0.0147,
      "step": 318
    },
    {
      "epoch": 0.5071542130365659,
      "grad_norm": 0.13452281057834625,
      "learning_rate": 9.967948717948719e-05,
      "loss": 0.0174,
      "step": 319
    },
    {
      "epoch": 0.5087440381558028,
      "grad_norm": 0.03699894994497299,
      "learning_rate": 9.935897435897437e-05,
      "loss": 0.0064,
      "step": 320
    },
    {
      "epoch": 0.5103338632750397,
      "grad_norm": 0.2274172455072403,
      "learning_rate": 9.903846153846155e-05,
      "loss": 0.0796,
      "step": 321
    },
    {
      "epoch": 0.5119236883942766,
      "grad_norm": 0.0702248066663742,
      "learning_rate": 9.871794871794872e-05,
      "loss": 0.0087,
      "step": 322
    },
    {
      "epoch": 0.5135135135135135,
      "grad_norm": 0.2039029747247696,
      "learning_rate": 9.839743589743589e-05,
      "loss": 0.0457,
      "step": 323
    },
    {
      "epoch": 0.5151033386327504,
      "grad_norm": 0.0641154795885086,
      "learning_rate": 9.807692307692307e-05,
      "loss": 0.0065,
      "step": 324
    },
    {
      "epoch": 0.5166931637519873,
      "grad_norm": 0.19378620386123657,
      "learning_rate": 9.775641025641025e-05,
      "loss": 0.0667,
      "step": 325
    },
    {
      "epoch": 0.5182829888712241,
      "grad_norm": 0.16385897994041443,
      "learning_rate": 9.743589743589744e-05,
      "loss": 0.0183,
      "step": 326
    },
    {
      "epoch": 0.519872813990461,
      "grad_norm": 0.08641927689313889,
      "learning_rate": 9.711538461538462e-05,
      "loss": 0.0148,
      "step": 327
    },
    {
      "epoch": 0.5214626391096979,
      "grad_norm": 0.11345919966697693,
      "learning_rate": 9.67948717948718e-05,
      "loss": 0.0061,
      "step": 328
    },
    {
      "epoch": 0.5230524642289348,
      "grad_norm": 0.499679297208786,
      "learning_rate": 9.647435897435898e-05,
      "loss": 0.1852,
      "step": 329
    },
    {
      "epoch": 0.5246422893481717,
      "grad_norm": 0.5937680602073669,
      "learning_rate": 9.615384615384617e-05,
      "loss": 0.0287,
      "step": 330
    },
    {
      "epoch": 0.5262321144674086,
      "grad_norm": 0.3960418105125427,
      "learning_rate": 9.583333333333334e-05,
      "loss": 0.1455,
      "step": 331
    },
    {
      "epoch": 0.5278219395866455,
      "grad_norm": 0.06566470861434937,
      "learning_rate": 9.551282051282052e-05,
      "loss": 0.0089,
      "step": 332
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.4582453668117523,
      "learning_rate": 9.519230769230769e-05,
      "loss": 0.1159,
      "step": 333
    },
    {
      "epoch": 0.5310015898251192,
      "grad_norm": 0.13444699347019196,
      "learning_rate": 9.487179487179487e-05,
      "loss": 0.0182,
      "step": 334
    },
    {
      "epoch": 0.5325914149443561,
      "grad_norm": 0.013176311738789082,
      "learning_rate": 9.455128205128205e-05,
      "loss": 0.0024,
      "step": 335
    },
    {
      "epoch": 0.534181240063593,
      "grad_norm": 3.3692314624786377,
      "learning_rate": 9.423076923076924e-05,
      "loss": 0.0392,
      "step": 336
    },
    {
      "epoch": 0.5357710651828299,
      "grad_norm": 0.17271320521831512,
      "learning_rate": 9.391025641025642e-05,
      "loss": 0.0626,
      "step": 337
    },
    {
      "epoch": 0.5373608903020668,
      "grad_norm": 0.03246154636144638,
      "learning_rate": 9.35897435897436e-05,
      "loss": 0.0044,
      "step": 338
    },
    {
      "epoch": 0.5389507154213037,
      "grad_norm": 0.20889797806739807,
      "learning_rate": 9.326923076923077e-05,
      "loss": 0.1134,
      "step": 339
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 0.12116879224777222,
      "learning_rate": 9.294871794871795e-05,
      "loss": 0.0378,
      "step": 340
    },
    {
      "epoch": 0.5421303656597775,
      "grad_norm": 0.3565886616706848,
      "learning_rate": 9.262820512820513e-05,
      "loss": 0.0318,
      "step": 341
    },
    {
      "epoch": 0.5437201907790143,
      "grad_norm": 0.15650281310081482,
      "learning_rate": 9.230769230769232e-05,
      "loss": 0.0126,
      "step": 342
    },
    {
      "epoch": 0.5453100158982512,
      "grad_norm": 0.6759397387504578,
      "learning_rate": 9.198717948717949e-05,
      "loss": 0.2216,
      "step": 343
    },
    {
      "epoch": 0.5468998410174881,
      "grad_norm": 0.5066829919815063,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.2636,
      "step": 344
    },
    {
      "epoch": 0.548489666136725,
      "grad_norm": 0.3995247483253479,
      "learning_rate": 9.134615384615385e-05,
      "loss": 0.1289,
      "step": 345
    },
    {
      "epoch": 0.5500794912559619,
      "grad_norm": 0.029242385178804398,
      "learning_rate": 9.102564102564103e-05,
      "loss": 0.0049,
      "step": 346
    },
    {
      "epoch": 0.5516693163751988,
      "grad_norm": 0.6637786030769348,
      "learning_rate": 9.070512820512822e-05,
      "loss": 0.313,
      "step": 347
    },
    {
      "epoch": 0.5532591414944356,
      "grad_norm": 0.41472917795181274,
      "learning_rate": 9.038461538461538e-05,
      "loss": 0.1568,
      "step": 348
    },
    {
      "epoch": 0.5548489666136724,
      "grad_norm": 0.10559442639350891,
      "learning_rate": 9.006410256410257e-05,
      "loss": 0.0137,
      "step": 349
    },
    {
      "epoch": 0.5564387917329093,
      "grad_norm": 0.3453231155872345,
      "learning_rate": 8.974358974358975e-05,
      "loss": 0.1376,
      "step": 350
    },
    {
      "epoch": 0.5580286168521462,
      "grad_norm": 0.24938951432704926,
      "learning_rate": 8.942307692307693e-05,
      "loss": 0.0718,
      "step": 351
    },
    {
      "epoch": 0.5596184419713831,
      "grad_norm": 0.2357766181230545,
      "learning_rate": 8.910256410256411e-05,
      "loss": 0.0785,
      "step": 352
    },
    {
      "epoch": 0.56120826709062,
      "grad_norm": 0.2535892426967621,
      "learning_rate": 8.878205128205128e-05,
      "loss": 0.0693,
      "step": 353
    },
    {
      "epoch": 0.5627980922098569,
      "grad_norm": 0.3421500623226166,
      "learning_rate": 8.846153846153847e-05,
      "loss": 0.0264,
      "step": 354
    },
    {
      "epoch": 0.5643879173290938,
      "grad_norm": 0.13815785944461823,
      "learning_rate": 8.814102564102565e-05,
      "loss": 0.0228,
      "step": 355
    },
    {
      "epoch": 0.5659777424483307,
      "grad_norm": 0.09962806105613708,
      "learning_rate": 8.782051282051283e-05,
      "loss": 0.0139,
      "step": 356
    },
    {
      "epoch": 0.5675675675675675,
      "grad_norm": 0.357034295797348,
      "learning_rate": 8.75e-05,
      "loss": 0.1949,
      "step": 357
    },
    {
      "epoch": 0.5691573926868044,
      "grad_norm": 0.5761215686798096,
      "learning_rate": 8.717948717948718e-05,
      "loss": 0.1077,
      "step": 358
    },
    {
      "epoch": 0.5707472178060413,
      "grad_norm": 0.3331221342086792,
      "learning_rate": 8.685897435897436e-05,
      "loss": 0.0609,
      "step": 359
    },
    {
      "epoch": 0.5723370429252782,
      "grad_norm": 0.230582132935524,
      "learning_rate": 8.653846153846155e-05,
      "loss": 0.0212,
      "step": 360
    },
    {
      "epoch": 0.5739268680445151,
      "grad_norm": 0.3680291473865509,
      "learning_rate": 8.621794871794873e-05,
      "loss": 0.1824,
      "step": 361
    },
    {
      "epoch": 0.575516693163752,
      "grad_norm": 0.16629599034786224,
      "learning_rate": 8.58974358974359e-05,
      "loss": 0.0179,
      "step": 362
    },
    {
      "epoch": 0.5771065182829889,
      "grad_norm": 0.23651844263076782,
      "learning_rate": 8.557692307692308e-05,
      "loss": 0.0305,
      "step": 363
    },
    {
      "epoch": 0.5786963434022258,
      "grad_norm": 1.1780712604522705,
      "learning_rate": 8.525641025641026e-05,
      "loss": 0.3001,
      "step": 364
    },
    {
      "epoch": 0.5802861685214626,
      "grad_norm": 0.29711925983428955,
      "learning_rate": 8.493589743589743e-05,
      "loss": 0.0237,
      "step": 365
    },
    {
      "epoch": 0.5818759936406995,
      "grad_norm": 0.1861262023448944,
      "learning_rate": 8.461538461538461e-05,
      "loss": 0.0236,
      "step": 366
    },
    {
      "epoch": 0.5834658187599364,
      "grad_norm": 0.09930045157670975,
      "learning_rate": 8.42948717948718e-05,
      "loss": 0.0076,
      "step": 367
    },
    {
      "epoch": 0.5850556438791733,
      "grad_norm": 0.28235840797424316,
      "learning_rate": 8.397435897435898e-05,
      "loss": 0.0579,
      "step": 368
    },
    {
      "epoch": 0.5866454689984102,
      "grad_norm": 0.05892818048596382,
      "learning_rate": 8.365384615384616e-05,
      "loss": 0.0109,
      "step": 369
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.2882421612739563,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0703,
      "step": 370
    },
    {
      "epoch": 0.589825119236884,
      "grad_norm": 0.22228339314460754,
      "learning_rate": 8.301282051282053e-05,
      "loss": 0.0137,
      "step": 371
    },
    {
      "epoch": 0.5914149443561209,
      "grad_norm": 0.5902008414268494,
      "learning_rate": 8.26923076923077e-05,
      "loss": 0.0832,
      "step": 372
    },
    {
      "epoch": 0.5930047694753577,
      "grad_norm": 0.8109852075576782,
      "learning_rate": 8.237179487179488e-05,
      "loss": 0.2169,
      "step": 373
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 0.6075869798660278,
      "learning_rate": 8.205128205128205e-05,
      "loss": 0.0842,
      "step": 374
    },
    {
      "epoch": 0.5961844197138315,
      "grad_norm": 0.3927268087863922,
      "learning_rate": 8.173076923076923e-05,
      "loss": 0.0509,
      "step": 375
    },
    {
      "epoch": 0.5977742448330684,
      "grad_norm": 0.45437368750572205,
      "learning_rate": 8.141025641025641e-05,
      "loss": 0.0725,
      "step": 376
    },
    {
      "epoch": 0.5993640699523053,
      "grad_norm": 0.18939341604709625,
      "learning_rate": 8.10897435897436e-05,
      "loss": 0.0112,
      "step": 377
    },
    {
      "epoch": 0.6009538950715422,
      "grad_norm": 4.790078163146973,
      "learning_rate": 8.076923076923078e-05,
      "loss": 0.112,
      "step": 378
    },
    {
      "epoch": 0.6025437201907791,
      "grad_norm": 0.1617463231086731,
      "learning_rate": 8.044871794871796e-05,
      "loss": 0.0124,
      "step": 379
    },
    {
      "epoch": 0.604133545310016,
      "grad_norm": 0.7815133929252625,
      "learning_rate": 8.012820512820514e-05,
      "loss": 0.082,
      "step": 380
    },
    {
      "epoch": 0.6057233704292527,
      "grad_norm": 0.16164742410182953,
      "learning_rate": 7.980769230769231e-05,
      "loss": 0.0113,
      "step": 381
    },
    {
      "epoch": 0.6073131955484896,
      "grad_norm": 0.3529549539089203,
      "learning_rate": 7.948717948717948e-05,
      "loss": 0.1472,
      "step": 382
    },
    {
      "epoch": 0.6089030206677265,
      "grad_norm": 0.9418883919715881,
      "learning_rate": 7.916666666666666e-05,
      "loss": 0.1834,
      "step": 383
    },
    {
      "epoch": 0.6104928457869634,
      "grad_norm": 0.09752427786588669,
      "learning_rate": 7.884615384615384e-05,
      "loss": 0.008,
      "step": 384
    },
    {
      "epoch": 0.6120826709062003,
      "grad_norm": 0.3833617866039276,
      "learning_rate": 7.852564102564103e-05,
      "loss": 0.0154,
      "step": 385
    },
    {
      "epoch": 0.6136724960254372,
      "grad_norm": 0.24252949655056,
      "learning_rate": 7.820512820512821e-05,
      "loss": 0.016,
      "step": 386
    },
    {
      "epoch": 0.615262321144674,
      "grad_norm": 0.37746289372444153,
      "learning_rate": 7.788461538461539e-05,
      "loss": 0.0691,
      "step": 387
    },
    {
      "epoch": 0.6168521462639109,
      "grad_norm": 0.022243179380893707,
      "learning_rate": 7.756410256410257e-05,
      "loss": 0.0031,
      "step": 388
    },
    {
      "epoch": 0.6184419713831478,
      "grad_norm": 0.06411202251911163,
      "learning_rate": 7.724358974358976e-05,
      "loss": 0.0048,
      "step": 389
    },
    {
      "epoch": 0.6200317965023847,
      "grad_norm": 1.5008955001831055,
      "learning_rate": 7.692307692307693e-05,
      "loss": 0.0901,
      "step": 390
    },
    {
      "epoch": 0.6216216216216216,
      "grad_norm": 0.23259542882442474,
      "learning_rate": 7.660256410256411e-05,
      "loss": 0.0172,
      "step": 391
    },
    {
      "epoch": 0.6232114467408585,
      "grad_norm": 0.9843189716339111,
      "learning_rate": 7.628205128205128e-05,
      "loss": 0.0259,
      "step": 392
    },
    {
      "epoch": 0.6248012718600954,
      "grad_norm": 0.054138053208589554,
      "learning_rate": 7.596153846153846e-05,
      "loss": 0.0064,
      "step": 393
    },
    {
      "epoch": 0.6263910969793323,
      "grad_norm": 0.08178772777318954,
      "learning_rate": 7.564102564102564e-05,
      "loss": 0.0073,
      "step": 394
    },
    {
      "epoch": 0.6279809220985691,
      "grad_norm": 0.7908502817153931,
      "learning_rate": 7.532051282051282e-05,
      "loss": 0.0542,
      "step": 395
    },
    {
      "epoch": 0.629570747217806,
      "grad_norm": 0.3369275629520416,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.0157,
      "step": 396
    },
    {
      "epoch": 0.6311605723370429,
      "grad_norm": 0.4112912714481354,
      "learning_rate": 7.467948717948719e-05,
      "loss": 0.0662,
      "step": 397
    },
    {
      "epoch": 0.6327503974562798,
      "grad_norm": 0.0723024234175682,
      "learning_rate": 7.435897435897436e-05,
      "loss": 0.0051,
      "step": 398
    },
    {
      "epoch": 0.6343402225755167,
      "grad_norm": 0.5073957443237305,
      "learning_rate": 7.403846153846154e-05,
      "loss": 0.077,
      "step": 399
    },
    {
      "epoch": 0.6359300476947536,
      "grad_norm": 2.1382336616516113,
      "learning_rate": 7.371794871794872e-05,
      "loss": 0.2853,
      "step": 400
    },
    {
      "epoch": 0.6375198728139905,
      "grad_norm": 0.11616919189691544,
      "learning_rate": 7.339743589743589e-05,
      "loss": 0.0024,
      "step": 401
    },
    {
      "epoch": 0.6391096979332274,
      "grad_norm": 0.7072839736938477,
      "learning_rate": 7.307692307692307e-05,
      "loss": 0.2142,
      "step": 402
    },
    {
      "epoch": 0.6406995230524642,
      "grad_norm": 0.18775232136249542,
      "learning_rate": 7.275641025641026e-05,
      "loss": 0.0082,
      "step": 403
    },
    {
      "epoch": 0.6422893481717011,
      "grad_norm": 0.151427760720253,
      "learning_rate": 7.243589743589744e-05,
      "loss": 0.0433,
      "step": 404
    },
    {
      "epoch": 0.643879173290938,
      "grad_norm": 1.2183023691177368,
      "learning_rate": 7.211538461538462e-05,
      "loss": 0.1303,
      "step": 405
    },
    {
      "epoch": 0.6454689984101749,
      "grad_norm": 0.015810150653123856,
      "learning_rate": 7.17948717948718e-05,
      "loss": 0.0026,
      "step": 406
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.923730194568634,
      "learning_rate": 7.147435897435897e-05,
      "loss": 0.2625,
      "step": 407
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 0.055990055203437805,
      "learning_rate": 7.115384615384616e-05,
      "loss": 0.0058,
      "step": 408
    },
    {
      "epoch": 0.6502384737678856,
      "grad_norm": 0.673793613910675,
      "learning_rate": 7.083333333333334e-05,
      "loss": 0.0501,
      "step": 409
    },
    {
      "epoch": 0.6518282988871225,
      "grad_norm": 0.3621536195278168,
      "learning_rate": 7.051282051282052e-05,
      "loss": 0.0876,
      "step": 410
    },
    {
      "epoch": 0.6534181240063593,
      "grad_norm": 0.28157100081443787,
      "learning_rate": 7.019230769230769e-05,
      "loss": 0.0242,
      "step": 411
    },
    {
      "epoch": 0.6550079491255962,
      "grad_norm": 0.6173439621925354,
      "learning_rate": 6.987179487179487e-05,
      "loss": 0.0162,
      "step": 412
    },
    {
      "epoch": 0.6565977742448331,
      "grad_norm": 0.2987532317638397,
      "learning_rate": 6.955128205128205e-05,
      "loss": 0.0255,
      "step": 413
    },
    {
      "epoch": 0.6581875993640699,
      "grad_norm": 0.05451059341430664,
      "learning_rate": 6.923076923076924e-05,
      "loss": 0.0083,
      "step": 414
    },
    {
      "epoch": 0.6597774244833068,
      "grad_norm": 0.580333411693573,
      "learning_rate": 6.891025641025642e-05,
      "loss": 0.2138,
      "step": 415
    },
    {
      "epoch": 0.6613672496025437,
      "grad_norm": 0.01635676622390747,
      "learning_rate": 6.858974358974359e-05,
      "loss": 0.003,
      "step": 416
    },
    {
      "epoch": 0.6629570747217806,
      "grad_norm": 0.4346982538700104,
      "learning_rate": 6.826923076923077e-05,
      "loss": 0.2154,
      "step": 417
    },
    {
      "epoch": 0.6645468998410174,
      "grad_norm": 0.22367137670516968,
      "learning_rate": 6.794871794871795e-05,
      "loss": 0.1519,
      "step": 418
    },
    {
      "epoch": 0.6661367249602543,
      "grad_norm": 0.22539077699184418,
      "learning_rate": 6.762820512820514e-05,
      "loss": 0.06,
      "step": 419
    },
    {
      "epoch": 0.6677265500794912,
      "grad_norm": 1.1176289319992065,
      "learning_rate": 6.730769230769232e-05,
      "loss": 0.1808,
      "step": 420
    },
    {
      "epoch": 0.6693163751987281,
      "grad_norm": 0.5003845691680908,
      "learning_rate": 6.698717948717949e-05,
      "loss": 0.0682,
      "step": 421
    },
    {
      "epoch": 0.670906200317965,
      "grad_norm": 0.6439017057418823,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.1493,
      "step": 422
    },
    {
      "epoch": 0.6724960254372019,
      "grad_norm": 0.271682471036911,
      "learning_rate": 6.634615384615385e-05,
      "loss": 0.0679,
      "step": 423
    },
    {
      "epoch": 0.6740858505564388,
      "grad_norm": 0.4665146470069885,
      "learning_rate": 6.602564102564102e-05,
      "loss": 0.1519,
      "step": 424
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 0.1121600940823555,
      "learning_rate": 6.57051282051282e-05,
      "loss": 0.006,
      "step": 425
    },
    {
      "epoch": 0.6772655007949125,
      "grad_norm": 0.055248577147722244,
      "learning_rate": 6.538461538461539e-05,
      "loss": 0.006,
      "step": 426
    },
    {
      "epoch": 0.6788553259141494,
      "grad_norm": 0.26095256209373474,
      "learning_rate": 6.506410256410257e-05,
      "loss": 0.0211,
      "step": 427
    },
    {
      "epoch": 0.6804451510333863,
      "grad_norm": 0.4062195122241974,
      "learning_rate": 6.474358974358975e-05,
      "loss": 0.1756,
      "step": 428
    },
    {
      "epoch": 0.6820349761526232,
      "grad_norm": 0.018197275698184967,
      "learning_rate": 6.442307692307693e-05,
      "loss": 0.0027,
      "step": 429
    },
    {
      "epoch": 0.6836248012718601,
      "grad_norm": 0.36265262961387634,
      "learning_rate": 6.410256410256412e-05,
      "loss": 0.0173,
      "step": 430
    },
    {
      "epoch": 0.685214626391097,
      "grad_norm": 0.1537858545780182,
      "learning_rate": 6.378205128205128e-05,
      "loss": 0.0112,
      "step": 431
    },
    {
      "epoch": 0.6868044515103339,
      "grad_norm": 0.019435888156294823,
      "learning_rate": 6.346153846153847e-05,
      "loss": 0.0038,
      "step": 432
    },
    {
      "epoch": 0.6883942766295708,
      "grad_norm": 0.5994793772697449,
      "learning_rate": 6.314102564102564e-05,
      "loss": 0.0126,
      "step": 433
    },
    {
      "epoch": 0.6899841017488076,
      "grad_norm": 0.05748901888728142,
      "learning_rate": 6.282051282051282e-05,
      "loss": 0.0035,
      "step": 434
    },
    {
      "epoch": 0.6915739268680445,
      "grad_norm": 0.5740198493003845,
      "learning_rate": 6.25e-05,
      "loss": 0.3025,
      "step": 435
    },
    {
      "epoch": 0.6931637519872814,
      "grad_norm": 0.39674118161201477,
      "learning_rate": 6.217948717948718e-05,
      "loss": 0.1015,
      "step": 436
    },
    {
      "epoch": 0.6947535771065183,
      "grad_norm": 0.057448145002126694,
      "learning_rate": 6.185897435897437e-05,
      "loss": 0.0048,
      "step": 437
    },
    {
      "epoch": 0.6963434022257552,
      "grad_norm": 0.476184219121933,
      "learning_rate": 6.153846153846155e-05,
      "loss": 0.1544,
      "step": 438
    },
    {
      "epoch": 0.6979332273449921,
      "grad_norm": 0.2676135301589966,
      "learning_rate": 6.121794871794873e-05,
      "loss": 0.0273,
      "step": 439
    },
    {
      "epoch": 0.699523052464229,
      "grad_norm": 0.19490210711956024,
      "learning_rate": 6.089743589743589e-05,
      "loss": 0.0092,
      "step": 440
    },
    {
      "epoch": 0.7011128775834659,
      "grad_norm": 0.9059067964553833,
      "learning_rate": 6.0576923076923076e-05,
      "loss": 0.1121,
      "step": 441
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 0.14103089272975922,
      "learning_rate": 6.025641025641026e-05,
      "loss": 0.0119,
      "step": 442
    },
    {
      "epoch": 0.7042925278219396,
      "grad_norm": 0.7647470235824585,
      "learning_rate": 5.9935897435897434e-05,
      "loss": 0.0069,
      "step": 443
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.05064069479703903,
      "learning_rate": 5.9615384615384616e-05,
      "loss": 0.0037,
      "step": 444
    },
    {
      "epoch": 0.7074721780604134,
      "grad_norm": 0.2060815393924713,
      "learning_rate": 5.92948717948718e-05,
      "loss": 0.0114,
      "step": 445
    },
    {
      "epoch": 0.7090620031796503,
      "grad_norm": 0.8886581659317017,
      "learning_rate": 5.897435897435898e-05,
      "loss": 0.1592,
      "step": 446
    },
    {
      "epoch": 0.7106518282988871,
      "grad_norm": 0.4381757080554962,
      "learning_rate": 5.865384615384616e-05,
      "loss": 0.0919,
      "step": 447
    },
    {
      "epoch": 0.712241653418124,
      "grad_norm": 0.7069726586341858,
      "learning_rate": 5.833333333333334e-05,
      "loss": 0.1145,
      "step": 448
    },
    {
      "epoch": 0.7138314785373608,
      "grad_norm": 0.5198897123336792,
      "learning_rate": 5.801282051282052e-05,
      "loss": 0.0223,
      "step": 449
    },
    {
      "epoch": 0.7154213036565977,
      "grad_norm": 0.07311658561229706,
      "learning_rate": 5.769230769230769e-05,
      "loss": 0.0063,
      "step": 450
    },
    {
      "epoch": 0.7170111287758346,
      "grad_norm": 0.37422508001327515,
      "learning_rate": 5.737179487179487e-05,
      "loss": 0.0674,
      "step": 451
    },
    {
      "epoch": 0.7186009538950715,
      "grad_norm": 0.038113392889499664,
      "learning_rate": 5.705128205128205e-05,
      "loss": 0.0047,
      "step": 452
    },
    {
      "epoch": 0.7201907790143084,
      "grad_norm": 0.07107924669981003,
      "learning_rate": 5.673076923076923e-05,
      "loss": 0.0066,
      "step": 453
    },
    {
      "epoch": 0.7217806041335453,
      "grad_norm": 0.047776948660612106,
      "learning_rate": 5.6410256410256414e-05,
      "loss": 0.0045,
      "step": 454
    },
    {
      "epoch": 0.7233704292527822,
      "grad_norm": 0.029329223558306694,
      "learning_rate": 5.608974358974359e-05,
      "loss": 0.0036,
      "step": 455
    },
    {
      "epoch": 0.724960254372019,
      "grad_norm": 0.7100270390510559,
      "learning_rate": 5.576923076923077e-05,
      "loss": 0.0579,
      "step": 456
    },
    {
      "epoch": 0.7265500794912559,
      "grad_norm": 0.33858221769332886,
      "learning_rate": 5.5448717948717955e-05,
      "loss": 0.1635,
      "step": 457
    },
    {
      "epoch": 0.7281399046104928,
      "grad_norm": 0.28126323223114014,
      "learning_rate": 5.512820512820514e-05,
      "loss": 0.1051,
      "step": 458
    },
    {
      "epoch": 0.7297297297297297,
      "grad_norm": 0.7615417838096619,
      "learning_rate": 5.480769230769231e-05,
      "loss": 0.1192,
      "step": 459
    },
    {
      "epoch": 0.7313195548489666,
      "grad_norm": 0.07096432149410248,
      "learning_rate": 5.448717948717948e-05,
      "loss": 0.0066,
      "step": 460
    },
    {
      "epoch": 0.7329093799682035,
      "grad_norm": 0.39722785353660583,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 0.2187,
      "step": 461
    },
    {
      "epoch": 0.7344992050874404,
      "grad_norm": 0.06484033912420273,
      "learning_rate": 5.384615384615385e-05,
      "loss": 0.0055,
      "step": 462
    },
    {
      "epoch": 0.7360890302066773,
      "grad_norm": 0.43679511547088623,
      "learning_rate": 5.352564102564103e-05,
      "loss": 0.1992,
      "step": 463
    },
    {
      "epoch": 0.7376788553259142,
      "grad_norm": 0.38307881355285645,
      "learning_rate": 5.3205128205128205e-05,
      "loss": 0.0929,
      "step": 464
    },
    {
      "epoch": 0.739268680445151,
      "grad_norm": 0.09781390428543091,
      "learning_rate": 5.288461538461539e-05,
      "loss": 0.0099,
      "step": 465
    },
    {
      "epoch": 0.7408585055643879,
      "grad_norm": 0.4745712876319885,
      "learning_rate": 5.256410256410257e-05,
      "loss": 0.0638,
      "step": 466
    },
    {
      "epoch": 0.7424483306836248,
      "grad_norm": 0.043424129486083984,
      "learning_rate": 5.224358974358975e-05,
      "loss": 0.0062,
      "step": 467
    },
    {
      "epoch": 0.7440381558028617,
      "grad_norm": 0.10538580268621445,
      "learning_rate": 5.192307692307693e-05,
      "loss": 0.0121,
      "step": 468
    },
    {
      "epoch": 0.7456279809220986,
      "grad_norm": 0.07399966567754745,
      "learning_rate": 5.160256410256411e-05,
      "loss": 0.0103,
      "step": 469
    },
    {
      "epoch": 0.7472178060413355,
      "grad_norm": 0.6803885102272034,
      "learning_rate": 5.128205128205128e-05,
      "loss": 0.0647,
      "step": 470
    },
    {
      "epoch": 0.7488076311605724,
      "grad_norm": 0.5837774276733398,
      "learning_rate": 5.096153846153846e-05,
      "loss": 0.1086,
      "step": 471
    },
    {
      "epoch": 0.7503974562798092,
      "grad_norm": 1.2180873155593872,
      "learning_rate": 5.0641025641025644e-05,
      "loss": 0.0551,
      "step": 472
    },
    {
      "epoch": 0.7519872813990461,
      "grad_norm": 0.7271531820297241,
      "learning_rate": 5.032051282051282e-05,
      "loss": 0.2061,
      "step": 473
    },
    {
      "epoch": 0.753577106518283,
      "grad_norm": 1.2645918130874634,
      "learning_rate": 5e-05,
      "loss": 0.131,
      "step": 474
    },
    {
      "epoch": 0.7551669316375199,
      "grad_norm": 0.25617125630378723,
      "learning_rate": 4.9679487179487185e-05,
      "loss": 0.0089,
      "step": 475
    },
    {
      "epoch": 0.7567567567567568,
      "grad_norm": 0.4470112919807434,
      "learning_rate": 4.935897435897436e-05,
      "loss": 0.1022,
      "step": 476
    },
    {
      "epoch": 0.7583465818759937,
      "grad_norm": 0.5794839262962341,
      "learning_rate": 4.9038461538461536e-05,
      "loss": 0.0736,
      "step": 477
    },
    {
      "epoch": 0.7599364069952306,
      "grad_norm": 0.7078384757041931,
      "learning_rate": 4.871794871794872e-05,
      "loss": 0.1264,
      "step": 478
    },
    {
      "epoch": 0.7615262321144675,
      "grad_norm": 0.2737548351287842,
      "learning_rate": 4.83974358974359e-05,
      "loss": 0.0275,
      "step": 479
    },
    {
      "epoch": 0.7631160572337043,
      "grad_norm": 0.3559434711933136,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.0823,
      "step": 480
    },
    {
      "epoch": 0.7647058823529411,
      "grad_norm": 1.016586184501648,
      "learning_rate": 4.775641025641026e-05,
      "loss": 0.1043,
      "step": 481
    },
    {
      "epoch": 0.766295707472178,
      "grad_norm": 0.35064664483070374,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 0.0917,
      "step": 482
    },
    {
      "epoch": 0.7678855325914149,
      "grad_norm": 1.4021183252334595,
      "learning_rate": 4.711538461538462e-05,
      "loss": 0.0978,
      "step": 483
    },
    {
      "epoch": 0.7694753577106518,
      "grad_norm": 0.029113195836544037,
      "learning_rate": 4.67948717948718e-05,
      "loss": 0.0046,
      "step": 484
    },
    {
      "epoch": 0.7710651828298887,
      "grad_norm": 1.0644540786743164,
      "learning_rate": 4.6474358974358976e-05,
      "loss": 0.1135,
      "step": 485
    },
    {
      "epoch": 0.7726550079491256,
      "grad_norm": 1.0386345386505127,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.1259,
      "step": 486
    },
    {
      "epoch": 0.7742448330683624,
      "grad_norm": 0.4414748549461365,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.049,
      "step": 487
    },
    {
      "epoch": 0.7758346581875993,
      "grad_norm": 0.05321142449975014,
      "learning_rate": 4.5512820512820516e-05,
      "loss": 0.0068,
      "step": 488
    },
    {
      "epoch": 0.7774244833068362,
      "grad_norm": 0.24876399338245392,
      "learning_rate": 4.519230769230769e-05,
      "loss": 0.0345,
      "step": 489
    },
    {
      "epoch": 0.7790143084260731,
      "grad_norm": 0.6509722471237183,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 0.1278,
      "step": 490
    },
    {
      "epoch": 0.78060413354531,
      "grad_norm": 0.7901574373245239,
      "learning_rate": 4.455128205128206e-05,
      "loss": 0.1905,
      "step": 491
    },
    {
      "epoch": 0.7821939586645469,
      "grad_norm": 0.3163345456123352,
      "learning_rate": 4.423076923076923e-05,
      "loss": 0.0377,
      "step": 492
    },
    {
      "epoch": 0.7837837837837838,
      "grad_norm": 0.07569623738527298,
      "learning_rate": 4.3910256410256415e-05,
      "loss": 0.0088,
      "step": 493
    },
    {
      "epoch": 0.7853736089030207,
      "grad_norm": 0.13490818440914154,
      "learning_rate": 4.358974358974359e-05,
      "loss": 0.0135,
      "step": 494
    },
    {
      "epoch": 0.7869634340222575,
      "grad_norm": 0.6277848482131958,
      "learning_rate": 4.326923076923077e-05,
      "loss": 0.1204,
      "step": 495
    },
    {
      "epoch": 0.7885532591414944,
      "grad_norm": 0.7491888403892517,
      "learning_rate": 4.294871794871795e-05,
      "loss": 0.1135,
      "step": 496
    },
    {
      "epoch": 0.7901430842607313,
      "grad_norm": 0.26266399025917053,
      "learning_rate": 4.262820512820513e-05,
      "loss": 0.0521,
      "step": 497
    },
    {
      "epoch": 0.7917329093799682,
      "grad_norm": 0.2532867193222046,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.0185,
      "step": 498
    },
    {
      "epoch": 0.7933227344992051,
      "grad_norm": 0.3692580461502075,
      "learning_rate": 4.198717948717949e-05,
      "loss": 0.0958,
      "step": 499
    },
    {
      "epoch": 0.794912559618442,
      "grad_norm": 0.12262450158596039,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.01,
      "step": 500
    },
    {
      "epoch": 0.7965023847376789,
      "grad_norm": 0.08233499526977539,
      "learning_rate": 4.134615384615385e-05,
      "loss": 0.0078,
      "step": 501
    },
    {
      "epoch": 0.7980922098569158,
      "grad_norm": 0.5225794911384583,
      "learning_rate": 4.1025641025641023e-05,
      "loss": 0.0373,
      "step": 502
    },
    {
      "epoch": 0.7996820349761526,
      "grad_norm": 0.13028468191623688,
      "learning_rate": 4.0705128205128206e-05,
      "loss": 0.0094,
      "step": 503
    },
    {
      "epoch": 0.8012718600953895,
      "grad_norm": 0.4878152012825012,
      "learning_rate": 4.038461538461539e-05,
      "loss": 0.024,
      "step": 504
    },
    {
      "epoch": 0.8028616852146264,
      "grad_norm": 0.3273022472858429,
      "learning_rate": 4.006410256410257e-05,
      "loss": 0.0202,
      "step": 505
    },
    {
      "epoch": 0.8044515103338633,
      "grad_norm": 0.0800720751285553,
      "learning_rate": 3.974358974358974e-05,
      "loss": 0.0088,
      "step": 506
    },
    {
      "epoch": 0.8060413354531002,
      "grad_norm": 0.3615128993988037,
      "learning_rate": 3.942307692307692e-05,
      "loss": 0.0942,
      "step": 507
    },
    {
      "epoch": 0.8076311605723371,
      "grad_norm": 0.16100797057151794,
      "learning_rate": 3.9102564102564105e-05,
      "loss": 0.0189,
      "step": 508
    },
    {
      "epoch": 0.809220985691574,
      "grad_norm": 1.1281542778015137,
      "learning_rate": 3.878205128205129e-05,
      "loss": 0.2601,
      "step": 509
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 0.3244267702102661,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.0575,
      "step": 510
    },
    {
      "epoch": 0.8124006359300477,
      "grad_norm": 0.18732352554798126,
      "learning_rate": 3.814102564102564e-05,
      "loss": 0.0289,
      "step": 511
    },
    {
      "epoch": 0.8139904610492846,
      "grad_norm": 0.22477571666240692,
      "learning_rate": 3.782051282051282e-05,
      "loss": 0.02,
      "step": 512
    },
    {
      "epoch": 0.8155802861685215,
      "grad_norm": 1.0630759000778198,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.2468,
      "step": 513
    },
    {
      "epoch": 0.8171701112877583,
      "grad_norm": 3.5844967365264893,
      "learning_rate": 3.717948717948718e-05,
      "loss": 0.0688,
      "step": 514
    },
    {
      "epoch": 0.8187599364069952,
      "grad_norm": 0.29318520426750183,
      "learning_rate": 3.685897435897436e-05,
      "loss": 0.044,
      "step": 515
    },
    {
      "epoch": 0.8203497615262321,
      "grad_norm": 0.1847694218158722,
      "learning_rate": 3.653846153846154e-05,
      "loss": 0.076,
      "step": 516
    },
    {
      "epoch": 0.821939586645469,
      "grad_norm": 0.028306376188993454,
      "learning_rate": 3.621794871794872e-05,
      "loss": 0.0052,
      "step": 517
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.4104081392288208,
      "learning_rate": 3.58974358974359e-05,
      "loss": 0.105,
      "step": 518
    },
    {
      "epoch": 0.8251192368839427,
      "grad_norm": 0.514201283454895,
      "learning_rate": 3.557692307692308e-05,
      "loss": 0.0692,
      "step": 519
    },
    {
      "epoch": 0.8267090620031796,
      "grad_norm": 2.9920859336853027,
      "learning_rate": 3.525641025641026e-05,
      "loss": 0.0793,
      "step": 520
    },
    {
      "epoch": 0.8282988871224165,
      "grad_norm": 0.11031050235033035,
      "learning_rate": 3.4935897435897436e-05,
      "loss": 0.0278,
      "step": 521
    },
    {
      "epoch": 0.8298887122416534,
      "grad_norm": 0.33295464515686035,
      "learning_rate": 3.461538461538462e-05,
      "loss": 0.0249,
      "step": 522
    },
    {
      "epoch": 0.8314785373608903,
      "grad_norm": 0.1325834095478058,
      "learning_rate": 3.4294871794871794e-05,
      "loss": 0.012,
      "step": 523
    },
    {
      "epoch": 0.8330683624801272,
      "grad_norm": 0.028166208416223526,
      "learning_rate": 3.397435897435898e-05,
      "loss": 0.0044,
      "step": 524
    },
    {
      "epoch": 0.834658187599364,
      "grad_norm": 0.11423420161008835,
      "learning_rate": 3.365384615384616e-05,
      "loss": 0.008,
      "step": 525
    },
    {
      "epoch": 0.8362480127186009,
      "grad_norm": 0.027298124507069588,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0039,
      "step": 526
    },
    {
      "epoch": 0.8378378378378378,
      "grad_norm": 0.10101578384637833,
      "learning_rate": 3.301282051282051e-05,
      "loss": 0.008,
      "step": 527
    },
    {
      "epoch": 0.8394276629570747,
      "grad_norm": 0.022622017189860344,
      "learning_rate": 3.269230769230769e-05,
      "loss": 0.0027,
      "step": 528
    },
    {
      "epoch": 0.8410174880763116,
      "grad_norm": 0.01927768997848034,
      "learning_rate": 3.2371794871794876e-05,
      "loss": 0.003,
      "step": 529
    },
    {
      "epoch": 0.8426073131955485,
      "grad_norm": 0.195185124874115,
      "learning_rate": 3.205128205128206e-05,
      "loss": 0.0145,
      "step": 530
    },
    {
      "epoch": 0.8441971383147854,
      "grad_norm": 0.6776255369186401,
      "learning_rate": 3.1730769230769234e-05,
      "loss": 0.1752,
      "step": 531
    },
    {
      "epoch": 0.8457869634340223,
      "grad_norm": 0.26589953899383545,
      "learning_rate": 3.141025641025641e-05,
      "loss": 0.0306,
      "step": 532
    },
    {
      "epoch": 0.8473767885532592,
      "grad_norm": 0.047695573419332504,
      "learning_rate": 3.108974358974359e-05,
      "loss": 0.0037,
      "step": 533
    },
    {
      "epoch": 0.848966613672496,
      "grad_norm": 0.952949583530426,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.0969,
      "step": 534
    },
    {
      "epoch": 0.8505564387917329,
      "grad_norm": 15.068031311035156,
      "learning_rate": 3.0448717948717947e-05,
      "loss": 0.0213,
      "step": 535
    },
    {
      "epoch": 0.8521462639109698,
      "grad_norm": 0.031705182045698166,
      "learning_rate": 3.012820512820513e-05,
      "loss": 0.003,
      "step": 536
    },
    {
      "epoch": 0.8537360890302067,
      "grad_norm": 0.11410104483366013,
      "learning_rate": 2.9807692307692308e-05,
      "loss": 0.0139,
      "step": 537
    },
    {
      "epoch": 0.8553259141494436,
      "grad_norm": 0.021350299939513206,
      "learning_rate": 2.948717948717949e-05,
      "loss": 0.0035,
      "step": 538
    },
    {
      "epoch": 0.8569157392686805,
      "grad_norm": 0.00897607859224081,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0017,
      "step": 539
    },
    {
      "epoch": 0.8585055643879174,
      "grad_norm": 0.01773044466972351,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 0.0023,
      "step": 540
    },
    {
      "epoch": 0.8600953895071543,
      "grad_norm": 0.013403394259512424,
      "learning_rate": 2.8525641025641025e-05,
      "loss": 0.0032,
      "step": 541
    },
    {
      "epoch": 0.8616852146263911,
      "grad_norm": 0.05406513437628746,
      "learning_rate": 2.8205128205128207e-05,
      "loss": 0.0064,
      "step": 542
    },
    {
      "epoch": 0.863275039745628,
      "grad_norm": 0.49079403281211853,
      "learning_rate": 2.7884615384615386e-05,
      "loss": 0.0952,
      "step": 543
    },
    {
      "epoch": 0.8648648648648649,
      "grad_norm": 0.1740490347146988,
      "learning_rate": 2.756410256410257e-05,
      "loss": 0.0464,
      "step": 544
    },
    {
      "epoch": 0.8664546899841018,
      "grad_norm": 0.10218187421560287,
      "learning_rate": 2.724358974358974e-05,
      "loss": 0.0133,
      "step": 545
    },
    {
      "epoch": 0.8680445151033387,
      "grad_norm": 0.2712876498699188,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 0.1986,
      "step": 546
    },
    {
      "epoch": 0.8696343402225755,
      "grad_norm": 0.14431405067443848,
      "learning_rate": 2.6602564102564102e-05,
      "loss": 0.031,
      "step": 547
    },
    {
      "epoch": 0.8712241653418124,
      "grad_norm": 0.04445243626832962,
      "learning_rate": 2.6282051282051285e-05,
      "loss": 0.0029,
      "step": 548
    },
    {
      "epoch": 0.8728139904610492,
      "grad_norm": 1.1393450498580933,
      "learning_rate": 2.5961538461538464e-05,
      "loss": 0.1474,
      "step": 549
    },
    {
      "epoch": 0.8744038155802861,
      "grad_norm": 0.29243651032447815,
      "learning_rate": 2.564102564102564e-05,
      "loss": 0.0544,
      "step": 550
    },
    {
      "epoch": 0.875993640699523,
      "grad_norm": 0.3013174831867218,
      "learning_rate": 2.5320512820512822e-05,
      "loss": 0.0675,
      "step": 551
    },
    {
      "epoch": 0.8775834658187599,
      "grad_norm": 0.5137269496917725,
      "learning_rate": 2.5e-05,
      "loss": 0.1003,
      "step": 552
    },
    {
      "epoch": 0.8791732909379968,
      "grad_norm": 0.21304857730865479,
      "learning_rate": 2.467948717948718e-05,
      "loss": 0.0141,
      "step": 553
    },
    {
      "epoch": 0.8807631160572337,
      "grad_norm": 0.09941130131483078,
      "learning_rate": 2.435897435897436e-05,
      "loss": 0.0129,
      "step": 554
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.05878536403179169,
      "learning_rate": 2.4038461538461542e-05,
      "loss": 0.0072,
      "step": 555
    },
    {
      "epoch": 0.8839427662957074,
      "grad_norm": 0.030853156000375748,
      "learning_rate": 2.3717948717948718e-05,
      "loss": 0.0046,
      "step": 556
    },
    {
      "epoch": 0.8855325914149443,
      "grad_norm": 0.20913590490818024,
      "learning_rate": 2.33974358974359e-05,
      "loss": 0.091,
      "step": 557
    },
    {
      "epoch": 0.8871224165341812,
      "grad_norm": 0.3327164649963379,
      "learning_rate": 2.307692307692308e-05,
      "loss": 0.0545,
      "step": 558
    },
    {
      "epoch": 0.8887122416534181,
      "grad_norm": 0.38901931047439575,
      "learning_rate": 2.2756410256410258e-05,
      "loss": 0.1104,
      "step": 559
    },
    {
      "epoch": 0.890302066772655,
      "grad_norm": 0.039977967739105225,
      "learning_rate": 2.2435897435897437e-05,
      "loss": 0.0057,
      "step": 560
    },
    {
      "epoch": 0.8918918918918919,
      "grad_norm": 1.3999594449996948,
      "learning_rate": 2.2115384615384616e-05,
      "loss": 0.1946,
      "step": 561
    },
    {
      "epoch": 0.8934817170111288,
      "grad_norm": 0.350887656211853,
      "learning_rate": 2.1794871794871795e-05,
      "loss": 0.0478,
      "step": 562
    },
    {
      "epoch": 0.8950715421303657,
      "grad_norm": 0.39901936054229736,
      "learning_rate": 2.1474358974358974e-05,
      "loss": 0.1071,
      "step": 563
    },
    {
      "epoch": 0.8966613672496025,
      "grad_norm": 0.1523878276348114,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 0.0458,
      "step": 564
    },
    {
      "epoch": 0.8982511923688394,
      "grad_norm": 0.24243880808353424,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0208,
      "step": 565
    },
    {
      "epoch": 0.8998410174880763,
      "grad_norm": 0.6030502319335938,
      "learning_rate": 2.0512820512820512e-05,
      "loss": 0.1139,
      "step": 566
    },
    {
      "epoch": 0.9014308426073132,
      "grad_norm": 0.7137147188186646,
      "learning_rate": 2.0192307692307694e-05,
      "loss": 0.2177,
      "step": 567
    },
    {
      "epoch": 0.9030206677265501,
      "grad_norm": 0.2681841254234314,
      "learning_rate": 1.987179487179487e-05,
      "loss": 0.0988,
      "step": 568
    },
    {
      "epoch": 0.904610492845787,
      "grad_norm": 0.17929919064044952,
      "learning_rate": 1.9551282051282052e-05,
      "loss": 0.0095,
      "step": 569
    },
    {
      "epoch": 0.9062003179650239,
      "grad_norm": 0.19896769523620605,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.0447,
      "step": 570
    },
    {
      "epoch": 0.9077901430842608,
      "grad_norm": 0.027038849890232086,
      "learning_rate": 1.891025641025641e-05,
      "loss": 0.0039,
      "step": 571
    },
    {
      "epoch": 0.9093799682034976,
      "grad_norm": 0.03378848358988762,
      "learning_rate": 1.858974358974359e-05,
      "loss": 0.0049,
      "step": 572
    },
    {
      "epoch": 0.9109697933227345,
      "grad_norm": 0.1931103765964508,
      "learning_rate": 1.826923076923077e-05,
      "loss": 0.0166,
      "step": 573
    },
    {
      "epoch": 0.9125596184419714,
      "grad_norm": 0.5214623808860779,
      "learning_rate": 1.794871794871795e-05,
      "loss": 0.184,
      "step": 574
    },
    {
      "epoch": 0.9141494435612083,
      "grad_norm": 0.32721707224845886,
      "learning_rate": 1.762820512820513e-05,
      "loss": 0.0426,
      "step": 575
    },
    {
      "epoch": 0.9157392686804452,
      "grad_norm": 0.023780403658747673,
      "learning_rate": 1.730769230769231e-05,
      "loss": 0.0041,
      "step": 576
    },
    {
      "epoch": 0.9173290937996821,
      "grad_norm": 0.40310800075531006,
      "learning_rate": 1.698717948717949e-05,
      "loss": 0.0724,
      "step": 577
    },
    {
      "epoch": 0.918918918918919,
      "grad_norm": 0.20568832755088806,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0229,
      "step": 578
    },
    {
      "epoch": 0.9205087440381559,
      "grad_norm": 1.1144770383834839,
      "learning_rate": 1.6346153846153847e-05,
      "loss": 0.2394,
      "step": 579
    },
    {
      "epoch": 0.9220985691573926,
      "grad_norm": 0.38368135690689087,
      "learning_rate": 1.602564102564103e-05,
      "loss": 0.0332,
      "step": 580
    },
    {
      "epoch": 0.9236883942766295,
      "grad_norm": 0.7197378277778625,
      "learning_rate": 1.5705128205128205e-05,
      "loss": 0.1901,
      "step": 581
    },
    {
      "epoch": 0.9252782193958664,
      "grad_norm": 0.2785551846027374,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.0738,
      "step": 582
    },
    {
      "epoch": 0.9268680445151033,
      "grad_norm": 0.34358781576156616,
      "learning_rate": 1.5064102564102565e-05,
      "loss": 0.1038,
      "step": 583
    },
    {
      "epoch": 0.9284578696343402,
      "grad_norm": 0.8063223361968994,
      "learning_rate": 1.4743589743589745e-05,
      "loss": 0.1329,
      "step": 584
    },
    {
      "epoch": 0.9300476947535771,
      "grad_norm": 0.26527607440948486,
      "learning_rate": 1.4423076923076923e-05,
      "loss": 0.0563,
      "step": 585
    },
    {
      "epoch": 0.931637519872814,
      "grad_norm": 0.03494034707546234,
      "learning_rate": 1.4102564102564104e-05,
      "loss": 0.0038,
      "step": 586
    },
    {
      "epoch": 0.9332273449920508,
      "grad_norm": 0.3091793656349182,
      "learning_rate": 1.3782051282051284e-05,
      "loss": 0.0531,
      "step": 587
    },
    {
      "epoch": 0.9348171701112877,
      "grad_norm": 0.15547113120555878,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 0.0289,
      "step": 588
    },
    {
      "epoch": 0.9364069952305246,
      "grad_norm": 0.25894346833229065,
      "learning_rate": 1.3141025641025642e-05,
      "loss": 0.081,
      "step": 589
    },
    {
      "epoch": 0.9379968203497615,
      "grad_norm": 1.074092984199524,
      "learning_rate": 1.282051282051282e-05,
      "loss": 0.4028,
      "step": 590
    },
    {
      "epoch": 0.9395866454689984,
      "grad_norm": 0.9590990543365479,
      "learning_rate": 1.25e-05,
      "loss": 0.2223,
      "step": 591
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 0.5778259038925171,
      "learning_rate": 1.217948717948718e-05,
      "loss": 0.0954,
      "step": 592
    },
    {
      "epoch": 0.9427662957074722,
      "grad_norm": 0.05473596230149269,
      "learning_rate": 1.1858974358974359e-05,
      "loss": 0.0095,
      "step": 593
    },
    {
      "epoch": 0.9443561208267091,
      "grad_norm": 0.728413999080658,
      "learning_rate": 1.153846153846154e-05,
      "loss": 0.1075,
      "step": 594
    },
    {
      "epoch": 0.9459459459459459,
      "grad_norm": 0.20075443387031555,
      "learning_rate": 1.1217948717948719e-05,
      "loss": 0.0453,
      "step": 595
    },
    {
      "epoch": 0.9475357710651828,
      "grad_norm": 0.2955781817436218,
      "learning_rate": 1.0897435897435898e-05,
      "loss": 0.1052,
      "step": 596
    },
    {
      "epoch": 0.9491255961844197,
      "grad_norm": 0.5409541726112366,
      "learning_rate": 1.0576923076923077e-05,
      "loss": 0.1138,
      "step": 597
    },
    {
      "epoch": 0.9507154213036566,
      "grad_norm": 0.4581395089626312,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 0.1328,
      "step": 598
    },
    {
      "epoch": 0.9523052464228935,
      "grad_norm": 0.20902368426322937,
      "learning_rate": 9.935897435897435e-06,
      "loss": 0.0332,
      "step": 599
    },
    {
      "epoch": 0.9538950715421304,
      "grad_norm": 0.945778489112854,
      "learning_rate": 9.615384615384616e-06,
      "loss": 0.3731,
      "step": 600
    },
    {
      "epoch": 0.9554848966613673,
      "grad_norm": 0.6501793265342712,
      "learning_rate": 9.294871794871795e-06,
      "loss": 0.0679,
      "step": 601
    },
    {
      "epoch": 0.9570747217806042,
      "grad_norm": 0.7264038920402527,
      "learning_rate": 8.974358974358976e-06,
      "loss": 0.0424,
      "step": 602
    },
    {
      "epoch": 0.958664546899841,
      "grad_norm": 0.1112428605556488,
      "learning_rate": 8.653846153846155e-06,
      "loss": 0.0121,
      "step": 603
    },
    {
      "epoch": 0.9602543720190779,
      "grad_norm": 0.07825534045696259,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0047,
      "step": 604
    },
    {
      "epoch": 0.9618441971383148,
      "grad_norm": 1.4282974004745483,
      "learning_rate": 8.012820512820515e-06,
      "loss": 0.1196,
      "step": 605
    },
    {
      "epoch": 0.9634340222575517,
      "grad_norm": 0.9786293506622314,
      "learning_rate": 7.692307692307694e-06,
      "loss": 0.3487,
      "step": 606
    },
    {
      "epoch": 0.9650238473767886,
      "grad_norm": 0.7550525665283203,
      "learning_rate": 7.371794871794873e-06,
      "loss": 0.235,
      "step": 607
    },
    {
      "epoch": 0.9666136724960255,
      "grad_norm": 0.4755193591117859,
      "learning_rate": 7.051282051282052e-06,
      "loss": 0.0402,
      "step": 608
    },
    {
      "epoch": 0.9682034976152624,
      "grad_norm": 0.1876600682735443,
      "learning_rate": 6.730769230769231e-06,
      "loss": 0.0304,
      "step": 609
    },
    {
      "epoch": 0.9697933227344993,
      "grad_norm": 0.3316862881183624,
      "learning_rate": 6.41025641025641e-06,
      "loss": 0.0911,
      "step": 610
    },
    {
      "epoch": 0.9713831478537361,
      "grad_norm": 0.2576664388179779,
      "learning_rate": 6.08974358974359e-06,
      "loss": 0.0491,
      "step": 611
    },
    {
      "epoch": 0.972972972972973,
      "grad_norm": 0.3016000986099243,
      "learning_rate": 5.76923076923077e-06,
      "loss": 0.1119,
      "step": 612
    },
    {
      "epoch": 0.9745627980922098,
      "grad_norm": 1.252873420715332,
      "learning_rate": 5.448717948717949e-06,
      "loss": 0.5109,
      "step": 613
    },
    {
      "epoch": 0.9761526232114467,
      "grad_norm": 0.36293232440948486,
      "learning_rate": 5.128205128205128e-06,
      "loss": 0.0095,
      "step": 614
    },
    {
      "epoch": 0.9777424483306836,
      "grad_norm": 0.04506388679146767,
      "learning_rate": 4.807692307692308e-06,
      "loss": 0.0052,
      "step": 615
    },
    {
      "epoch": 0.9793322734499205,
      "grad_norm": 0.36272361874580383,
      "learning_rate": 4.487179487179488e-06,
      "loss": 0.1661,
      "step": 616
    },
    {
      "epoch": 0.9809220985691574,
      "grad_norm": 0.19996796548366547,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0204,
      "step": 617
    },
    {
      "epoch": 0.9825119236883942,
      "grad_norm": 0.25923722982406616,
      "learning_rate": 3.846153846153847e-06,
      "loss": 0.0355,
      "step": 618
    },
    {
      "epoch": 0.9841017488076311,
      "grad_norm": 0.08581427484750748,
      "learning_rate": 3.525641025641026e-06,
      "loss": 0.0118,
      "step": 619
    },
    {
      "epoch": 0.985691573926868,
      "grad_norm": 0.023814938962459564,
      "learning_rate": 3.205128205128205e-06,
      "loss": 0.0041,
      "step": 620
    },
    {
      "epoch": 0.9872813990461049,
      "grad_norm": 0.048914361745119095,
      "learning_rate": 2.884615384615385e-06,
      "loss": 0.0067,
      "step": 621
    },
    {
      "epoch": 0.9888712241653418,
      "grad_norm": 0.5196375250816345,
      "learning_rate": 2.564102564102564e-06,
      "loss": 0.1076,
      "step": 622
    },
    {
      "epoch": 0.9904610492845787,
      "grad_norm": 0.39141029119491577,
      "learning_rate": 2.243589743589744e-06,
      "loss": 0.0828,
      "step": 623
    },
    {
      "epoch": 0.9920508744038156,
      "grad_norm": 0.07142277806997299,
      "learning_rate": 1.9230769230769234e-06,
      "loss": 0.0079,
      "step": 624
    },
    {
      "epoch": 0.9936406995230525,
      "grad_norm": 0.22021885216236115,
      "learning_rate": 1.6025641025641025e-06,
      "loss": 0.0688,
      "step": 625
    },
    {
      "epoch": 0.9952305246422893,
      "grad_norm": 0.1393493115901947,
      "learning_rate": 1.282051282051282e-06,
      "loss": 0.0179,
      "step": 626
    },
    {
      "epoch": 0.9968203497615262,
      "grad_norm": 0.2218915969133377,
      "learning_rate": 9.615384615384617e-07,
      "loss": 0.0657,
      "step": 627
    },
    {
      "epoch": 0.9984101748807631,
      "grad_norm": 0.21446487307548523,
      "learning_rate": 6.41025641025641e-07,
      "loss": 0.0393,
      "step": 628
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.48409372568130493,
      "learning_rate": 3.205128205128205e-07,
      "loss": 0.0565,
      "step": 629
    }
  ],
  "logging_steps": 1,
  "max_steps": 629,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.521764817642127e+17,
  "train_batch_size": 14,
  "trial_name": null,
  "trial_params": null
}
