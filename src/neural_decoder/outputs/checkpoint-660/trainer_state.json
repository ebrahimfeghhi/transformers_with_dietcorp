{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 660,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015151515151515152,
      "grad_norm": 1.6581772565841675,
      "learning_rate": 0.0,
      "loss": 0.166,
      "step": 1
    },
    {
      "epoch": 0.0030303030303030303,
      "grad_norm": 2.194613218307495,
      "learning_rate": 4e-05,
      "loss": 0.2086,
      "step": 2
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 1.79635751247406,
      "learning_rate": 8e-05,
      "loss": 0.2227,
      "step": 3
    },
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 1.0684008598327637,
      "learning_rate": 0.00012,
      "loss": 0.2063,
      "step": 4
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 1.0232521295547485,
      "learning_rate": 0.00016,
      "loss": 0.2106,
      "step": 5
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.6701675653457642,
      "learning_rate": 0.0002,
      "loss": 0.1793,
      "step": 6
    },
    {
      "epoch": 0.010606060606060607,
      "grad_norm": 1.5113996267318726,
      "learning_rate": 0.00019969465648854963,
      "loss": 0.5249,
      "step": 7
    },
    {
      "epoch": 0.012121212121212121,
      "grad_norm": 0.737917959690094,
      "learning_rate": 0.00019938931297709925,
      "loss": 0.2971,
      "step": 8
    },
    {
      "epoch": 0.013636363636363636,
      "grad_norm": 0.6561943888664246,
      "learning_rate": 0.00019908396946564886,
      "loss": 0.2603,
      "step": 9
    },
    {
      "epoch": 0.015151515151515152,
      "grad_norm": 0.6145666241645813,
      "learning_rate": 0.00019877862595419848,
      "loss": 0.0536,
      "step": 10
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.1468768119812012,
      "learning_rate": 0.0001984732824427481,
      "loss": 0.6294,
      "step": 11
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.3557593822479248,
      "learning_rate": 0.00019816793893129772,
      "loss": 0.0275,
      "step": 12
    },
    {
      "epoch": 0.019696969696969695,
      "grad_norm": 0.5426167845726013,
      "learning_rate": 0.00019786259541984734,
      "loss": 0.2979,
      "step": 13
    },
    {
      "epoch": 0.021212121212121213,
      "grad_norm": 0.445272833108902,
      "learning_rate": 0.00019755725190839695,
      "loss": 0.081,
      "step": 14
    },
    {
      "epoch": 0.022727272727272728,
      "grad_norm": 0.3235660493373871,
      "learning_rate": 0.00019725190839694657,
      "loss": 0.1765,
      "step": 15
    },
    {
      "epoch": 0.024242424242424242,
      "grad_norm": 0.43838825821876526,
      "learning_rate": 0.0001969465648854962,
      "loss": 0.1728,
      "step": 16
    },
    {
      "epoch": 0.025757575757575757,
      "grad_norm": 0.35360342264175415,
      "learning_rate": 0.0001966412213740458,
      "loss": 0.0908,
      "step": 17
    },
    {
      "epoch": 0.02727272727272727,
      "grad_norm": 0.6584914326667786,
      "learning_rate": 0.00019633587786259542,
      "loss": 0.2451,
      "step": 18
    },
    {
      "epoch": 0.02878787878787879,
      "grad_norm": 0.11616305261850357,
      "learning_rate": 0.00019603053435114504,
      "loss": 0.0078,
      "step": 19
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.722429633140564,
      "learning_rate": 0.00019572519083969466,
      "loss": 0.326,
      "step": 20
    },
    {
      "epoch": 0.031818181818181815,
      "grad_norm": 0.46776217222213745,
      "learning_rate": 0.00019541984732824428,
      "loss": 0.1361,
      "step": 21
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 27.798128128051758,
      "learning_rate": 0.0001951145038167939,
      "loss": 0.0585,
      "step": 22
    },
    {
      "epoch": 0.03484848484848485,
      "grad_norm": 0.458374947309494,
      "learning_rate": 0.00019480916030534354,
      "loss": 0.098,
      "step": 23
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.6128565073013306,
      "learning_rate": 0.00019450381679389313,
      "loss": 0.2398,
      "step": 24
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 0.5255648493766785,
      "learning_rate": 0.00019419847328244275,
      "loss": 0.0806,
      "step": 25
    },
    {
      "epoch": 0.03939393939393939,
      "grad_norm": 192.87557983398438,
      "learning_rate": 0.00019389312977099237,
      "loss": -0.1412,
      "step": 26
    },
    {
      "epoch": 0.04090909090909091,
      "grad_norm": 6.91743803024292,
      "learning_rate": 0.00019358778625954199,
      "loss": 0.2007,
      "step": 27
    },
    {
      "epoch": 0.04242424242424243,
      "grad_norm": 0.8006024360656738,
      "learning_rate": 0.00019328244274809163,
      "loss": 0.0604,
      "step": 28
    },
    {
      "epoch": 0.04393939393939394,
      "grad_norm": 1.1584186553955078,
      "learning_rate": 0.00019297709923664122,
      "loss": 0.3751,
      "step": 29
    },
    {
      "epoch": 0.045454545454545456,
      "grad_norm": 0.5772829055786133,
      "learning_rate": 0.00019267175572519084,
      "loss": 0.3493,
      "step": 30
    },
    {
      "epoch": 0.04696969696969697,
      "grad_norm": 0.672780454158783,
      "learning_rate": 0.00019236641221374049,
      "loss": 0.2435,
      "step": 31
    },
    {
      "epoch": 0.048484848484848485,
      "grad_norm": 0.5603072047233582,
      "learning_rate": 0.00019206106870229008,
      "loss": 0.191,
      "step": 32
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.44735682010650635,
      "learning_rate": 0.00019175572519083972,
      "loss": 0.1538,
      "step": 33
    },
    {
      "epoch": 0.051515151515151514,
      "grad_norm": 0.35993626713752747,
      "learning_rate": 0.0001914503816793893,
      "loss": 0.1684,
      "step": 34
    },
    {
      "epoch": 0.05303030303030303,
      "grad_norm": 0.9726187586784363,
      "learning_rate": 0.00019114503816793893,
      "loss": 0.234,
      "step": 35
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.5320051312446594,
      "learning_rate": 0.00019083969465648857,
      "loss": 0.1692,
      "step": 36
    },
    {
      "epoch": 0.05606060606060606,
      "grad_norm": 0.05403595417737961,
      "learning_rate": 0.00019053435114503817,
      "loss": 0.0054,
      "step": 37
    },
    {
      "epoch": 0.05757575757575758,
      "grad_norm": 0.4000055491924286,
      "learning_rate": 0.00019022900763358778,
      "loss": 0.1135,
      "step": 38
    },
    {
      "epoch": 0.05909090909090909,
      "grad_norm": 0.4608137905597687,
      "learning_rate": 0.00018992366412213743,
      "loss": 0.1809,
      "step": 39
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.4365978538990021,
      "learning_rate": 0.00018961832061068702,
      "loss": 0.1636,
      "step": 40
    },
    {
      "epoch": 0.06212121212121212,
      "grad_norm": 0.7151630520820618,
      "learning_rate": 0.00018931297709923666,
      "loss": 0.3963,
      "step": 41
    },
    {
      "epoch": 0.06363636363636363,
      "grad_norm": 0.43384233117103577,
      "learning_rate": 0.00018900763358778626,
      "loss": 0.1845,
      "step": 42
    },
    {
      "epoch": 0.06515151515151515,
      "grad_norm": 0.4858304560184479,
      "learning_rate": 0.00018870229007633587,
      "loss": 0.0768,
      "step": 43
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.49826541543006897,
      "learning_rate": 0.00018839694656488552,
      "loss": 0.1637,
      "step": 44
    },
    {
      "epoch": 0.06818181818181818,
      "grad_norm": 0.5848826169967651,
      "learning_rate": 0.0001880916030534351,
      "loss": 0.1594,
      "step": 45
    },
    {
      "epoch": 0.0696969696969697,
      "grad_norm": 0.3985864222049713,
      "learning_rate": 0.00018778625954198475,
      "loss": 0.1282,
      "step": 46
    },
    {
      "epoch": 0.07121212121212121,
      "grad_norm": 0.3474891483783722,
      "learning_rate": 0.00018748091603053437,
      "loss": 0.0896,
      "step": 47
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.5732653141021729,
      "learning_rate": 0.00018717557251908396,
      "loss": 0.1602,
      "step": 48
    },
    {
      "epoch": 0.07424242424242425,
      "grad_norm": 0.38505634665489197,
      "learning_rate": 0.0001868702290076336,
      "loss": 0.2038,
      "step": 49
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 0.4328509271144867,
      "learning_rate": 0.0001865648854961832,
      "loss": 0.1589,
      "step": 50
    },
    {
      "epoch": 0.07727272727272727,
      "grad_norm": 0.2898190915584564,
      "learning_rate": 0.00018625954198473284,
      "loss": 0.0712,
      "step": 51
    },
    {
      "epoch": 0.07878787878787878,
      "grad_norm": 0.6744569540023804,
      "learning_rate": 0.00018595419847328246,
      "loss": 0.2246,
      "step": 52
    },
    {
      "epoch": 0.0803030303030303,
      "grad_norm": 0.14617370069026947,
      "learning_rate": 0.00018564885496183205,
      "loss": 0.0109,
      "step": 53
    },
    {
      "epoch": 0.08181818181818182,
      "grad_norm": 0.21181219816207886,
      "learning_rate": 0.0001853435114503817,
      "loss": 0.0326,
      "step": 54
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 103.29425811767578,
      "learning_rate": 0.00018503816793893132,
      "loss": -0.0394,
      "step": 55
    },
    {
      "epoch": 0.08484848484848485,
      "grad_norm": 0.32915037870407104,
      "learning_rate": 0.00018473282442748093,
      "loss": 0.0844,
      "step": 56
    },
    {
      "epoch": 0.08636363636363636,
      "grad_norm": 0.39656445384025574,
      "learning_rate": 0.00018442748091603055,
      "loss": 0.1163,
      "step": 57
    },
    {
      "epoch": 0.08787878787878788,
      "grad_norm": 0.4278269112110138,
      "learning_rate": 0.00018412213740458014,
      "loss": 0.0896,
      "step": 58
    },
    {
      "epoch": 0.0893939393939394,
      "grad_norm": 0.32654425501823425,
      "learning_rate": 0.0001838167938931298,
      "loss": 0.0929,
      "step": 59
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.45452678203582764,
      "learning_rate": 0.0001835114503816794,
      "loss": 0.109,
      "step": 60
    },
    {
      "epoch": 0.09242424242424242,
      "grad_norm": 0.3594706356525421,
      "learning_rate": 0.00018320610687022902,
      "loss": 0.1167,
      "step": 61
    },
    {
      "epoch": 0.09393939393939393,
      "grad_norm": 0.30860328674316406,
      "learning_rate": 0.00018290076335877864,
      "loss": 0.1032,
      "step": 62
    },
    {
      "epoch": 0.09545454545454546,
      "grad_norm": 0.38647449016571045,
      "learning_rate": 0.00018259541984732826,
      "loss": 0.0923,
      "step": 63
    },
    {
      "epoch": 0.09696969696969697,
      "grad_norm": 0.7072554230690002,
      "learning_rate": 0.00018229007633587788,
      "loss": 0.3322,
      "step": 64
    },
    {
      "epoch": 0.09848484848484848,
      "grad_norm": 0.4744860827922821,
      "learning_rate": 0.0001819847328244275,
      "loss": 0.21,
      "step": 65
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.3426970839500427,
      "learning_rate": 0.0001816793893129771,
      "loss": 0.092,
      "step": 66
    },
    {
      "epoch": 0.10151515151515152,
      "grad_norm": 0.4093404710292816,
      "learning_rate": 0.00018137404580152673,
      "loss": 0.1511,
      "step": 67
    },
    {
      "epoch": 0.10303030303030303,
      "grad_norm": 16.467866897583008,
      "learning_rate": 0.00018106870229007635,
      "loss": 0.0871,
      "step": 68
    },
    {
      "epoch": 0.10454545454545454,
      "grad_norm": 0.2626763880252838,
      "learning_rate": 0.00018076335877862597,
      "loss": 0.0387,
      "step": 69
    },
    {
      "epoch": 0.10606060606060606,
      "grad_norm": 0.32667770981788635,
      "learning_rate": 0.00018045801526717558,
      "loss": 0.0633,
      "step": 70
    },
    {
      "epoch": 0.10757575757575757,
      "grad_norm": 0.19869866967201233,
      "learning_rate": 0.00018015267175572518,
      "loss": 0.0188,
      "step": 71
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.37606486678123474,
      "learning_rate": 0.00017984732824427482,
      "loss": 0.1482,
      "step": 72
    },
    {
      "epoch": 0.11060606060606061,
      "grad_norm": 0.23567691445350647,
      "learning_rate": 0.00017954198473282444,
      "loss": 0.0745,
      "step": 73
    },
    {
      "epoch": 0.11212121212121212,
      "grad_norm": 0.2607952952384949,
      "learning_rate": 0.00017923664122137406,
      "loss": 0.1226,
      "step": 74
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 0.1422782838344574,
      "learning_rate": 0.00017893129770992367,
      "loss": 0.0169,
      "step": 75
    },
    {
      "epoch": 0.11515151515151516,
      "grad_norm": 0.349584698677063,
      "learning_rate": 0.0001786259541984733,
      "loss": 0.116,
      "step": 76
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.73060142993927,
      "learning_rate": 0.0001783206106870229,
      "loss": 0.3508,
      "step": 77
    },
    {
      "epoch": 0.11818181818181818,
      "grad_norm": 17.871902465820312,
      "learning_rate": 0.00017801526717557253,
      "loss": 0.1119,
      "step": 78
    },
    {
      "epoch": 0.11969696969696969,
      "grad_norm": 0.6164959073066711,
      "learning_rate": 0.00017770992366412215,
      "loss": 0.1837,
      "step": 79
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.47737494111061096,
      "learning_rate": 0.00017740458015267176,
      "loss": 0.1158,
      "step": 80
    },
    {
      "epoch": 0.12272727272727273,
      "grad_norm": 0.1601424515247345,
      "learning_rate": 0.00017709923664122138,
      "loss": 0.0214,
      "step": 81
    },
    {
      "epoch": 0.12424242424242424,
      "grad_norm": 0.4546339809894562,
      "learning_rate": 0.000176793893129771,
      "loss": 0.2133,
      "step": 82
    },
    {
      "epoch": 0.12575757575757576,
      "grad_norm": 0.19080351293087006,
      "learning_rate": 0.00017648854961832062,
      "loss": 0.0229,
      "step": 83
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.3900983929634094,
      "learning_rate": 0.00017618320610687024,
      "loss": 0.0992,
      "step": 84
    },
    {
      "epoch": 0.12878787878787878,
      "grad_norm": 0.28389546275138855,
      "learning_rate": 0.00017587786259541985,
      "loss": 0.0609,
      "step": 85
    },
    {
      "epoch": 0.1303030303030303,
      "grad_norm": 0.45237621665000916,
      "learning_rate": 0.00017557251908396947,
      "loss": 0.1936,
      "step": 86
    },
    {
      "epoch": 0.1318181818181818,
      "grad_norm": 0.7109979391098022,
      "learning_rate": 0.0001752671755725191,
      "loss": 0.445,
      "step": 87
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.28745853900909424,
      "learning_rate": 0.0001749618320610687,
      "loss": 0.1243,
      "step": 88
    },
    {
      "epoch": 0.13484848484848486,
      "grad_norm": 35.483863830566406,
      "learning_rate": 0.00017465648854961833,
      "loss": 0.3965,
      "step": 89
    },
    {
      "epoch": 0.13636363636363635,
      "grad_norm": 0.4670080244541168,
      "learning_rate": 0.00017435114503816794,
      "loss": 0.1586,
      "step": 90
    },
    {
      "epoch": 0.13787878787878788,
      "grad_norm": 0.7415956258773804,
      "learning_rate": 0.00017404580152671756,
      "loss": 0.4256,
      "step": 91
    },
    {
      "epoch": 0.1393939393939394,
      "grad_norm": 0.5406932234764099,
      "learning_rate": 0.0001737404580152672,
      "loss": 0.2383,
      "step": 92
    },
    {
      "epoch": 0.1409090909090909,
      "grad_norm": 0.441890686750412,
      "learning_rate": 0.0001734351145038168,
      "loss": 0.0589,
      "step": 93
    },
    {
      "epoch": 0.14242424242424243,
      "grad_norm": 0.5473478436470032,
      "learning_rate": 0.00017312977099236641,
      "loss": 0.3149,
      "step": 94
    },
    {
      "epoch": 0.14393939393939395,
      "grad_norm": 0.3762539327144623,
      "learning_rate": 0.00017282442748091603,
      "loss": 0.1256,
      "step": 95
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.5249650478363037,
      "learning_rate": 0.00017251908396946565,
      "loss": 0.0905,
      "step": 96
    },
    {
      "epoch": 0.14696969696969697,
      "grad_norm": 0.6602277755737305,
      "learning_rate": 0.0001722137404580153,
      "loss": 0.3111,
      "step": 97
    },
    {
      "epoch": 0.1484848484848485,
      "grad_norm": 0.5767425298690796,
      "learning_rate": 0.0001719083969465649,
      "loss": 0.2094,
      "step": 98
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.21825043857097626,
      "learning_rate": 0.0001716030534351145,
      "loss": 0.084,
      "step": 99
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 6.989753723144531,
      "learning_rate": 0.00017129770992366415,
      "loss": 0.0597,
      "step": 100
    },
    {
      "epoch": 0.15303030303030302,
      "grad_norm": 5.158901214599609,
      "learning_rate": 0.00017099236641221374,
      "loss": 0.2118,
      "step": 101
    },
    {
      "epoch": 0.15454545454545454,
      "grad_norm": 0.12968626618385315,
      "learning_rate": 0.00017068702290076336,
      "loss": 0.0158,
      "step": 102
    },
    {
      "epoch": 0.15606060606060607,
      "grad_norm": 0.3885848820209503,
      "learning_rate": 0.00017038167938931298,
      "loss": 0.2039,
      "step": 103
    },
    {
      "epoch": 0.15757575757575756,
      "grad_norm": 0.28342750668525696,
      "learning_rate": 0.0001700763358778626,
      "loss": 0.1164,
      "step": 104
    },
    {
      "epoch": 0.1590909090909091,
      "grad_norm": 0.2690761387348175,
      "learning_rate": 0.00016977099236641224,
      "loss": 0.1189,
      "step": 105
    },
    {
      "epoch": 0.1606060606060606,
      "grad_norm": 0.5210319757461548,
      "learning_rate": 0.00016946564885496183,
      "loss": 0.2367,
      "step": 106
    },
    {
      "epoch": 0.1621212121212121,
      "grad_norm": 0.1969844400882721,
      "learning_rate": 0.00016916030534351145,
      "loss": 0.0248,
      "step": 107
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.5081754326820374,
      "learning_rate": 0.0001688549618320611,
      "loss": 0.1882,
      "step": 108
    },
    {
      "epoch": 0.16515151515151516,
      "grad_norm": 0.9131084680557251,
      "learning_rate": 0.00016854961832061068,
      "loss": 0.5751,
      "step": 109
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.6216911673545837,
      "learning_rate": 0.00016824427480916033,
      "loss": 0.252,
      "step": 110
    },
    {
      "epoch": 0.16818181818181818,
      "grad_norm": 0.2440713495016098,
      "learning_rate": 0.00016793893129770992,
      "loss": 0.0773,
      "step": 111
    },
    {
      "epoch": 0.1696969696969697,
      "grad_norm": 0.17353583872318268,
      "learning_rate": 0.00016763358778625954,
      "loss": 0.0614,
      "step": 112
    },
    {
      "epoch": 0.1712121212121212,
      "grad_norm": 0.3451027274131775,
      "learning_rate": 0.00016732824427480918,
      "loss": 0.1621,
      "step": 113
    },
    {
      "epoch": 0.17272727272727273,
      "grad_norm": 0.19788450002670288,
      "learning_rate": 0.00016702290076335877,
      "loss": 0.026,
      "step": 114
    },
    {
      "epoch": 0.17424242424242425,
      "grad_norm": 0.36091580986976624,
      "learning_rate": 0.00016671755725190842,
      "loss": 0.2096,
      "step": 115
    },
    {
      "epoch": 0.17575757575757575,
      "grad_norm": 0.06625908613204956,
      "learning_rate": 0.00016641221374045804,
      "loss": 0.0054,
      "step": 116
    },
    {
      "epoch": 0.17727272727272728,
      "grad_norm": 0.4987415075302124,
      "learning_rate": 0.00016610687022900763,
      "loss": 0.2253,
      "step": 117
    },
    {
      "epoch": 0.1787878787878788,
      "grad_norm": 0.5438210964202881,
      "learning_rate": 0.00016580152671755727,
      "loss": 0.1858,
      "step": 118
    },
    {
      "epoch": 0.1803030303030303,
      "grad_norm": 0.32577013969421387,
      "learning_rate": 0.00016549618320610686,
      "loss": 0.033,
      "step": 119
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.33136481046676636,
      "learning_rate": 0.0001651908396946565,
      "loss": 0.0372,
      "step": 120
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.45318806171417236,
      "learning_rate": 0.00016488549618320613,
      "loss": 0.1279,
      "step": 121
    },
    {
      "epoch": 0.18484848484848485,
      "grad_norm": 0.07060866802930832,
      "learning_rate": 0.00016458015267175572,
      "loss": 0.0084,
      "step": 122
    },
    {
      "epoch": 0.18636363636363637,
      "grad_norm": 0.08484664559364319,
      "learning_rate": 0.00016427480916030536,
      "loss": 0.0084,
      "step": 123
    },
    {
      "epoch": 0.18787878787878787,
      "grad_norm": 0.753382682800293,
      "learning_rate": 0.00016396946564885498,
      "loss": 0.3442,
      "step": 124
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 0.4779284596443176,
      "learning_rate": 0.0001636641221374046,
      "loss": 0.0934,
      "step": 125
    },
    {
      "epoch": 0.19090909090909092,
      "grad_norm": 0.4200878441333771,
      "learning_rate": 0.00016335877862595422,
      "loss": 0.0503,
      "step": 126
    },
    {
      "epoch": 0.19242424242424241,
      "grad_norm": 0.17489269375801086,
      "learning_rate": 0.0001630534351145038,
      "loss": 0.021,
      "step": 127
    },
    {
      "epoch": 0.19393939393939394,
      "grad_norm": 0.9513810276985168,
      "learning_rate": 0.00016274809160305345,
      "loss": 0.3178,
      "step": 128
    },
    {
      "epoch": 0.19545454545454546,
      "grad_norm": 0.5225187540054321,
      "learning_rate": 0.00016244274809160307,
      "loss": 0.1592,
      "step": 129
    },
    {
      "epoch": 0.19696969696969696,
      "grad_norm": 0.03684423491358757,
      "learning_rate": 0.0001621374045801527,
      "loss": 0.0035,
      "step": 130
    },
    {
      "epoch": 0.1984848484848485,
      "grad_norm": 0.4187193810939789,
      "learning_rate": 0.0001618320610687023,
      "loss": 0.1935,
      "step": 131
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.31738197803497314,
      "learning_rate": 0.0001615267175572519,
      "loss": 0.107,
      "step": 132
    },
    {
      "epoch": 0.2015151515151515,
      "grad_norm": 0.29593414068222046,
      "learning_rate": 0.00016122137404580154,
      "loss": 0.0458,
      "step": 133
    },
    {
      "epoch": 0.20303030303030303,
      "grad_norm": 0.2636069655418396,
      "learning_rate": 0.00016091603053435116,
      "loss": 0.051,
      "step": 134
    },
    {
      "epoch": 0.20454545454545456,
      "grad_norm": 0.2670671343803406,
      "learning_rate": 0.00016061068702290075,
      "loss": 0.0327,
      "step": 135
    },
    {
      "epoch": 0.20606060606060606,
      "grad_norm": 0.4422185719013214,
      "learning_rate": 0.0001603053435114504,
      "loss": 0.1007,
      "step": 136
    },
    {
      "epoch": 0.20757575757575758,
      "grad_norm": 0.10765037685632706,
      "learning_rate": 0.00016,
      "loss": 0.0093,
      "step": 137
    },
    {
      "epoch": 0.20909090909090908,
      "grad_norm": 0.9126988053321838,
      "learning_rate": 0.00015969465648854963,
      "loss": 0.2556,
      "step": 138
    },
    {
      "epoch": 0.2106060606060606,
      "grad_norm": 263.7380676269531,
      "learning_rate": 0.00015938931297709925,
      "loss": -0.0427,
      "step": 139
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.3028531074523926,
      "learning_rate": 0.00015908396946564884,
      "loss": 0.0811,
      "step": 140
    },
    {
      "epoch": 0.21363636363636362,
      "grad_norm": 109.03386688232422,
      "learning_rate": 0.00015877862595419848,
      "loss": -0.0091,
      "step": 141
    },
    {
      "epoch": 0.21515151515151515,
      "grad_norm": 0.6157652139663696,
      "learning_rate": 0.0001584732824427481,
      "loss": 0.1565,
      "step": 142
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.5624358654022217,
      "learning_rate": 0.00015816793893129772,
      "loss": 0.2831,
      "step": 143
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.5258870720863342,
      "learning_rate": 0.00015786259541984734,
      "loss": 0.0897,
      "step": 144
    },
    {
      "epoch": 0.2196969696969697,
      "grad_norm": 0.6182785630226135,
      "learning_rate": 0.00015755725190839696,
      "loss": 0.2615,
      "step": 145
    },
    {
      "epoch": 0.22121212121212122,
      "grad_norm": 0.5104339718818665,
      "learning_rate": 0.00015725190839694657,
      "loss": 0.1321,
      "step": 146
    },
    {
      "epoch": 0.22272727272727272,
      "grad_norm": 0.6746822595596313,
      "learning_rate": 0.0001569465648854962,
      "loss": 0.0803,
      "step": 147
    },
    {
      "epoch": 0.22424242424242424,
      "grad_norm": 0.1675143986940384,
      "learning_rate": 0.0001566412213740458,
      "loss": 0.0197,
      "step": 148
    },
    {
      "epoch": 0.22575757575757577,
      "grad_norm": 0.2514567971229553,
      "learning_rate": 0.00015633587786259543,
      "loss": 0.0226,
      "step": 149
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.3757849931716919,
      "learning_rate": 0.00015603053435114505,
      "loss": 0.0954,
      "step": 150
    },
    {
      "epoch": 0.2287878787878788,
      "grad_norm": 0.2599860429763794,
      "learning_rate": 0.00015572519083969466,
      "loss": 0.0676,
      "step": 151
    },
    {
      "epoch": 0.23030303030303031,
      "grad_norm": 0.6462053656578064,
      "learning_rate": 0.00015541984732824428,
      "loss": 0.1476,
      "step": 152
    },
    {
      "epoch": 0.2318181818181818,
      "grad_norm": 0.47653433680534363,
      "learning_rate": 0.0001551145038167939,
      "loss": 0.1224,
      "step": 153
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.33283382654190063,
      "learning_rate": 0.00015480916030534352,
      "loss": 0.0903,
      "step": 154
    },
    {
      "epoch": 0.23484848484848486,
      "grad_norm": 0.5367865562438965,
      "learning_rate": 0.00015450381679389314,
      "loss": 0.1061,
      "step": 155
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.5632612109184265,
      "learning_rate": 0.00015419847328244275,
      "loss": 0.1333,
      "step": 156
    },
    {
      "epoch": 0.23787878787878788,
      "grad_norm": 0.5272950530052185,
      "learning_rate": 0.00015389312977099237,
      "loss": 0.2326,
      "step": 157
    },
    {
      "epoch": 0.23939393939393938,
      "grad_norm": 0.5013611316680908,
      "learning_rate": 0.000153587786259542,
      "loss": 0.0501,
      "step": 158
    },
    {
      "epoch": 0.2409090909090909,
      "grad_norm": 0.4160436987876892,
      "learning_rate": 0.0001532824427480916,
      "loss": 0.0489,
      "step": 159
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.3626369833946228,
      "learning_rate": 0.00015297709923664123,
      "loss": 0.1265,
      "step": 160
    },
    {
      "epoch": 0.24393939393939393,
      "grad_norm": 0.30511632561683655,
      "learning_rate": 0.00015267175572519084,
      "loss": 0.1771,
      "step": 161
    },
    {
      "epoch": 0.24545454545454545,
      "grad_norm": 0.3611849248409271,
      "learning_rate": 0.00015236641221374046,
      "loss": 0.1142,
      "step": 162
    },
    {
      "epoch": 0.24696969696969698,
      "grad_norm": 0.47905632853507996,
      "learning_rate": 0.00015206106870229008,
      "loss": 0.2301,
      "step": 163
    },
    {
      "epoch": 0.24848484848484848,
      "grad_norm": 0.4990636110305786,
      "learning_rate": 0.0001517557251908397,
      "loss": 0.1764,
      "step": 164
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.5411873459815979,
      "learning_rate": 0.00015145038167938932,
      "loss": 0.1038,
      "step": 165
    },
    {
      "epoch": 0.2515151515151515,
      "grad_norm": 0.7891271114349365,
      "learning_rate": 0.00015114503816793893,
      "loss": 0.3525,
      "step": 166
    },
    {
      "epoch": 0.25303030303030305,
      "grad_norm": 0.36188092827796936,
      "learning_rate": 0.00015083969465648855,
      "loss": 0.0804,
      "step": 167
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.12410438805818558,
      "learning_rate": 0.00015053435114503817,
      "loss": 0.0145,
      "step": 168
    },
    {
      "epoch": 0.25606060606060604,
      "grad_norm": 0.4180682301521301,
      "learning_rate": 0.00015022900763358781,
      "loss": 0.1555,
      "step": 169
    },
    {
      "epoch": 0.25757575757575757,
      "grad_norm": 1.0809818506240845,
      "learning_rate": 0.0001499236641221374,
      "loss": 0.4053,
      "step": 170
    },
    {
      "epoch": 0.2590909090909091,
      "grad_norm": 0.2083192616701126,
      "learning_rate": 0.00014961832061068702,
      "loss": 0.0519,
      "step": 171
    },
    {
      "epoch": 0.2606060606060606,
      "grad_norm": 0.4370560348033905,
      "learning_rate": 0.00014931297709923664,
      "loss": 0.1575,
      "step": 172
    },
    {
      "epoch": 0.26212121212121214,
      "grad_norm": 0.7763327956199646,
      "learning_rate": 0.00014900763358778626,
      "loss": 0.2184,
      "step": 173
    },
    {
      "epoch": 0.2636363636363636,
      "grad_norm": 0.21391865611076355,
      "learning_rate": 0.0001487022900763359,
      "loss": 0.0298,
      "step": 174
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 0.6424331068992615,
      "learning_rate": 0.0001483969465648855,
      "loss": 0.2507,
      "step": 175
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.3909565806388855,
      "learning_rate": 0.0001480916030534351,
      "loss": 0.0522,
      "step": 176
    },
    {
      "epoch": 0.2681818181818182,
      "grad_norm": 0.09706159681081772,
      "learning_rate": 0.00014778625954198476,
      "loss": 0.0135,
      "step": 177
    },
    {
      "epoch": 0.2696969696969697,
      "grad_norm": 0.9273466467857361,
      "learning_rate": 0.00014748091603053435,
      "loss": 0.1129,
      "step": 178
    },
    {
      "epoch": 0.27121212121212124,
      "grad_norm": 0.6078581809997559,
      "learning_rate": 0.000147175572519084,
      "loss": 0.2095,
      "step": 179
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.5647233128547668,
      "learning_rate": 0.00014687022900763358,
      "loss": 0.1509,
      "step": 180
    },
    {
      "epoch": 0.27424242424242423,
      "grad_norm": 134.54905700683594,
      "learning_rate": 0.0001465648854961832,
      "loss": 0.025,
      "step": 181
    },
    {
      "epoch": 0.27575757575757576,
      "grad_norm": 0.37734487652778625,
      "learning_rate": 0.00014625954198473285,
      "loss": 0.0844,
      "step": 182
    },
    {
      "epoch": 0.2772727272727273,
      "grad_norm": 38.605308532714844,
      "learning_rate": 0.00014595419847328244,
      "loss": 0.0206,
      "step": 183
    },
    {
      "epoch": 0.2787878787878788,
      "grad_norm": 0.3756885528564453,
      "learning_rate": 0.00014564885496183208,
      "loss": 0.0823,
      "step": 184
    },
    {
      "epoch": 0.2803030303030303,
      "grad_norm": 0.3225860297679901,
      "learning_rate": 0.00014534351145038167,
      "loss": 0.1236,
      "step": 185
    },
    {
      "epoch": 0.2818181818181818,
      "grad_norm": 0.3812278211116791,
      "learning_rate": 0.0001450381679389313,
      "loss": 0.084,
      "step": 186
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.2842489182949066,
      "learning_rate": 0.00014473282442748094,
      "loss": 0.0429,
      "step": 187
    },
    {
      "epoch": 0.28484848484848485,
      "grad_norm": 0.5010543465614319,
      "learning_rate": 0.00014442748091603053,
      "loss": 0.1499,
      "step": 188
    },
    {
      "epoch": 0.2863636363636364,
      "grad_norm": 0.6453185081481934,
      "learning_rate": 0.00014412213740458017,
      "loss": 0.2171,
      "step": 189
    },
    {
      "epoch": 0.2878787878787879,
      "grad_norm": 0.4145536422729492,
      "learning_rate": 0.0001438167938931298,
      "loss": 0.0854,
      "step": 190
    },
    {
      "epoch": 0.28939393939393937,
      "grad_norm": 0.5717375874519348,
      "learning_rate": 0.00014351145038167938,
      "loss": 0.2863,
      "step": 191
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.31676799058914185,
      "learning_rate": 0.00014320610687022903,
      "loss": 0.0892,
      "step": 192
    },
    {
      "epoch": 0.2924242424242424,
      "grad_norm": 0.7586724162101746,
      "learning_rate": 0.00014290076335877862,
      "loss": 0.2974,
      "step": 193
    },
    {
      "epoch": 0.29393939393939394,
      "grad_norm": 57.3895263671875,
      "learning_rate": 0.00014259541984732824,
      "loss": -0.009,
      "step": 194
    },
    {
      "epoch": 0.29545454545454547,
      "grad_norm": 0.43186765909194946,
      "learning_rate": 0.00014229007633587788,
      "loss": 0.1408,
      "step": 195
    },
    {
      "epoch": 0.296969696969697,
      "grad_norm": 0.6593014001846313,
      "learning_rate": 0.00014198473282442747,
      "loss": 0.0977,
      "step": 196
    },
    {
      "epoch": 0.29848484848484846,
      "grad_norm": 0.45190685987472534,
      "learning_rate": 0.00014167938931297712,
      "loss": 0.156,
      "step": 197
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.2690173089504242,
      "learning_rate": 0.00014137404580152673,
      "loss": 0.0653,
      "step": 198
    },
    {
      "epoch": 0.3015151515151515,
      "grad_norm": 0.665794849395752,
      "learning_rate": 0.00014106870229007632,
      "loss": 0.3528,
      "step": 199
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.6502280235290527,
      "learning_rate": 0.00014076335877862597,
      "loss": 0.1849,
      "step": 200
    },
    {
      "epoch": 0.30454545454545456,
      "grad_norm": 0.21881096065044403,
      "learning_rate": 0.00014045801526717556,
      "loss": 0.0325,
      "step": 201
    },
    {
      "epoch": 0.30606060606060603,
      "grad_norm": 0.6322373151779175,
      "learning_rate": 0.0001401526717557252,
      "loss": 0.2789,
      "step": 202
    },
    {
      "epoch": 0.30757575757575756,
      "grad_norm": 0.46295344829559326,
      "learning_rate": 0.00013984732824427482,
      "loss": 0.1211,
      "step": 203
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.8610186576843262,
      "learning_rate": 0.00013954198473282441,
      "loss": 0.269,
      "step": 204
    },
    {
      "epoch": 0.3106060606060606,
      "grad_norm": 0.5127667188644409,
      "learning_rate": 0.00013923664122137406,
      "loss": 0.2584,
      "step": 205
    },
    {
      "epoch": 0.31212121212121213,
      "grad_norm": 0.6458544731140137,
      "learning_rate": 0.00013893129770992368,
      "loss": 0.1842,
      "step": 206
    },
    {
      "epoch": 0.31363636363636366,
      "grad_norm": 0.5214458107948303,
      "learning_rate": 0.0001386259541984733,
      "loss": 0.2614,
      "step": 207
    },
    {
      "epoch": 0.3151515151515151,
      "grad_norm": 77.16895294189453,
      "learning_rate": 0.0001383206106870229,
      "loss": -0.031,
      "step": 208
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.09312012046575546,
      "learning_rate": 0.0001380152671755725,
      "loss": 0.0109,
      "step": 209
    },
    {
      "epoch": 0.3181818181818182,
      "grad_norm": 0.5664196014404297,
      "learning_rate": 0.00013770992366412215,
      "loss": 0.1946,
      "step": 210
    },
    {
      "epoch": 0.3196969696969697,
      "grad_norm": 0.25476914644241333,
      "learning_rate": 0.00013740458015267177,
      "loss": 0.0488,
      "step": 211
    },
    {
      "epoch": 0.3212121212121212,
      "grad_norm": 9.824403762817383,
      "learning_rate": 0.00013709923664122139,
      "loss": -0.023,
      "step": 212
    },
    {
      "epoch": 0.32272727272727275,
      "grad_norm": 0.07753824442625046,
      "learning_rate": 0.000136793893129771,
      "loss": 0.0082,
      "step": 213
    },
    {
      "epoch": 0.3242424242424242,
      "grad_norm": 0.5554985404014587,
      "learning_rate": 0.00013648854961832062,
      "loss": 0.2715,
      "step": 214
    },
    {
      "epoch": 0.32575757575757575,
      "grad_norm": 0.2127777338027954,
      "learning_rate": 0.00013618320610687024,
      "loss": 0.0491,
      "step": 215
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.3344554603099823,
      "learning_rate": 0.00013587786259541986,
      "loss": 0.1567,
      "step": 216
    },
    {
      "epoch": 0.3287878787878788,
      "grad_norm": 0.2310340851545334,
      "learning_rate": 0.00013557251908396947,
      "loss": 0.0564,
      "step": 217
    },
    {
      "epoch": 0.3303030303030303,
      "grad_norm": 0.447344571352005,
      "learning_rate": 0.0001352671755725191,
      "loss": 0.1144,
      "step": 218
    },
    {
      "epoch": 0.33181818181818185,
      "grad_norm": 0.4466055929660797,
      "learning_rate": 0.0001349618320610687,
      "loss": 0.2547,
      "step": 219
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.0516337156295776,
      "learning_rate": 0.00013465648854961833,
      "loss": 0.4991,
      "step": 220
    },
    {
      "epoch": 0.33484848484848484,
      "grad_norm": 0.5473957061767578,
      "learning_rate": 0.00013435114503816795,
      "loss": 0.2141,
      "step": 221
    },
    {
      "epoch": 0.33636363636363636,
      "grad_norm": 0.2597101926803589,
      "learning_rate": 0.00013404580152671756,
      "loss": 0.088,
      "step": 222
    },
    {
      "epoch": 0.3378787878787879,
      "grad_norm": 0.5605872869491577,
      "learning_rate": 0.00013374045801526718,
      "loss": 0.1634,
      "step": 223
    },
    {
      "epoch": 0.3393939393939394,
      "grad_norm": 12.93014907836914,
      "learning_rate": 0.0001334351145038168,
      "loss": 0.0957,
      "step": 224
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 0.3119237720966339,
      "learning_rate": 0.00013312977099236642,
      "loss": 0.1223,
      "step": 225
    },
    {
      "epoch": 0.3424242424242424,
      "grad_norm": 0.3055102825164795,
      "learning_rate": 0.00013282442748091604,
      "loss": 0.0371,
      "step": 226
    },
    {
      "epoch": 0.34393939393939393,
      "grad_norm": 0.4661175310611725,
      "learning_rate": 0.00013251908396946565,
      "loss": 0.1934,
      "step": 227
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.4444158971309662,
      "learning_rate": 0.00013221374045801527,
      "loss": 0.0731,
      "step": 228
    },
    {
      "epoch": 0.346969696969697,
      "grad_norm": 433.896728515625,
      "learning_rate": 0.0001319083969465649,
      "loss": 0.0708,
      "step": 229
    },
    {
      "epoch": 0.3484848484848485,
      "grad_norm": 0.4247788190841675,
      "learning_rate": 0.0001316030534351145,
      "loss": 0.1058,
      "step": 230
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.6316037178039551,
      "learning_rate": 0.00013129770992366413,
      "loss": 0.2324,
      "step": 231
    },
    {
      "epoch": 0.3515151515151515,
      "grad_norm": 0.35305294394493103,
      "learning_rate": 0.00013099236641221374,
      "loss": 0.0642,
      "step": 232
    },
    {
      "epoch": 0.353030303030303,
      "grad_norm": 0.28516432642936707,
      "learning_rate": 0.00013068702290076336,
      "loss": 0.0451,
      "step": 233
    },
    {
      "epoch": 0.35454545454545455,
      "grad_norm": 0.6390625238418579,
      "learning_rate": 0.00013038167938931298,
      "loss": 0.2115,
      "step": 234
    },
    {
      "epoch": 0.3560606060606061,
      "grad_norm": 0.3023185729980469,
      "learning_rate": 0.0001300763358778626,
      "loss": 0.0506,
      "step": 235
    },
    {
      "epoch": 0.3575757575757576,
      "grad_norm": 0.607515275478363,
      "learning_rate": 0.00012977099236641222,
      "loss": 0.2054,
      "step": 236
    },
    {
      "epoch": 0.35909090909090907,
      "grad_norm": 0.436372309923172,
      "learning_rate": 0.00012946564885496183,
      "loss": 0.1668,
      "step": 237
    },
    {
      "epoch": 0.3606060606060606,
      "grad_norm": 0.4497843086719513,
      "learning_rate": 0.00012916030534351148,
      "loss": 0.0848,
      "step": 238
    },
    {
      "epoch": 0.3621212121212121,
      "grad_norm": 114.4992904663086,
      "learning_rate": 0.00012885496183206107,
      "loss": -0.0712,
      "step": 239
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.1738242208957672,
      "learning_rate": 0.0001285496183206107,
      "loss": 0.018,
      "step": 240
    },
    {
      "epoch": 0.36515151515151517,
      "grad_norm": 0.3746147155761719,
      "learning_rate": 0.0001282442748091603,
      "loss": 0.0994,
      "step": 241
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.8537232875823975,
      "learning_rate": 0.00012793893129770992,
      "loss": 0.4103,
      "step": 242
    },
    {
      "epoch": 0.36818181818181817,
      "grad_norm": 0.7958910465240479,
      "learning_rate": 0.00012763358778625957,
      "loss": 0.1859,
      "step": 243
    },
    {
      "epoch": 0.3696969696969697,
      "grad_norm": 0.7110143899917603,
      "learning_rate": 0.00012732824427480916,
      "loss": 0.1682,
      "step": 244
    },
    {
      "epoch": 0.3712121212121212,
      "grad_norm": 0.38867974281311035,
      "learning_rate": 0.00012702290076335878,
      "loss": 0.1269,
      "step": 245
    },
    {
      "epoch": 0.37272727272727274,
      "grad_norm": 0.1878398209810257,
      "learning_rate": 0.0001267175572519084,
      "loss": 0.0247,
      "step": 246
    },
    {
      "epoch": 0.37424242424242427,
      "grad_norm": 0.48075294494628906,
      "learning_rate": 0.000126412213740458,
      "loss": 0.2107,
      "step": 247
    },
    {
      "epoch": 0.37575757575757573,
      "grad_norm": 0.8547677993774414,
      "learning_rate": 0.00012610687022900766,
      "loss": 0.3024,
      "step": 248
    },
    {
      "epoch": 0.37727272727272726,
      "grad_norm": 0.26835769414901733,
      "learning_rate": 0.00012580152671755725,
      "loss": 0.0316,
      "step": 249
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 770.7052001953125,
      "learning_rate": 0.00012549618320610687,
      "loss": -0.4365,
      "step": 250
    },
    {
      "epoch": 0.3803030303030303,
      "grad_norm": 0.936552882194519,
      "learning_rate": 0.0001251908396946565,
      "loss": 0.3105,
      "step": 251
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.16363027691841125,
      "learning_rate": 0.0001248854961832061,
      "loss": 0.0221,
      "step": 252
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 0.603888750076294,
      "learning_rate": 0.00012458015267175575,
      "loss": 0.1682,
      "step": 253
    },
    {
      "epoch": 0.38484848484848483,
      "grad_norm": 0.15674112737178802,
      "learning_rate": 0.00012427480916030534,
      "loss": 0.014,
      "step": 254
    },
    {
      "epoch": 0.38636363636363635,
      "grad_norm": 0.2151639759540558,
      "learning_rate": 0.00012396946564885496,
      "loss": 0.0217,
      "step": 255
    },
    {
      "epoch": 0.3878787878787879,
      "grad_norm": 0.5533366203308105,
      "learning_rate": 0.0001236641221374046,
      "loss": 0.1557,
      "step": 256
    },
    {
      "epoch": 0.3893939393939394,
      "grad_norm": 0.7971954345703125,
      "learning_rate": 0.0001233587786259542,
      "loss": 0.3185,
      "step": 257
    },
    {
      "epoch": 0.39090909090909093,
      "grad_norm": 0.20721209049224854,
      "learning_rate": 0.0001230534351145038,
      "loss": 0.0773,
      "step": 258
    },
    {
      "epoch": 0.3924242424242424,
      "grad_norm": 0.9393828511238098,
      "learning_rate": 0.00012274809160305346,
      "loss": 0.2819,
      "step": 259
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.6774687170982361,
      "learning_rate": 0.00012244274809160305,
      "loss": 0.3552,
      "step": 260
    },
    {
      "epoch": 0.39545454545454545,
      "grad_norm": 0.4392916262149811,
      "learning_rate": 0.0001221374045801527,
      "loss": 0.0948,
      "step": 261
    },
    {
      "epoch": 0.396969696969697,
      "grad_norm": 0.20673847198486328,
      "learning_rate": 0.00012183206106870228,
      "loss": 0.0279,
      "step": 262
    },
    {
      "epoch": 0.3984848484848485,
      "grad_norm": 0.8485525250434875,
      "learning_rate": 0.00012152671755725191,
      "loss": 0.5238,
      "step": 263
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.3956034183502197,
      "learning_rate": 0.00012122137404580154,
      "loss": 0.068,
      "step": 264
    },
    {
      "epoch": 0.4015151515151515,
      "grad_norm": 0.8126867413520813,
      "learning_rate": 0.00012091603053435115,
      "loss": 0.2452,
      "step": 265
    },
    {
      "epoch": 0.403030303030303,
      "grad_norm": 0.7981924414634705,
      "learning_rate": 0.00012061068702290077,
      "loss": 0.1935,
      "step": 266
    },
    {
      "epoch": 0.40454545454545454,
      "grad_norm": 0.4218292832374573,
      "learning_rate": 0.0001203053435114504,
      "loss": 0.0637,
      "step": 267
    },
    {
      "epoch": 0.40606060606060607,
      "grad_norm": 0.2827475965023041,
      "learning_rate": 0.00012,
      "loss": 0.0215,
      "step": 268
    },
    {
      "epoch": 0.4075757575757576,
      "grad_norm": 0.3457774519920349,
      "learning_rate": 0.00011969465648854963,
      "loss": 0.1878,
      "step": 269
    },
    {
      "epoch": 0.4090909090909091,
      "grad_norm": 0.7461788654327393,
      "learning_rate": 0.00011938931297709924,
      "loss": 0.2892,
      "step": 270
    },
    {
      "epoch": 0.4106060606060606,
      "grad_norm": 0.11785706877708435,
      "learning_rate": 0.00011908396946564886,
      "loss": 0.0106,
      "step": 271
    },
    {
      "epoch": 0.4121212121212121,
      "grad_norm": 0.1292726993560791,
      "learning_rate": 0.00011877862595419849,
      "loss": 0.0124,
      "step": 272
    },
    {
      "epoch": 0.41363636363636364,
      "grad_norm": 0.6402590274810791,
      "learning_rate": 0.00011847328244274809,
      "loss": 0.1465,
      "step": 273
    },
    {
      "epoch": 0.41515151515151516,
      "grad_norm": 0.14304441213607788,
      "learning_rate": 0.00011816793893129771,
      "loss": 0.0137,
      "step": 274
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.17571836709976196,
      "learning_rate": 0.00011786259541984734,
      "loss": 0.02,
      "step": 275
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.9687709808349609,
      "learning_rate": 0.00011755725190839695,
      "loss": 0.3585,
      "step": 276
    },
    {
      "epoch": 0.4196969696969697,
      "grad_norm": 0.9326005578041077,
      "learning_rate": 0.00011725190839694658,
      "loss": 0.2343,
      "step": 277
    },
    {
      "epoch": 0.4212121212121212,
      "grad_norm": 0.641667902469635,
      "learning_rate": 0.00011694656488549618,
      "loss": 0.2032,
      "step": 278
    },
    {
      "epoch": 0.42272727272727273,
      "grad_norm": 0.5488392114639282,
      "learning_rate": 0.0001166412213740458,
      "loss": 0.2656,
      "step": 279
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.6382659077644348,
      "learning_rate": 0.00011633587786259543,
      "loss": 0.1797,
      "step": 280
    },
    {
      "epoch": 0.4257575757575758,
      "grad_norm": 1.1831884384155273,
      "learning_rate": 0.00011603053435114504,
      "loss": 0.6106,
      "step": 281
    },
    {
      "epoch": 0.42727272727272725,
      "grad_norm": 0.5787655711174011,
      "learning_rate": 0.00011572519083969467,
      "loss": 0.1126,
      "step": 282
    },
    {
      "epoch": 0.4287878787878788,
      "grad_norm": 0.8091986179351807,
      "learning_rate": 0.00011541984732824429,
      "loss": 0.3179,
      "step": 283
    },
    {
      "epoch": 0.4303030303030303,
      "grad_norm": 0.5632410645484924,
      "learning_rate": 0.00011511450381679389,
      "loss": 0.1052,
      "step": 284
    },
    {
      "epoch": 0.4318181818181818,
      "grad_norm": 0.4057340919971466,
      "learning_rate": 0.00011480916030534352,
      "loss": 0.0599,
      "step": 285
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.1485772579908371,
      "learning_rate": 0.00011450381679389313,
      "loss": 0.0173,
      "step": 286
    },
    {
      "epoch": 0.4348484848484849,
      "grad_norm": 0.46948614716529846,
      "learning_rate": 0.00011419847328244276,
      "loss": 0.1413,
      "step": 287
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.328360378742218,
      "learning_rate": 0.00011389312977099238,
      "loss": 0.084,
      "step": 288
    },
    {
      "epoch": 0.43787878787878787,
      "grad_norm": 0.43120458722114563,
      "learning_rate": 0.00011358778625954198,
      "loss": 0.1294,
      "step": 289
    },
    {
      "epoch": 0.4393939393939394,
      "grad_norm": 0.6463081240653992,
      "learning_rate": 0.00011328244274809161,
      "loss": 0.1694,
      "step": 290
    },
    {
      "epoch": 0.4409090909090909,
      "grad_norm": 0.27809035778045654,
      "learning_rate": 0.00011297709923664124,
      "loss": 0.0727,
      "step": 291
    },
    {
      "epoch": 0.44242424242424244,
      "grad_norm": 0.25395697355270386,
      "learning_rate": 0.00011267175572519085,
      "loss": 0.0268,
      "step": 292
    },
    {
      "epoch": 0.44393939393939397,
      "grad_norm": 0.30475059151649475,
      "learning_rate": 0.00011236641221374046,
      "loss": 0.0676,
      "step": 293
    },
    {
      "epoch": 0.44545454545454544,
      "grad_norm": 0.4184897840023041,
      "learning_rate": 0.00011206106870229007,
      "loss": 0.1014,
      "step": 294
    },
    {
      "epoch": 0.44696969696969696,
      "grad_norm": 0.44563567638397217,
      "learning_rate": 0.0001117557251908397,
      "loss": 0.0912,
      "step": 295
    },
    {
      "epoch": 0.4484848484848485,
      "grad_norm": 0.2635926604270935,
      "learning_rate": 0.00011145038167938933,
      "loss": 0.052,
      "step": 296
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.13896842300891876,
      "learning_rate": 0.00011114503816793894,
      "loss": 0.0142,
      "step": 297
    },
    {
      "epoch": 0.45151515151515154,
      "grad_norm": 0.18571193516254425,
      "learning_rate": 0.00011083969465648855,
      "loss": 0.0209,
      "step": 298
    },
    {
      "epoch": 0.453030303030303,
      "grad_norm": 0.44622018933296204,
      "learning_rate": 0.00011053435114503819,
      "loss": 0.1523,
      "step": 299
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.29354003071784973,
      "learning_rate": 0.00011022900763358779,
      "loss": 0.0799,
      "step": 300
    },
    {
      "epoch": 0.45606060606060606,
      "grad_norm": 0.7501465082168579,
      "learning_rate": 0.00010992366412213742,
      "loss": 0.2021,
      "step": 301
    },
    {
      "epoch": 0.4575757575757576,
      "grad_norm": 0.6178967952728271,
      "learning_rate": 0.00010961832061068703,
      "loss": 0.2007,
      "step": 302
    },
    {
      "epoch": 0.4590909090909091,
      "grad_norm": 0.24147437512874603,
      "learning_rate": 0.00010931297709923664,
      "loss": 0.0217,
      "step": 303
    },
    {
      "epoch": 0.46060606060606063,
      "grad_norm": 0.6599210500717163,
      "learning_rate": 0.00010900763358778628,
      "loss": 0.1702,
      "step": 304
    },
    {
      "epoch": 0.4621212121212121,
      "grad_norm": 0.1359911561012268,
      "learning_rate": 0.00010870229007633588,
      "loss": 0.015,
      "step": 305
    },
    {
      "epoch": 0.4636363636363636,
      "grad_norm": 0.5936852693557739,
      "learning_rate": 0.0001083969465648855,
      "loss": 0.2458,
      "step": 306
    },
    {
      "epoch": 0.46515151515151515,
      "grad_norm": 0.5329695343971252,
      "learning_rate": 0.0001080916030534351,
      "loss": 0.1638,
      "step": 307
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.687938392162323,
      "learning_rate": 0.00010778625954198473,
      "loss": 0.1679,
      "step": 308
    },
    {
      "epoch": 0.4681818181818182,
      "grad_norm": 0.42970937490463257,
      "learning_rate": 0.00010748091603053437,
      "loss": 0.0941,
      "step": 309
    },
    {
      "epoch": 0.4696969696969697,
      "grad_norm": 0.46797847747802734,
      "learning_rate": 0.00010717557251908397,
      "loss": 0.1333,
      "step": 310
    },
    {
      "epoch": 0.4712121212121212,
      "grad_norm": 0.560409426689148,
      "learning_rate": 0.00010687022900763359,
      "loss": 0.0435,
      "step": 311
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.12511923909187317,
      "learning_rate": 0.00010656488549618322,
      "loss": 0.0119,
      "step": 312
    },
    {
      "epoch": 0.47424242424242424,
      "grad_norm": 0.7218064069747925,
      "learning_rate": 0.00010625954198473282,
      "loss": 0.2011,
      "step": 313
    },
    {
      "epoch": 0.47575757575757577,
      "grad_norm": 0.31591692566871643,
      "learning_rate": 0.00010595419847328246,
      "loss": 0.0809,
      "step": 314
    },
    {
      "epoch": 0.4772727272727273,
      "grad_norm": 0.2339143604040146,
      "learning_rate": 0.00010564885496183206,
      "loss": 0.0215,
      "step": 315
    },
    {
      "epoch": 0.47878787878787876,
      "grad_norm": 0.4534263610839844,
      "learning_rate": 0.00010534351145038168,
      "loss": 0.0375,
      "step": 316
    },
    {
      "epoch": 0.4803030303030303,
      "grad_norm": 1.1245373487472534,
      "learning_rate": 0.00010503816793893131,
      "loss": 0.3696,
      "step": 317
    },
    {
      "epoch": 0.4818181818181818,
      "grad_norm": 0.8137890696525574,
      "learning_rate": 0.00010473282442748091,
      "loss": 0.2281,
      "step": 318
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.2837136685848236,
      "learning_rate": 0.00010442748091603054,
      "loss": 0.0215,
      "step": 319
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.9590227007865906,
      "learning_rate": 0.00010412213740458016,
      "loss": 0.3859,
      "step": 320
    },
    {
      "epoch": 0.4863636363636364,
      "grad_norm": 103.9735107421875,
      "learning_rate": 0.00010381679389312977,
      "loss": -0.0258,
      "step": 321
    },
    {
      "epoch": 0.48787878787878786,
      "grad_norm": 0.25780007243156433,
      "learning_rate": 0.0001035114503816794,
      "loss": 0.0305,
      "step": 322
    },
    {
      "epoch": 0.4893939393939394,
      "grad_norm": 0.9980632066726685,
      "learning_rate": 0.000103206106870229,
      "loss": 0.2005,
      "step": 323
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.3227614760398865,
      "learning_rate": 0.00010290076335877863,
      "loss": 0.1123,
      "step": 324
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 0.4781341850757599,
      "learning_rate": 0.00010259541984732825,
      "loss": 0.0917,
      "step": 325
    },
    {
      "epoch": 0.49393939393939396,
      "grad_norm": 0.08144758641719818,
      "learning_rate": 0.00010229007633587786,
      "loss": 0.0088,
      "step": 326
    },
    {
      "epoch": 0.4954545454545455,
      "grad_norm": 0.46939870715141296,
      "learning_rate": 0.00010198473282442749,
      "loss": 0.1237,
      "step": 327
    },
    {
      "epoch": 0.49696969696969695,
      "grad_norm": 0.8600637912750244,
      "learning_rate": 0.00010167938931297712,
      "loss": 0.2465,
      "step": 328
    },
    {
      "epoch": 0.4984848484848485,
      "grad_norm": 0.3148422837257385,
      "learning_rate": 0.00010137404580152672,
      "loss": 0.0721,
      "step": 329
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.25206950306892395,
      "learning_rate": 0.00010106870229007634,
      "loss": 0.027,
      "step": 330
    },
    {
      "epoch": 0.5015151515151515,
      "grad_norm": 0.1992357224225998,
      "learning_rate": 0.00010076335877862595,
      "loss": 0.0234,
      "step": 331
    },
    {
      "epoch": 0.503030303030303,
      "grad_norm": 0.41223618388175964,
      "learning_rate": 0.00010045801526717558,
      "loss": 0.0772,
      "step": 332
    },
    {
      "epoch": 0.5045454545454545,
      "grad_norm": 0.6631112694740295,
      "learning_rate": 0.0001001526717557252,
      "loss": 0.0909,
      "step": 333
    },
    {
      "epoch": 0.5060606060606061,
      "grad_norm": 0.4301077723503113,
      "learning_rate": 9.984732824427481e-05,
      "loss": 0.0827,
      "step": 334
    },
    {
      "epoch": 0.5075757575757576,
      "grad_norm": 0.4906691312789917,
      "learning_rate": 9.954198473282443e-05,
      "loss": 0.1538,
      "step": 335
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.6331457495689392,
      "learning_rate": 9.923664122137405e-05,
      "loss": 0.1627,
      "step": 336
    },
    {
      "epoch": 0.5106060606060606,
      "grad_norm": 0.4983973503112793,
      "learning_rate": 9.893129770992367e-05,
      "loss": 0.1159,
      "step": 337
    },
    {
      "epoch": 0.5121212121212121,
      "grad_norm": 0.07117606699466705,
      "learning_rate": 9.862595419847329e-05,
      "loss": 0.0073,
      "step": 338
    },
    {
      "epoch": 0.5136363636363637,
      "grad_norm": 0.4932186007499695,
      "learning_rate": 9.83206106870229e-05,
      "loss": 0.1334,
      "step": 339
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.565961480140686,
      "learning_rate": 9.801526717557252e-05,
      "loss": 0.1701,
      "step": 340
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 18.35906982421875,
      "learning_rate": 9.770992366412214e-05,
      "loss": 0.3443,
      "step": 341
    },
    {
      "epoch": 0.5181818181818182,
      "grad_norm": 1.017071008682251,
      "learning_rate": 9.740458015267177e-05,
      "loss": 0.3033,
      "step": 342
    },
    {
      "epoch": 0.5196969696969697,
      "grad_norm": 0.21044988930225372,
      "learning_rate": 9.709923664122138e-05,
      "loss": 0.0177,
      "step": 343
    },
    {
      "epoch": 0.5212121212121212,
      "grad_norm": 9.644491195678711,
      "learning_rate": 9.679389312977099e-05,
      "loss": 0.0821,
      "step": 344
    },
    {
      "epoch": 0.5227272727272727,
      "grad_norm": 0.294768750667572,
      "learning_rate": 9.648854961832061e-05,
      "loss": 0.0806,
      "step": 345
    },
    {
      "epoch": 0.5242424242424243,
      "grad_norm": 0.28054049611091614,
      "learning_rate": 9.618320610687024e-05,
      "loss": 0.0938,
      "step": 346
    },
    {
      "epoch": 0.5257575757575758,
      "grad_norm": 0.29460957646369934,
      "learning_rate": 9.587786259541986e-05,
      "loss": 0.0445,
      "step": 347
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.15986071527004242,
      "learning_rate": 9.557251908396946e-05,
      "loss": 0.046,
      "step": 348
    },
    {
      "epoch": 0.5287878787878788,
      "grad_norm": 0.2835052013397217,
      "learning_rate": 9.526717557251908e-05,
      "loss": 0.035,
      "step": 349
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 0.47588101029396057,
      "learning_rate": 9.496183206106871e-05,
      "loss": 0.0883,
      "step": 350
    },
    {
      "epoch": 0.5318181818181819,
      "grad_norm": 0.6221034526824951,
      "learning_rate": 9.465648854961833e-05,
      "loss": 0.1444,
      "step": 351
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.1965126395225525,
      "learning_rate": 9.435114503816794e-05,
      "loss": 0.0243,
      "step": 352
    },
    {
      "epoch": 0.5348484848484848,
      "grad_norm": 0.7882241606712341,
      "learning_rate": 9.404580152671755e-05,
      "loss": 0.3507,
      "step": 353
    },
    {
      "epoch": 0.5363636363636364,
      "grad_norm": 0.6600844860076904,
      "learning_rate": 9.374045801526719e-05,
      "loss": 0.2301,
      "step": 354
    },
    {
      "epoch": 0.5378787878787878,
      "grad_norm": 0.3996831476688385,
      "learning_rate": 9.34351145038168e-05,
      "loss": 0.1032,
      "step": 355
    },
    {
      "epoch": 0.5393939393939394,
      "grad_norm": 0.8001863956451416,
      "learning_rate": 9.312977099236642e-05,
      "loss": 0.236,
      "step": 356
    },
    {
      "epoch": 0.5409090909090909,
      "grad_norm": 0.2334163635969162,
      "learning_rate": 9.282442748091603e-05,
      "loss": 0.0193,
      "step": 357
    },
    {
      "epoch": 0.5424242424242425,
      "grad_norm": 0.3583855628967285,
      "learning_rate": 9.251908396946566e-05,
      "loss": 0.1628,
      "step": 358
    },
    {
      "epoch": 0.543939393939394,
      "grad_norm": 0.3671588897705078,
      "learning_rate": 9.221374045801528e-05,
      "loss": 0.0829,
      "step": 359
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.6936058402061462,
      "learning_rate": 9.19083969465649e-05,
      "loss": 0.1937,
      "step": 360
    },
    {
      "epoch": 0.546969696969697,
      "grad_norm": 0.6662580966949463,
      "learning_rate": 9.160305343511451e-05,
      "loss": 0.1086,
      "step": 361
    },
    {
      "epoch": 0.5484848484848485,
      "grad_norm": 0.5697131752967834,
      "learning_rate": 9.129770992366413e-05,
      "loss": 0.0251,
      "step": 362
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.5774601697921753,
      "learning_rate": 9.099236641221375e-05,
      "loss": 0.3357,
      "step": 363
    },
    {
      "epoch": 0.5515151515151515,
      "grad_norm": 0.11025600880384445,
      "learning_rate": 9.068702290076337e-05,
      "loss": 0.0121,
      "step": 364
    },
    {
      "epoch": 0.553030303030303,
      "grad_norm": 0.38858070969581604,
      "learning_rate": 9.038167938931298e-05,
      "loss": 0.1313,
      "step": 365
    },
    {
      "epoch": 0.5545454545454546,
      "grad_norm": 0.18349872529506683,
      "learning_rate": 9.007633587786259e-05,
      "loss": 0.0506,
      "step": 366
    },
    {
      "epoch": 0.556060606060606,
      "grad_norm": 0.2594941556453705,
      "learning_rate": 8.977099236641222e-05,
      "loss": 0.0189,
      "step": 367
    },
    {
      "epoch": 0.5575757575757576,
      "grad_norm": 0.7107045650482178,
      "learning_rate": 8.946564885496184e-05,
      "loss": 0.1326,
      "step": 368
    },
    {
      "epoch": 0.5590909090909091,
      "grad_norm": 0.24419991672039032,
      "learning_rate": 8.916030534351145e-05,
      "loss": 0.0585,
      "step": 369
    },
    {
      "epoch": 0.5606060606060606,
      "grad_norm": 0.44588515162467957,
      "learning_rate": 8.885496183206107e-05,
      "loss": 0.1158,
      "step": 370
    },
    {
      "epoch": 0.5621212121212121,
      "grad_norm": 37.5907096862793,
      "learning_rate": 8.854961832061069e-05,
      "loss": 0.1594,
      "step": 371
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.5935845971107483,
      "learning_rate": 8.824427480916031e-05,
      "loss": 0.2374,
      "step": 372
    },
    {
      "epoch": 0.5651515151515152,
      "grad_norm": 0.6491841673851013,
      "learning_rate": 8.793893129770993e-05,
      "loss": 0.0644,
      "step": 373
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.40021026134490967,
      "learning_rate": 8.763358778625954e-05,
      "loss": 0.0497,
      "step": 374
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 0.3930496573448181,
      "learning_rate": 8.732824427480916e-05,
      "loss": 0.1026,
      "step": 375
    },
    {
      "epoch": 0.5696969696969697,
      "grad_norm": 0.3283165395259857,
      "learning_rate": 8.702290076335878e-05,
      "loss": 0.0775,
      "step": 376
    },
    {
      "epoch": 0.5712121212121212,
      "grad_norm": 0.09363902360200882,
      "learning_rate": 8.67175572519084e-05,
      "loss": 0.0109,
      "step": 377
    },
    {
      "epoch": 0.5727272727272728,
      "grad_norm": 0.6546093225479126,
      "learning_rate": 8.641221374045802e-05,
      "loss": 0.1738,
      "step": 378
    },
    {
      "epoch": 0.5742424242424242,
      "grad_norm": 0.3956509530544281,
      "learning_rate": 8.610687022900765e-05,
      "loss": 0.129,
      "step": 379
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.08740123361349106,
      "learning_rate": 8.580152671755725e-05,
      "loss": 0.0094,
      "step": 380
    },
    {
      "epoch": 0.5772727272727273,
      "grad_norm": 0.22065697610378265,
      "learning_rate": 8.549618320610687e-05,
      "loss": 0.0399,
      "step": 381
    },
    {
      "epoch": 0.5787878787878787,
      "grad_norm": 0.5361762046813965,
      "learning_rate": 8.519083969465649e-05,
      "loss": 0.1509,
      "step": 382
    },
    {
      "epoch": 0.5803030303030303,
      "grad_norm": 0.22811703383922577,
      "learning_rate": 8.488549618320612e-05,
      "loss": 0.0313,
      "step": 383
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.7079847455024719,
      "learning_rate": 8.458015267175572e-05,
      "loss": 0.2035,
      "step": 384
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 1.2084381580352783,
      "learning_rate": 8.427480916030534e-05,
      "loss": 0.3717,
      "step": 385
    },
    {
      "epoch": 0.5848484848484848,
      "grad_norm": 0.9669377207756042,
      "learning_rate": 8.396946564885496e-05,
      "loss": 0.4859,
      "step": 386
    },
    {
      "epoch": 0.5863636363636363,
      "grad_norm": 0.7562860250473022,
      "learning_rate": 8.366412213740459e-05,
      "loss": 0.1203,
      "step": 387
    },
    {
      "epoch": 0.5878787878787879,
      "grad_norm": 0.1771032065153122,
      "learning_rate": 8.335877862595421e-05,
      "loss": 0.0173,
      "step": 388
    },
    {
      "epoch": 0.5893939393939394,
      "grad_norm": 0.1635914295911789,
      "learning_rate": 8.305343511450381e-05,
      "loss": 0.016,
      "step": 389
    },
    {
      "epoch": 0.5909090909090909,
      "grad_norm": 0.7287520170211792,
      "learning_rate": 8.274809160305343e-05,
      "loss": 0.2459,
      "step": 390
    },
    {
      "epoch": 0.5924242424242424,
      "grad_norm": 0.9820735454559326,
      "learning_rate": 8.244274809160306e-05,
      "loss": 0.1765,
      "step": 391
    },
    {
      "epoch": 0.593939393939394,
      "grad_norm": 0.31729042530059814,
      "learning_rate": 8.213740458015268e-05,
      "loss": 0.0291,
      "step": 392
    },
    {
      "epoch": 0.5954545454545455,
      "grad_norm": 0.23191602528095245,
      "learning_rate": 8.18320610687023e-05,
      "loss": 0.035,
      "step": 393
    },
    {
      "epoch": 0.5969696969696969,
      "grad_norm": 0.82634037733078,
      "learning_rate": 8.15267175572519e-05,
      "loss": 0.334,
      "step": 394
    },
    {
      "epoch": 0.5984848484848485,
      "grad_norm": 0.0957590639591217,
      "learning_rate": 8.122137404580153e-05,
      "loss": 0.0103,
      "step": 395
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.4906759262084961,
      "learning_rate": 8.091603053435115e-05,
      "loss": 0.1353,
      "step": 396
    },
    {
      "epoch": 0.6015151515151516,
      "grad_norm": 0.3916061520576477,
      "learning_rate": 8.061068702290077e-05,
      "loss": 0.1061,
      "step": 397
    },
    {
      "epoch": 0.603030303030303,
      "grad_norm": 34.35593032836914,
      "learning_rate": 8.030534351145038e-05,
      "loss": 0.1342,
      "step": 398
    },
    {
      "epoch": 0.6045454545454545,
      "grad_norm": 0.18605650961399078,
      "learning_rate": 8e-05,
      "loss": 0.0339,
      "step": 399
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.5618500113487244,
      "learning_rate": 7.969465648854962e-05,
      "loss": 0.2201,
      "step": 400
    },
    {
      "epoch": 0.6075757575757575,
      "grad_norm": 0.14014814794063568,
      "learning_rate": 7.938931297709924e-05,
      "loss": 0.0121,
      "step": 401
    },
    {
      "epoch": 0.6090909090909091,
      "grad_norm": 0.0975859984755516,
      "learning_rate": 7.908396946564886e-05,
      "loss": 0.0109,
      "step": 402
    },
    {
      "epoch": 0.6106060606060606,
      "grad_norm": 0.8291078805923462,
      "learning_rate": 7.877862595419848e-05,
      "loss": 0.3182,
      "step": 403
    },
    {
      "epoch": 0.6121212121212121,
      "grad_norm": 0.12399886548519135,
      "learning_rate": 7.84732824427481e-05,
      "loss": 0.0121,
      "step": 404
    },
    {
      "epoch": 0.6136363636363636,
      "grad_norm": 0.610844612121582,
      "learning_rate": 7.816793893129771e-05,
      "loss": 0.1923,
      "step": 405
    },
    {
      "epoch": 0.6151515151515151,
      "grad_norm": 0.27602094411849976,
      "learning_rate": 7.786259541984733e-05,
      "loss": 0.0691,
      "step": 406
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.42980167269706726,
      "learning_rate": 7.755725190839695e-05,
      "loss": 0.1192,
      "step": 407
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.36755409836769104,
      "learning_rate": 7.725190839694657e-05,
      "loss": 0.087,
      "step": 408
    },
    {
      "epoch": 0.6196969696969697,
      "grad_norm": 0.6726722717285156,
      "learning_rate": 7.694656488549619e-05,
      "loss": 0.1872,
      "step": 409
    },
    {
      "epoch": 0.6212121212121212,
      "grad_norm": 0.06793967634439468,
      "learning_rate": 7.66412213740458e-05,
      "loss": 0.0072,
      "step": 410
    },
    {
      "epoch": 0.6227272727272727,
      "grad_norm": 0.8173485994338989,
      "learning_rate": 7.633587786259542e-05,
      "loss": 0.2275,
      "step": 411
    },
    {
      "epoch": 0.6242424242424243,
      "grad_norm": 0.43060243129730225,
      "learning_rate": 7.603053435114504e-05,
      "loss": 0.0903,
      "step": 412
    },
    {
      "epoch": 0.6257575757575757,
      "grad_norm": 0.42261943221092224,
      "learning_rate": 7.572519083969466e-05,
      "loss": 0.0738,
      "step": 413
    },
    {
      "epoch": 0.6272727272727273,
      "grad_norm": 0.3569929003715515,
      "learning_rate": 7.541984732824428e-05,
      "loss": 0.05,
      "step": 414
    },
    {
      "epoch": 0.6287878787878788,
      "grad_norm": 0.5259832739830017,
      "learning_rate": 7.511450381679391e-05,
      "loss": 0.1304,
      "step": 415
    },
    {
      "epoch": 0.6303030303030303,
      "grad_norm": 0.38992252945899963,
      "learning_rate": 7.480916030534351e-05,
      "loss": 0.0755,
      "step": 416
    },
    {
      "epoch": 0.6318181818181818,
      "grad_norm": 0.28872382640838623,
      "learning_rate": 7.450381679389313e-05,
      "loss": 0.029,
      "step": 417
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.29947513341903687,
      "learning_rate": 7.419847328244275e-05,
      "loss": 0.0482,
      "step": 418
    },
    {
      "epoch": 0.6348484848484849,
      "grad_norm": 0.36838456988334656,
      "learning_rate": 7.389312977099238e-05,
      "loss": 0.0328,
      "step": 419
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.31441399455070496,
      "learning_rate": 7.3587786259542e-05,
      "loss": 0.0756,
      "step": 420
    },
    {
      "epoch": 0.6378787878787879,
      "grad_norm": 464.401123046875,
      "learning_rate": 7.32824427480916e-05,
      "loss": -0.1314,
      "step": 421
    },
    {
      "epoch": 0.6393939393939394,
      "grad_norm": 17.481847763061523,
      "learning_rate": 7.297709923664122e-05,
      "loss": 0.0811,
      "step": 422
    },
    {
      "epoch": 0.6409090909090909,
      "grad_norm": 5.700222492218018,
      "learning_rate": 7.267175572519084e-05,
      "loss": 0.0605,
      "step": 423
    },
    {
      "epoch": 0.6424242424242425,
      "grad_norm": 0.42807555198669434,
      "learning_rate": 7.236641221374047e-05,
      "loss": 0.1355,
      "step": 424
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 0.9974828362464905,
      "learning_rate": 7.206106870229009e-05,
      "loss": 0.4378,
      "step": 425
    },
    {
      "epoch": 0.6454545454545455,
      "grad_norm": 0.18712088465690613,
      "learning_rate": 7.175572519083969e-05,
      "loss": 0.0426,
      "step": 426
    },
    {
      "epoch": 0.646969696969697,
      "grad_norm": 0.5028564929962158,
      "learning_rate": 7.145038167938931e-05,
      "loss": 0.0783,
      "step": 427
    },
    {
      "epoch": 0.6484848484848484,
      "grad_norm": 0.590790331363678,
      "learning_rate": 7.114503816793894e-05,
      "loss": 0.2448,
      "step": 428
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.312304824590683,
      "learning_rate": 7.083969465648856e-05,
      "loss": 0.0387,
      "step": 429
    },
    {
      "epoch": 0.6515151515151515,
      "grad_norm": 0.46110889315605164,
      "learning_rate": 7.053435114503816e-05,
      "loss": 0.1313,
      "step": 430
    },
    {
      "epoch": 0.6530303030303031,
      "grad_norm": 0.6100068092346191,
      "learning_rate": 7.022900763358778e-05,
      "loss": 0.1446,
      "step": 431
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.8509824872016907,
      "learning_rate": 6.992366412213741e-05,
      "loss": 0.1416,
      "step": 432
    },
    {
      "epoch": 0.656060606060606,
      "grad_norm": 0.37784186005592346,
      "learning_rate": 6.961832061068703e-05,
      "loss": 0.0786,
      "step": 433
    },
    {
      "epoch": 0.6575757575757576,
      "grad_norm": 0.6143186092376709,
      "learning_rate": 6.931297709923665e-05,
      "loss": 0.1077,
      "step": 434
    },
    {
      "epoch": 0.6590909090909091,
      "grad_norm": 0.4661424160003662,
      "learning_rate": 6.900763358778625e-05,
      "loss": 0.178,
      "step": 435
    },
    {
      "epoch": 0.6606060606060606,
      "grad_norm": 0.6751303672790527,
      "learning_rate": 6.870229007633588e-05,
      "loss": 0.1188,
      "step": 436
    },
    {
      "epoch": 0.6621212121212121,
      "grad_norm": 0.4215624928474426,
      "learning_rate": 6.83969465648855e-05,
      "loss": 0.1757,
      "step": 437
    },
    {
      "epoch": 0.6636363636363637,
      "grad_norm": 0.5211179256439209,
      "learning_rate": 6.809160305343512e-05,
      "loss": 0.1224,
      "step": 438
    },
    {
      "epoch": 0.6651515151515152,
      "grad_norm": 0.23250852525234222,
      "learning_rate": 6.778625954198474e-05,
      "loss": 0.0334,
      "step": 439
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.37111470103263855,
      "learning_rate": 6.748091603053436e-05,
      "loss": 0.0814,
      "step": 440
    },
    {
      "epoch": 0.6681818181818182,
      "grad_norm": 0.2656708359718323,
      "learning_rate": 6.717557251908397e-05,
      "loss": 0.1035,
      "step": 441
    },
    {
      "epoch": 0.6696969696969697,
      "grad_norm": 7.827458381652832,
      "learning_rate": 6.687022900763359e-05,
      "loss": -0.0016,
      "step": 442
    },
    {
      "epoch": 0.6712121212121213,
      "grad_norm": 0.29900696873664856,
      "learning_rate": 6.656488549618321e-05,
      "loss": 0.0599,
      "step": 443
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.5141376256942749,
      "learning_rate": 6.625954198473283e-05,
      "loss": 0.1619,
      "step": 444
    },
    {
      "epoch": 0.6742424242424242,
      "grad_norm": 0.06487928330898285,
      "learning_rate": 6.595419847328245e-05,
      "loss": 0.0061,
      "step": 445
    },
    {
      "epoch": 0.6757575757575758,
      "grad_norm": 0.25517383217811584,
      "learning_rate": 6.564885496183206e-05,
      "loss": 0.0964,
      "step": 446
    },
    {
      "epoch": 0.6772727272727272,
      "grad_norm": 0.29134514927864075,
      "learning_rate": 6.534351145038168e-05,
      "loss": 0.054,
      "step": 447
    },
    {
      "epoch": 0.6787878787878788,
      "grad_norm": 0.553036093711853,
      "learning_rate": 6.50381679389313e-05,
      "loss": 0.2155,
      "step": 448
    },
    {
      "epoch": 0.6803030303030303,
      "grad_norm": 0.3375648260116577,
      "learning_rate": 6.473282442748092e-05,
      "loss": 0.1129,
      "step": 449
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 0.6446625590324402,
      "learning_rate": 6.442748091603053e-05,
      "loss": 0.2323,
      "step": 450
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.0775652527809143,
      "learning_rate": 6.412213740458015e-05,
      "loss": 0.0091,
      "step": 451
    },
    {
      "epoch": 0.6848484848484848,
      "grad_norm": 0.1746111959218979,
      "learning_rate": 6.381679389312978e-05,
      "loss": 0.0207,
      "step": 452
    },
    {
      "epoch": 0.6863636363636364,
      "grad_norm": 0.20147404074668884,
      "learning_rate": 6.351145038167939e-05,
      "loss": 0.0243,
      "step": 453
    },
    {
      "epoch": 0.6878787878787879,
      "grad_norm": 0.597322940826416,
      "learning_rate": 6.3206106870229e-05,
      "loss": 0.1797,
      "step": 454
    },
    {
      "epoch": 0.6893939393939394,
      "grad_norm": 0.16109345853328705,
      "learning_rate": 6.290076335877862e-05,
      "loss": 0.0444,
      "step": 455
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.29320284724235535,
      "learning_rate": 6.259541984732826e-05,
      "loss": 0.0824,
      "step": 456
    },
    {
      "epoch": 0.6924242424242424,
      "grad_norm": 0.5384578704833984,
      "learning_rate": 6.229007633587787e-05,
      "loss": 0.1706,
      "step": 457
    },
    {
      "epoch": 0.693939393939394,
      "grad_norm": 0.09517376124858856,
      "learning_rate": 6.198473282442748e-05,
      "loss": 0.0103,
      "step": 458
    },
    {
      "epoch": 0.6954545454545454,
      "grad_norm": 0.48709923028945923,
      "learning_rate": 6.16793893129771e-05,
      "loss": 0.0974,
      "step": 459
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.7459394335746765,
      "learning_rate": 6.137404580152673e-05,
      "loss": 0.1272,
      "step": 460
    },
    {
      "epoch": 0.6984848484848485,
      "grad_norm": 0.4759877920150757,
      "learning_rate": 6.106870229007635e-05,
      "loss": 0.0796,
      "step": 461
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.6097705960273743,
      "learning_rate": 6.076335877862596e-05,
      "loss": 0.1623,
      "step": 462
    },
    {
      "epoch": 0.7015151515151515,
      "grad_norm": 0.4216431677341461,
      "learning_rate": 6.0458015267175575e-05,
      "loss": 0.1335,
      "step": 463
    },
    {
      "epoch": 0.703030303030303,
      "grad_norm": 0.4752521216869354,
      "learning_rate": 6.01526717557252e-05,
      "loss": 0.0828,
      "step": 464
    },
    {
      "epoch": 0.7045454545454546,
      "grad_norm": 0.726081371307373,
      "learning_rate": 5.984732824427482e-05,
      "loss": 0.2059,
      "step": 465
    },
    {
      "epoch": 0.706060606060606,
      "grad_norm": 0.27332615852355957,
      "learning_rate": 5.954198473282443e-05,
      "loss": 0.0488,
      "step": 466
    },
    {
      "epoch": 0.7075757575757575,
      "grad_norm": 0.3995267152786255,
      "learning_rate": 5.9236641221374046e-05,
      "loss": 0.0701,
      "step": 467
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.5720160603523254,
      "learning_rate": 5.893129770992367e-05,
      "loss": 0.1526,
      "step": 468
    },
    {
      "epoch": 0.7106060606060606,
      "grad_norm": 71.28150177001953,
      "learning_rate": 5.862595419847329e-05,
      "loss": 0.0126,
      "step": 469
    },
    {
      "epoch": 0.7121212121212122,
      "grad_norm": 0.2431587129831314,
      "learning_rate": 5.83206106870229e-05,
      "loss": 0.0242,
      "step": 470
    },
    {
      "epoch": 0.7136363636363636,
      "grad_norm": 0.7026090621948242,
      "learning_rate": 5.801526717557252e-05,
      "loss": 0.113,
      "step": 471
    },
    {
      "epoch": 0.7151515151515152,
      "grad_norm": 0.12963803112506866,
      "learning_rate": 5.770992366412214e-05,
      "loss": 0.0138,
      "step": 472
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.30763527750968933,
      "learning_rate": 5.740458015267176e-05,
      "loss": 0.0375,
      "step": 473
    },
    {
      "epoch": 0.7181818181818181,
      "grad_norm": 0.6752219796180725,
      "learning_rate": 5.709923664122138e-05,
      "loss": 0.2295,
      "step": 474
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 0.28200653195381165,
      "learning_rate": 5.679389312977099e-05,
      "loss": 0.029,
      "step": 475
    },
    {
      "epoch": 0.7212121212121212,
      "grad_norm": 0.5974808931350708,
      "learning_rate": 5.648854961832062e-05,
      "loss": 0.1067,
      "step": 476
    },
    {
      "epoch": 0.7227272727272728,
      "grad_norm": 0.4067476987838745,
      "learning_rate": 5.618320610687023e-05,
      "loss": 0.1195,
      "step": 477
    },
    {
      "epoch": 0.7242424242424242,
      "grad_norm": 0.3459715247154236,
      "learning_rate": 5.587786259541985e-05,
      "loss": 0.0889,
      "step": 478
    },
    {
      "epoch": 0.7257575757575757,
      "grad_norm": 0.4877287447452545,
      "learning_rate": 5.557251908396947e-05,
      "loss": 0.1294,
      "step": 479
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.3787226378917694,
      "learning_rate": 5.526717557251909e-05,
      "loss": 0.0628,
      "step": 480
    },
    {
      "epoch": 0.7287878787878788,
      "grad_norm": 0.3033280372619629,
      "learning_rate": 5.496183206106871e-05,
      "loss": 0.0314,
      "step": 481
    },
    {
      "epoch": 0.7303030303030303,
      "grad_norm": 0.598263680934906,
      "learning_rate": 5.465648854961832e-05,
      "loss": 0.1441,
      "step": 482
    },
    {
      "epoch": 0.7318181818181818,
      "grad_norm": 0.5130140781402588,
      "learning_rate": 5.435114503816794e-05,
      "loss": 0.1057,
      "step": 483
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.7649098634719849,
      "learning_rate": 5.404580152671755e-05,
      "loss": 0.1514,
      "step": 484
    },
    {
      "epoch": 0.7348484848484849,
      "grad_norm": 0.34565576910972595,
      "learning_rate": 5.374045801526718e-05,
      "loss": 0.1105,
      "step": 485
    },
    {
      "epoch": 0.7363636363636363,
      "grad_norm": 0.49451568722724915,
      "learning_rate": 5.3435114503816794e-05,
      "loss": 0.0519,
      "step": 486
    },
    {
      "epoch": 0.7378787878787879,
      "grad_norm": 0.6773707866668701,
      "learning_rate": 5.312977099236641e-05,
      "loss": 0.126,
      "step": 487
    },
    {
      "epoch": 0.7393939393939394,
      "grad_norm": 0.26742616295814514,
      "learning_rate": 5.282442748091603e-05,
      "loss": 0.037,
      "step": 488
    },
    {
      "epoch": 0.740909090909091,
      "grad_norm": 0.24062716960906982,
      "learning_rate": 5.2519083969465654e-05,
      "loss": 0.0223,
      "step": 489
    },
    {
      "epoch": 0.7424242424242424,
      "grad_norm": 0.7344539165496826,
      "learning_rate": 5.221374045801527e-05,
      "loss": 0.2888,
      "step": 490
    },
    {
      "epoch": 0.7439393939393939,
      "grad_norm": 0.8177419304847717,
      "learning_rate": 5.1908396946564884e-05,
      "loss": 0.1673,
      "step": 491
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.6031894087791443,
      "learning_rate": 5.16030534351145e-05,
      "loss": 0.1324,
      "step": 492
    },
    {
      "epoch": 0.746969696969697,
      "grad_norm": 0.08576486259698868,
      "learning_rate": 5.1297709923664126e-05,
      "loss": 0.0073,
      "step": 493
    },
    {
      "epoch": 0.7484848484848485,
      "grad_norm": 0.9570363759994507,
      "learning_rate": 5.0992366412213744e-05,
      "loss": 0.3348,
      "step": 494
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.17959940433502197,
      "learning_rate": 5.068702290076336e-05,
      "loss": 0.0445,
      "step": 495
    },
    {
      "epoch": 0.7515151515151515,
      "grad_norm": 0.0326959602534771,
      "learning_rate": 5.038167938931297e-05,
      "loss": 0.0038,
      "step": 496
    },
    {
      "epoch": 0.753030303030303,
      "grad_norm": 0.40622231364250183,
      "learning_rate": 5.00763358778626e-05,
      "loss": 0.0756,
      "step": 497
    },
    {
      "epoch": 0.7545454545454545,
      "grad_norm": 1.0938122272491455,
      "learning_rate": 4.9770992366412216e-05,
      "loss": 0.2685,
      "step": 498
    },
    {
      "epoch": 0.7560606060606061,
      "grad_norm": 0.3422105610370636,
      "learning_rate": 4.9465648854961834e-05,
      "loss": 0.0214,
      "step": 499
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.20904725790023804,
      "learning_rate": 4.916030534351145e-05,
      "loss": 0.0258,
      "step": 500
    },
    {
      "epoch": 0.759090909090909,
      "grad_norm": 0.4783310890197754,
      "learning_rate": 4.885496183206107e-05,
      "loss": 0.1413,
      "step": 501
    },
    {
      "epoch": 0.7606060606060606,
      "grad_norm": 0.30531924962997437,
      "learning_rate": 4.854961832061069e-05,
      "loss": 0.0426,
      "step": 502
    },
    {
      "epoch": 0.7621212121212121,
      "grad_norm": 0.4981398284435272,
      "learning_rate": 4.8244274809160306e-05,
      "loss": 0.1327,
      "step": 503
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.7495597004890442,
      "learning_rate": 4.793893129770993e-05,
      "loss": 0.1551,
      "step": 504
    },
    {
      "epoch": 0.7651515151515151,
      "grad_norm": 0.4072515368461609,
      "learning_rate": 4.763358778625954e-05,
      "loss": 0.0945,
      "step": 505
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.5132972598075867,
      "learning_rate": 4.7328244274809166e-05,
      "loss": 0.0645,
      "step": 506
    },
    {
      "epoch": 0.7681818181818182,
      "grad_norm": 0.21957281231880188,
      "learning_rate": 4.702290076335878e-05,
      "loss": 0.018,
      "step": 507
    },
    {
      "epoch": 0.7696969696969697,
      "grad_norm": 0.1639454960823059,
      "learning_rate": 4.67175572519084e-05,
      "loss": 0.0354,
      "step": 508
    },
    {
      "epoch": 0.7712121212121212,
      "grad_norm": 0.20670610666275024,
      "learning_rate": 4.641221374045801e-05,
      "loss": 0.0371,
      "step": 509
    },
    {
      "epoch": 0.7727272727272727,
      "grad_norm": 0.3101746737957001,
      "learning_rate": 4.610687022900764e-05,
      "loss": 0.104,
      "step": 510
    },
    {
      "epoch": 0.7742424242424243,
      "grad_norm": 0.4307829737663269,
      "learning_rate": 4.5801526717557256e-05,
      "loss": 0.0898,
      "step": 511
    },
    {
      "epoch": 0.7757575757575758,
      "grad_norm": 1.328255295753479,
      "learning_rate": 4.5496183206106874e-05,
      "loss": 0.2388,
      "step": 512
    },
    {
      "epoch": 0.7772727272727272,
      "grad_norm": 0.44761577248573303,
      "learning_rate": 4.519083969465649e-05,
      "loss": 0.2009,
      "step": 513
    },
    {
      "epoch": 0.7787878787878788,
      "grad_norm": 0.3322591185569763,
      "learning_rate": 4.488549618320611e-05,
      "loss": 0.0381,
      "step": 514
    },
    {
      "epoch": 0.7803030303030303,
      "grad_norm": 0.966162383556366,
      "learning_rate": 4.458015267175573e-05,
      "loss": 0.2581,
      "step": 515
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.5420047640800476,
      "learning_rate": 4.4274809160305345e-05,
      "loss": 0.1193,
      "step": 516
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.1828266680240631,
      "learning_rate": 4.396946564885496e-05,
      "loss": 0.0125,
      "step": 517
    },
    {
      "epoch": 0.7848484848484848,
      "grad_norm": 0.7119349837303162,
      "learning_rate": 4.366412213740458e-05,
      "loss": 0.0777,
      "step": 518
    },
    {
      "epoch": 0.7863636363636364,
      "grad_norm": 0.4171217978000641,
      "learning_rate": 4.33587786259542e-05,
      "loss": 0.0515,
      "step": 519
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.5430949330329895,
      "learning_rate": 4.3053435114503824e-05,
      "loss": 0.1189,
      "step": 520
    },
    {
      "epoch": 0.7893939393939394,
      "grad_norm": 0.08525571972131729,
      "learning_rate": 4.2748091603053435e-05,
      "loss": 0.0094,
      "step": 521
    },
    {
      "epoch": 0.7909090909090909,
      "grad_norm": 0.28233638405799866,
      "learning_rate": 4.244274809160306e-05,
      "loss": 0.0402,
      "step": 522
    },
    {
      "epoch": 0.7924242424242425,
      "grad_norm": 0.2154582440853119,
      "learning_rate": 4.213740458015267e-05,
      "loss": 0.0191,
      "step": 523
    },
    {
      "epoch": 0.793939393939394,
      "grad_norm": 0.4767914116382599,
      "learning_rate": 4.1832061068702296e-05,
      "loss": 0.0986,
      "step": 524
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 44.84900665283203,
      "learning_rate": 4.152671755725191e-05,
      "loss": 0.1113,
      "step": 525
    },
    {
      "epoch": 0.796969696969697,
      "grad_norm": 0.6310067772865295,
      "learning_rate": 4.122137404580153e-05,
      "loss": 0.0817,
      "step": 526
    },
    {
      "epoch": 0.7984848484848485,
      "grad_norm": 0.8620818853378296,
      "learning_rate": 4.091603053435115e-05,
      "loss": 0.2358,
      "step": 527
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1259767860174179,
      "learning_rate": 4.061068702290077e-05,
      "loss": 0.0132,
      "step": 528
    },
    {
      "epoch": 0.8015151515151515,
      "grad_norm": 0.9302865266799927,
      "learning_rate": 4.0305343511450385e-05,
      "loss": 0.2173,
      "step": 529
    },
    {
      "epoch": 0.803030303030303,
      "grad_norm": 0.1577412486076355,
      "learning_rate": 4e-05,
      "loss": 0.0132,
      "step": 530
    },
    {
      "epoch": 0.8045454545454546,
      "grad_norm": 0.6629482507705688,
      "learning_rate": 3.969465648854962e-05,
      "loss": 0.0743,
      "step": 531
    },
    {
      "epoch": 0.806060606060606,
      "grad_norm": 0.23105843365192413,
      "learning_rate": 3.938931297709924e-05,
      "loss": 0.0208,
      "step": 532
    },
    {
      "epoch": 0.8075757575757576,
      "grad_norm": 0.5786416530609131,
      "learning_rate": 3.908396946564886e-05,
      "loss": 0.127,
      "step": 533
    },
    {
      "epoch": 0.8090909090909091,
      "grad_norm": 0.13615210354328156,
      "learning_rate": 3.8778625954198475e-05,
      "loss": 0.0105,
      "step": 534
    },
    {
      "epoch": 0.8106060606060606,
      "grad_norm": 0.47267672419548035,
      "learning_rate": 3.847328244274809e-05,
      "loss": 0.0769,
      "step": 535
    },
    {
      "epoch": 0.8121212121212121,
      "grad_norm": 0.7434735298156738,
      "learning_rate": 3.816793893129771e-05,
      "loss": 0.2849,
      "step": 536
    },
    {
      "epoch": 0.8136363636363636,
      "grad_norm": 0.8743471503257751,
      "learning_rate": 3.786259541984733e-05,
      "loss": 0.1232,
      "step": 537
    },
    {
      "epoch": 0.8151515151515152,
      "grad_norm": 0.34027597308158875,
      "learning_rate": 3.7557251908396954e-05,
      "loss": 0.0715,
      "step": 538
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 1.1835805177688599,
      "learning_rate": 3.7251908396946565e-05,
      "loss": 0.3352,
      "step": 539
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.148158997297287,
      "learning_rate": 3.694656488549619e-05,
      "loss": 0.02,
      "step": 540
    },
    {
      "epoch": 0.8196969696969697,
      "grad_norm": 0.5740606188774109,
      "learning_rate": 3.66412213740458e-05,
      "loss": 0.1365,
      "step": 541
    },
    {
      "epoch": 0.8212121212121212,
      "grad_norm": 0.771653950214386,
      "learning_rate": 3.633587786259542e-05,
      "loss": 0.205,
      "step": 542
    },
    {
      "epoch": 0.8227272727272728,
      "grad_norm": 0.40429002046585083,
      "learning_rate": 3.603053435114504e-05,
      "loss": 0.0456,
      "step": 543
    },
    {
      "epoch": 0.8242424242424242,
      "grad_norm": 0.03314452990889549,
      "learning_rate": 3.5725190839694654e-05,
      "loss": 0.0043,
      "step": 544
    },
    {
      "epoch": 0.8257575757575758,
      "grad_norm": 0.2826584577560425,
      "learning_rate": 3.541984732824428e-05,
      "loss": 0.054,
      "step": 545
    },
    {
      "epoch": 0.8272727272727273,
      "grad_norm": 0.4170972406864166,
      "learning_rate": 3.511450381679389e-05,
      "loss": 0.1336,
      "step": 546
    },
    {
      "epoch": 0.8287878787878787,
      "grad_norm": 0.37230247259140015,
      "learning_rate": 3.4809160305343515e-05,
      "loss": 0.0479,
      "step": 547
    },
    {
      "epoch": 0.8303030303030303,
      "grad_norm": 0.5882379412651062,
      "learning_rate": 3.4503816793893126e-05,
      "loss": 0.1575,
      "step": 548
    },
    {
      "epoch": 0.8318181818181818,
      "grad_norm": 0.5658047795295715,
      "learning_rate": 3.419847328244275e-05,
      "loss": 0.1297,
      "step": 549
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.46895831823349,
      "learning_rate": 3.389312977099237e-05,
      "loss": 0.0832,
      "step": 550
    },
    {
      "epoch": 0.8348484848484848,
      "grad_norm": 0.42032915353775024,
      "learning_rate": 3.358778625954199e-05,
      "loss": 0.1127,
      "step": 551
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.17098839581012726,
      "learning_rate": 3.3282442748091605e-05,
      "loss": 0.017,
      "step": 552
    },
    {
      "epoch": 0.8378787878787879,
      "grad_norm": 0.7480338215827942,
      "learning_rate": 3.297709923664122e-05,
      "loss": 0.215,
      "step": 553
    },
    {
      "epoch": 0.8393939393939394,
      "grad_norm": 0.10553053766489029,
      "learning_rate": 3.267175572519084e-05,
      "loss": 0.0104,
      "step": 554
    },
    {
      "epoch": 0.8409090909090909,
      "grad_norm": 0.39767885208129883,
      "learning_rate": 3.236641221374046e-05,
      "loss": 0.0527,
      "step": 555
    },
    {
      "epoch": 0.8424242424242424,
      "grad_norm": 0.30235105752944946,
      "learning_rate": 3.2061068702290076e-05,
      "loss": 0.0482,
      "step": 556
    },
    {
      "epoch": 0.843939393939394,
      "grad_norm": 0.2582724690437317,
      "learning_rate": 3.1755725190839694e-05,
      "loss": 0.0165,
      "step": 557
    },
    {
      "epoch": 0.8454545454545455,
      "grad_norm": 0.3546486496925354,
      "learning_rate": 3.145038167938931e-05,
      "loss": 0.0992,
      "step": 558
    },
    {
      "epoch": 0.8469696969696969,
      "grad_norm": 0.36780449748039246,
      "learning_rate": 3.114503816793894e-05,
      "loss": 0.0895,
      "step": 559
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.12257436662912369,
      "learning_rate": 3.083969465648855e-05,
      "loss": 0.0113,
      "step": 560
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.39411109685897827,
      "learning_rate": 3.053435114503817e-05,
      "loss": 0.0903,
      "step": 561
    },
    {
      "epoch": 0.8515151515151516,
      "grad_norm": 0.5908203721046448,
      "learning_rate": 3.0229007633587787e-05,
      "loss": 0.0917,
      "step": 562
    },
    {
      "epoch": 0.853030303030303,
      "grad_norm": 12.099297523498535,
      "learning_rate": 2.992366412213741e-05,
      "loss": 0.0547,
      "step": 563
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.48664921522140503,
      "learning_rate": 2.9618320610687023e-05,
      "loss": 0.2207,
      "step": 564
    },
    {
      "epoch": 0.8560606060606061,
      "grad_norm": 0.7089791297912598,
      "learning_rate": 2.9312977099236644e-05,
      "loss": 0.2042,
      "step": 565
    },
    {
      "epoch": 0.8575757575757575,
      "grad_norm": 509.351806640625,
      "learning_rate": 2.900763358778626e-05,
      "loss": 0.0353,
      "step": 566
    },
    {
      "epoch": 0.8590909090909091,
      "grad_norm": 0.2884448766708374,
      "learning_rate": 2.870229007633588e-05,
      "loss": 0.068,
      "step": 567
    },
    {
      "epoch": 0.8606060606060606,
      "grad_norm": 0.2756645381450653,
      "learning_rate": 2.8396946564885495e-05,
      "loss": 0.0301,
      "step": 568
    },
    {
      "epoch": 0.8621212121212121,
      "grad_norm": 0.40816524624824524,
      "learning_rate": 2.8091603053435116e-05,
      "loss": 0.0561,
      "step": 569
    },
    {
      "epoch": 0.8636363636363636,
      "grad_norm": 0.2979455590248108,
      "learning_rate": 2.7786259541984734e-05,
      "loss": 0.1297,
      "step": 570
    },
    {
      "epoch": 0.8651515151515151,
      "grad_norm": 0.11172470450401306,
      "learning_rate": 2.7480916030534355e-05,
      "loss": 0.0132,
      "step": 571
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.8269058465957642,
      "learning_rate": 2.717557251908397e-05,
      "loss": 0.2711,
      "step": 572
    },
    {
      "epoch": 0.8681818181818182,
      "grad_norm": 0.2836683988571167,
      "learning_rate": 2.687022900763359e-05,
      "loss": 0.0378,
      "step": 573
    },
    {
      "epoch": 0.8696969696969697,
      "grad_norm": 0.17733192443847656,
      "learning_rate": 2.6564885496183206e-05,
      "loss": 0.0151,
      "step": 574
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 0.21439829468727112,
      "learning_rate": 2.6259541984732827e-05,
      "loss": 0.0153,
      "step": 575
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.49366357922554016,
      "learning_rate": 2.5954198473282442e-05,
      "loss": 0.1591,
      "step": 576
    },
    {
      "epoch": 0.8742424242424243,
      "grad_norm": 0.3093149960041046,
      "learning_rate": 2.5648854961832063e-05,
      "loss": 0.0451,
      "step": 577
    },
    {
      "epoch": 0.8757575757575757,
      "grad_norm": 0.6096674203872681,
      "learning_rate": 2.534351145038168e-05,
      "loss": 0.1228,
      "step": 578
    },
    {
      "epoch": 0.8772727272727273,
      "grad_norm": 0.21203021705150604,
      "learning_rate": 2.50381679389313e-05,
      "loss": 0.0564,
      "step": 579
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.3211614191532135,
      "learning_rate": 2.4732824427480917e-05,
      "loss": 0.0934,
      "step": 580
    },
    {
      "epoch": 0.8803030303030303,
      "grad_norm": 0.4193664491176605,
      "learning_rate": 2.4427480916030535e-05,
      "loss": 0.0955,
      "step": 581
    },
    {
      "epoch": 0.8818181818181818,
      "grad_norm": 12.370789527893066,
      "learning_rate": 2.4122137404580153e-05,
      "loss": 0.0621,
      "step": 582
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.09654518216848373,
      "learning_rate": 2.381679389312977e-05,
      "loss": 0.009,
      "step": 583
    },
    {
      "epoch": 0.8848484848484849,
      "grad_norm": 0.2948465943336487,
      "learning_rate": 2.351145038167939e-05,
      "loss": 0.0586,
      "step": 584
    },
    {
      "epoch": 0.8863636363636364,
      "grad_norm": 0.3208974599838257,
      "learning_rate": 2.3206106870229007e-05,
      "loss": 0.1091,
      "step": 585
    },
    {
      "epoch": 0.8878787878787879,
      "grad_norm": 0.4010107219219208,
      "learning_rate": 2.2900763358778628e-05,
      "loss": 0.1024,
      "step": 586
    },
    {
      "epoch": 0.8893939393939394,
      "grad_norm": 0.5544306635856628,
      "learning_rate": 2.2595419847328246e-05,
      "loss": 0.0785,
      "step": 587
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.08786910027265549,
      "learning_rate": 2.2290076335877864e-05,
      "loss": 0.0091,
      "step": 588
    },
    {
      "epoch": 0.8924242424242425,
      "grad_norm": 0.830164909362793,
      "learning_rate": 2.198473282442748e-05,
      "loss": 0.2884,
      "step": 589
    },
    {
      "epoch": 0.8939393939393939,
      "grad_norm": 0.3533829152584076,
      "learning_rate": 2.16793893129771e-05,
      "loss": 0.0629,
      "step": 590
    },
    {
      "epoch": 0.8954545454545455,
      "grad_norm": 0.4127090573310852,
      "learning_rate": 2.1374045801526718e-05,
      "loss": 0.1597,
      "step": 591
    },
    {
      "epoch": 0.896969696969697,
      "grad_norm": 0.2541947364807129,
      "learning_rate": 2.1068702290076335e-05,
      "loss": 0.0362,
      "step": 592
    },
    {
      "epoch": 0.8984848484848484,
      "grad_norm": 0.2795379161834717,
      "learning_rate": 2.0763358778625953e-05,
      "loss": 0.0473,
      "step": 593
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.5235795974731445,
      "learning_rate": 2.0458015267175575e-05,
      "loss": 0.1602,
      "step": 594
    },
    {
      "epoch": 0.9015151515151515,
      "grad_norm": 0.50551438331604,
      "learning_rate": 2.0152671755725193e-05,
      "loss": 0.0871,
      "step": 595
    },
    {
      "epoch": 0.9030303030303031,
      "grad_norm": 0.26475027203559875,
      "learning_rate": 1.984732824427481e-05,
      "loss": 0.0599,
      "step": 596
    },
    {
      "epoch": 0.9045454545454545,
      "grad_norm": 0.43259352445602417,
      "learning_rate": 1.954198473282443e-05,
      "loss": 0.1155,
      "step": 597
    },
    {
      "epoch": 0.906060606060606,
      "grad_norm": 0.6345124244689941,
      "learning_rate": 1.9236641221374046e-05,
      "loss": 0.1577,
      "step": 598
    },
    {
      "epoch": 0.9075757575757576,
      "grad_norm": 0.3123522102832794,
      "learning_rate": 1.8931297709923664e-05,
      "loss": 0.1567,
      "step": 599
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.9353688359260559,
      "learning_rate": 1.8625954198473282e-05,
      "loss": 0.1828,
      "step": 600
    },
    {
      "epoch": 0.9106060606060606,
      "grad_norm": 0.3578297793865204,
      "learning_rate": 1.83206106870229e-05,
      "loss": 0.0708,
      "step": 601
    },
    {
      "epoch": 0.9121212121212121,
      "grad_norm": 0.21012195944786072,
      "learning_rate": 1.801526717557252e-05,
      "loss": 0.0586,
      "step": 602
    },
    {
      "epoch": 0.9136363636363637,
      "grad_norm": 0.42484182119369507,
      "learning_rate": 1.770992366412214e-05,
      "loss": 0.1224,
      "step": 603
    },
    {
      "epoch": 0.9151515151515152,
      "grad_norm": 0.3605823516845703,
      "learning_rate": 1.7404580152671757e-05,
      "loss": 0.0349,
      "step": 604
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.4697563052177429,
      "learning_rate": 1.7099236641221375e-05,
      "loss": 0.0615,
      "step": 605
    },
    {
      "epoch": 0.9181818181818182,
      "grad_norm": 0.5086492300033569,
      "learning_rate": 1.6793893129770993e-05,
      "loss": 0.0528,
      "step": 606
    },
    {
      "epoch": 0.9196969696969697,
      "grad_norm": 0.21373805403709412,
      "learning_rate": 1.648854961832061e-05,
      "loss": 0.0167,
      "step": 607
    },
    {
      "epoch": 0.9212121212121213,
      "grad_norm": 0.2875567376613617,
      "learning_rate": 1.618320610687023e-05,
      "loss": 0.0223,
      "step": 608
    },
    {
      "epoch": 0.9227272727272727,
      "grad_norm": 0.41473570466041565,
      "learning_rate": 1.5877862595419847e-05,
      "loss": 0.1056,
      "step": 609
    },
    {
      "epoch": 0.9242424242424242,
      "grad_norm": 0.5305079221725464,
      "learning_rate": 1.557251908396947e-05,
      "loss": 0.1711,
      "step": 610
    },
    {
      "epoch": 0.9257575757575758,
      "grad_norm": 631.2720336914062,
      "learning_rate": 1.5267175572519086e-05,
      "loss": -0.0543,
      "step": 611
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.5567845702171326,
      "learning_rate": 1.4961832061068704e-05,
      "loss": 0.165,
      "step": 612
    },
    {
      "epoch": 0.9287878787878788,
      "grad_norm": 0.3838441073894501,
      "learning_rate": 1.4656488549618322e-05,
      "loss": 0.0919,
      "step": 613
    },
    {
      "epoch": 0.9303030303030303,
      "grad_norm": 0.4319283962249756,
      "learning_rate": 1.435114503816794e-05,
      "loss": 0.0498,
      "step": 614
    },
    {
      "epoch": 0.9318181818181818,
      "grad_norm": 0.44305920600891113,
      "learning_rate": 1.4045801526717558e-05,
      "loss": 0.1245,
      "step": 615
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.6385934948921204,
      "learning_rate": 1.3740458015267178e-05,
      "loss": 0.2251,
      "step": 616
    },
    {
      "epoch": 0.9348484848484848,
      "grad_norm": 0.6884233951568604,
      "learning_rate": 1.3435114503816796e-05,
      "loss": 0.236,
      "step": 617
    },
    {
      "epoch": 0.9363636363636364,
      "grad_norm": 0.2890419065952301,
      "learning_rate": 1.3129770992366414e-05,
      "loss": 0.0339,
      "step": 618
    },
    {
      "epoch": 0.9378787878787879,
      "grad_norm": 0.4148971736431122,
      "learning_rate": 1.2824427480916032e-05,
      "loss": 0.0897,
      "step": 619
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.6878038048744202,
      "learning_rate": 1.251908396946565e-05,
      "loss": 0.1715,
      "step": 620
    },
    {
      "epoch": 0.9409090909090909,
      "grad_norm": 0.7301506400108337,
      "learning_rate": 1.2213740458015267e-05,
      "loss": 0.1677,
      "step": 621
    },
    {
      "epoch": 0.9424242424242424,
      "grad_norm": 0.5425914525985718,
      "learning_rate": 1.1908396946564885e-05,
      "loss": 0.1033,
      "step": 622
    },
    {
      "epoch": 0.943939393939394,
      "grad_norm": 0.169601172208786,
      "learning_rate": 1.1603053435114503e-05,
      "loss": 0.0268,
      "step": 623
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.39364656805992126,
      "learning_rate": 1.1297709923664123e-05,
      "loss": 0.0677,
      "step": 624
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 0.11837919801473618,
      "learning_rate": 1.099236641221374e-05,
      "loss": 0.0127,
      "step": 625
    },
    {
      "epoch": 0.9484848484848485,
      "grad_norm": 28.656816482543945,
      "learning_rate": 1.0687022900763359e-05,
      "loss": 0.0488,
      "step": 626
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.3554198741912842,
      "learning_rate": 1.0381679389312977e-05,
      "loss": 0.0427,
      "step": 627
    },
    {
      "epoch": 0.9515151515151515,
      "grad_norm": 0.7709974050521851,
      "learning_rate": 1.0076335877862596e-05,
      "loss": 0.2007,
      "step": 628
    },
    {
      "epoch": 0.953030303030303,
      "grad_norm": 0.7375274300575256,
      "learning_rate": 9.770992366412214e-06,
      "loss": 0.1831,
      "step": 629
    },
    {
      "epoch": 0.9545454545454546,
      "grad_norm": 0.13713973760604858,
      "learning_rate": 9.465648854961832e-06,
      "loss": 0.0151,
      "step": 630
    },
    {
      "epoch": 0.956060606060606,
      "grad_norm": 0.5221155285835266,
      "learning_rate": 9.16030534351145e-06,
      "loss": 0.0663,
      "step": 631
    },
    {
      "epoch": 0.9575757575757575,
      "grad_norm": 0.3027355670928955,
      "learning_rate": 8.85496183206107e-06,
      "loss": 0.0772,
      "step": 632
    },
    {
      "epoch": 0.9590909090909091,
      "grad_norm": 0.49717074632644653,
      "learning_rate": 8.549618320610688e-06,
      "loss": 0.0981,
      "step": 633
    },
    {
      "epoch": 0.9606060606060606,
      "grad_norm": 0.745517909526825,
      "learning_rate": 8.244274809160306e-06,
      "loss": 0.1573,
      "step": 634
    },
    {
      "epoch": 0.9621212121212122,
      "grad_norm": 0.17943909764289856,
      "learning_rate": 7.938931297709924e-06,
      "loss": 0.0151,
      "step": 635
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.7276730537414551,
      "learning_rate": 7.633587786259543e-06,
      "loss": 0.343,
      "step": 636
    },
    {
      "epoch": 0.9651515151515152,
      "grad_norm": 0.5699266791343689,
      "learning_rate": 7.328244274809161e-06,
      "loss": 0.0775,
      "step": 637
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.5289966464042664,
      "learning_rate": 7.022900763358779e-06,
      "loss": 0.1042,
      "step": 638
    },
    {
      "epoch": 0.9681818181818181,
      "grad_norm": 0.45286232233047485,
      "learning_rate": 6.717557251908398e-06,
      "loss": 0.0459,
      "step": 639
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.0859019011259079,
      "learning_rate": 6.412213740458016e-06,
      "loss": 0.0087,
      "step": 640
    },
    {
      "epoch": 0.9712121212121212,
      "grad_norm": 0.2888311743736267,
      "learning_rate": 6.106870229007634e-06,
      "loss": 0.0254,
      "step": 641
    },
    {
      "epoch": 0.9727272727272728,
      "grad_norm": 0.4059027433395386,
      "learning_rate": 5.801526717557252e-06,
      "loss": 0.072,
      "step": 642
    },
    {
      "epoch": 0.9742424242424242,
      "grad_norm": 0.5246990919113159,
      "learning_rate": 5.49618320610687e-06,
      "loss": 0.1279,
      "step": 643
    },
    {
      "epoch": 0.9757575757575757,
      "grad_norm": 0.8900821208953857,
      "learning_rate": 5.190839694656488e-06,
      "loss": 0.2113,
      "step": 644
    },
    {
      "epoch": 0.9772727272727273,
      "grad_norm": 0.23258955776691437,
      "learning_rate": 4.885496183206107e-06,
      "loss": 0.0251,
      "step": 645
    },
    {
      "epoch": 0.9787878787878788,
      "grad_norm": 0.7693197727203369,
      "learning_rate": 4.580152671755725e-06,
      "loss": 0.1258,
      "step": 646
    },
    {
      "epoch": 0.9803030303030303,
      "grad_norm": 0.5217005014419556,
      "learning_rate": 4.274809160305344e-06,
      "loss": 0.0892,
      "step": 647
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.7686299085617065,
      "learning_rate": 3.969465648854962e-06,
      "loss": 0.1496,
      "step": 648
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.12884902954101562,
      "learning_rate": 3.6641221374045806e-06,
      "loss": 0.0125,
      "step": 649
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 0.3736135959625244,
      "learning_rate": 3.358778625954199e-06,
      "loss": 0.123,
      "step": 650
    },
    {
      "epoch": 0.9863636363636363,
      "grad_norm": 0.34919577836990356,
      "learning_rate": 3.053435114503817e-06,
      "loss": 0.0702,
      "step": 651
    },
    {
      "epoch": 0.9878787878787879,
      "grad_norm": 0.3168410062789917,
      "learning_rate": 2.748091603053435e-06,
      "loss": 0.0377,
      "step": 652
    },
    {
      "epoch": 0.9893939393939394,
      "grad_norm": 0.37953171133995056,
      "learning_rate": 2.4427480916030536e-06,
      "loss": 0.0475,
      "step": 653
    },
    {
      "epoch": 0.990909090909091,
      "grad_norm": 0.7691612839698792,
      "learning_rate": 2.137404580152672e-06,
      "loss": 0.3365,
      "step": 654
    },
    {
      "epoch": 0.9924242424242424,
      "grad_norm": 0.34311535954475403,
      "learning_rate": 1.8320610687022903e-06,
      "loss": 0.054,
      "step": 655
    },
    {
      "epoch": 0.9939393939393939,
      "grad_norm": 0.12323601543903351,
      "learning_rate": 1.5267175572519084e-06,
      "loss": 0.0127,
      "step": 656
    },
    {
      "epoch": 0.9954545454545455,
      "grad_norm": 0.615180492401123,
      "learning_rate": 1.2213740458015268e-06,
      "loss": 0.1434,
      "step": 657
    },
    {
      "epoch": 0.996969696969697,
      "grad_norm": 0.11521672457456589,
      "learning_rate": 9.160305343511451e-07,
      "loss": 0.0129,
      "step": 658
    },
    {
      "epoch": 0.9984848484848485,
      "grad_norm": 0.4465996026992798,
      "learning_rate": 6.106870229007634e-07,
      "loss": 0.0685,
      "step": 659
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2402370572090149,
      "learning_rate": 3.053435114503817e-07,
      "loss": 0.0579,
      "step": 660
    }
  ],
  "logging_steps": 1,
  "max_steps": 660,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2454795022421197e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
