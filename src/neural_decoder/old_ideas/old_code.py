'''
                        MEMO
        #else:
        #    x = self.to_patch_embedding(neuralInput)
        #    # for memo TTA
        #    if n_masks > 1:
        #        x_masked = []
        #        for _ in range(n_masks):
        #            xtemp, _ = self.apply_specaugment_mask(x, X_len)
        #            x_masked.append(xtemp)
        #            
        #        x = torch.stack(x_masked).squeeze()
        #        
        #    else:
        #        x, mask = self.apply_specaugment_mask(x, X_len)
        #        
        #    x = self.patch_to_emb(x)
        #    
        #else:
        #    
         #   x = self.to_patch_embedding(neuralInput)
         
'''