{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/ebrahim/miniconda3/envs/speech-bci/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from edit_distance import SequenceMatcher\n",
    "import torch\n",
    "from dataset import SpeechDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.model import GRUDecoder\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.lm_utils import build_llama_1B\n",
    "from neural_decoder.model import GRUDecoder\n",
    "from neural_decoder.bit import BiT_Phoneme\n",
    "import pickle\n",
    "import argparse\n",
    "from lm_utils import _cer_and_wer\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence(s):\n",
    "    s = s.lower()\n",
    "    charMarks = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "                 \"'\", ' ']\n",
    "    ans = []\n",
    "    for i in s:\n",
    "        if(i in charMarks):\n",
    "            ans.append(i)\n",
    "    \n",
    "    return ''.join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING IN N-GRAM MODE\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home3/skaasyap/willett\"\n",
    "\n",
    "load_lm = True\n",
    "# LM decoding hyperparameters\n",
    "acoustic_scale = 0.8\n",
    "blank_penalty = np.log(2)\n",
    "\n",
    "run_for_llm = False\n",
    "\n",
    "if run_for_llm:\n",
    "    return_n_best = True\n",
    "    rescore = False\n",
    "    nbest = 100\n",
    "    print(\"RUNNING IN LLM MODE\")\n",
    "else:\n",
    "    return_n_best = False\n",
    "    rescore = False\n",
    "    nbest = 1\n",
    "    print(\"RUNNING IN N-GRAM MODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded LM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0728 20:11:06.811215 2117809 brain_speech_decoder.h:52] Reading fst /home3/skaasyap/willett/lm/languageModel/TLG.fst\n",
      "I0728 20:14:10.093220 2117809 brain_speech_decoder.h:81] Reading symbol table /home3/skaasyap/willett/lm/languageModel/words.txt\n"
     ]
    }
   ],
   "source": [
    "if load_lm and 'ngramDecoder' not in globals(): \n",
    "        \n",
    "    lmDir = base_dir +'/lm/languageModel'\n",
    "    ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "        lmDir,\n",
    "        acoustic_scale=acoustic_scale, #1.2\n",
    "        nbest=nbest,\n",
    "        beam=18\n",
    "    )\n",
    "    print(\"loaded LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path stuff for when running models on two servers (obiwan or leia)\n",
    "# model storage locations and data path within model args differ depending on \n",
    "# the server. \n",
    "current_server = 'leia'\n",
    "server_models_ran_on = 'obi'\n",
    "\n",
    "if current_server == 'obi':\n",
    "    \n",
    "    base_path = '/data/willett_data/'\n",
    "    \n",
    "    if server_models_ran_on == 'obi':\n",
    "        model_storage_path = '/data/willett_data/outputs/'\n",
    "        \n",
    "    elif server_models_ran_on == 'leia':\n",
    "        model_storage_path = '/data/willett_data/leia_outputs/'\n",
    "\n",
    "if current_server == 'leia':\n",
    "    \n",
    "    base_path = '/home3/ebrahim/save_transcripts/'\n",
    "    \n",
    "    if server_models_ran_on == 'obi':\n",
    "        model_storage_path = '/home3/skaasyap/willett/obi_models/outputs/'\n",
    "    \n",
    "    elif server_models_ran_on == 'leia':\n",
    "        model_storage_path = '/home3/skaasyap/willett/outputs/'\n",
    "        \n",
    "leia_to_obi_dict = {'/home3/skaasyap/willett/data_log_both':'/data/willett_data/ptDecoder_ctc_both', \n",
    "                    '/home3/skaasyap/willett/data':'/data/willett_data/ptDecoder_ctc',\n",
    "                    '/home3/skaasyap/willett/data_log_both_held_out_days': '/data/willett_data/ptDecoder_ctc_both_held_out_days', \n",
    "                    '/home3/skaasyap/willett/data_log_both_held_out_days_1': '/data/willett_data/ptDecoder_ctc_both_held_out_days_1',\n",
    "                    '/home3/skaasyap/willett/data_log_both_held_out_days_2': '/data/willett_data/ptDecoder_ctc_both_held_out_days_2'}\n",
    "\n",
    "obi_to_leia_dict = {v:k for k, v in leia_to_obi_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: neurips_gru_data_log_time_masked_lr_schedule_seed_0\n",
      "DATA FILE:  /home3/skaasyap/willett/data_log_both\n",
      "Loading GRU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/ebrahim/miniconda3/envs/speech-bci/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home3/ebrahim/neural_seq_decoder/src/neural_decoder/augmentations.py:170: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return self.conv(input, weight=self.weight, groups=self.groups, padding=\"same\")\n",
      "/tmp/ipykernel_2117809/3127162098.py:229: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER DAY:  0.3053691275167785\n",
      "CER DAY:  0.21379310344827587\n",
      "CER DAY:  0.19488817891373802\n",
      "CER DAY:  0.27472527472527475\n",
      "CER DAY:  0.11569506726457399\n",
      "CER DAY:  0.12052730696798493\n",
      "CER DAY:  0.12771996215704826\n",
      "CER DAY:  0.17557932263814616\n",
      "CER DAY:  0.14183266932270916\n",
      "CER DAY:  0.18333333333333332\n",
      "CER DAY:  0.17672790901137359\n",
      "CER DAY:  0.20128676470588236\n",
      "CER DAY:  0.17565055762081785\n",
      "CER DAY:  0.13137032842582105\n",
      "CER DAY:  0.1159678858162355\n",
      "CER DAY:  0.11732851985559567\n",
      "CER DAY:  0.14074074074074075\n",
      "CER DAY:  0.17769002961500494\n",
      "CER DAY:  0.15077605321507762\n",
      "CER DAY:  0.12043435340572557\n",
      "CER DAY:  0.10753768844221105\n",
      "CER DAY:  0.15182755388940955\n",
      "CER DAY:  0.15013901760889714\n",
      "CER DAY:  0.178125\n",
      "Model performance:  0.16103338698361602\n",
      "Running n-gram LM\n",
      "Average decoding time per sample: 0.033 seconds\n",
      "Standard deviation: 0.044 seconds\n",
      "CER and WER after 3-gram LM:  (0.11767696945483601, 0.10853764289293968, 0.12687042276706295) (0.17366793962538643, 0.1602196028287572, 0.18729585524432793)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:2\"\n",
    "\n",
    "models_to_run = ['neurips_gru_data_log_time_masked_lr_schedule']\n",
    "\n",
    "shared_output_file = ''\n",
    "\n",
    "if len(shared_output_file) > 0:\n",
    "    print(\"Writing to shared output file\")\n",
    "    write_mode = \"a\"\n",
    "else:\n",
    "    write_mode = \"w\"\n",
    "    \n",
    "seeds_list = [0]\n",
    "partition = \"test\" # \"test\"\n",
    "run_lm = True\n",
    "\n",
    "comp_on_reduced = False # if running restricted days analyses\n",
    "fill_max_day = False\n",
    "skip_days = [[]]\n",
    "\n",
    "day_edit_distance = 0\n",
    "day_seq_length = 0\n",
    "prev_day = None\n",
    "\n",
    "if partition == 'test':\n",
    "    saveFolder_transcripts = f\"{base_path}model_transcriptions/\"\n",
    "else:\n",
    "    saveFolder_transcripts = f\"{base_path}model_transcriptions_retest/\"\n",
    "    \n",
    "day_cer_dict = {}\n",
    "total_wer_dict = {}\n",
    "\n",
    "for seed in seeds_list:\n",
    "    \n",
    "    day_cer_dict[seed] = []\n",
    "    total_wer_dict[seed] = []\n",
    "            \n",
    "    for mn, model_name_str in enumerate(models_to_run):\n",
    "        \n",
    "        effective_length_arr = []\n",
    "     \n",
    "        modelPath = f\"{model_storage_path}{model_name_str}_seed_{seed}\"\n",
    "        \n",
    "        if len(shared_output_file) > 0:\n",
    "            output_file = f\"{shared_output_file}_seed_{seed}\"\n",
    "            print(output_file)\n",
    "        else:\n",
    "            output_file = f\"{model_name_str}_seed_{seed}\"\n",
    "            \n",
    "        print(f\"Running model: {model_name_str}_seed_{seed}\")\n",
    "            \n",
    "        with open(modelPath + \"/args\", \"rb\") as handle:\n",
    "            args = pickle.load(handle)\n",
    "            \n",
    "        # modify data file path based on which server model was ran on \n",
    "        # and which server is currently being used to run this notebok.\n",
    "        if server_models_ran_on != current_server:\n",
    "            \n",
    "            if current_server == 'leia':\n",
    "                data_file = obi_to_leia_dict[args['datasetPath']]\n",
    "                \n",
    "            elif current_server == 'obi':\n",
    "                data_file = leia_to_obi_dict[args['datasetPath']]\n",
    "                \n",
    "        else:\n",
    "\n",
    "            data_file = args['datasetPath']\n",
    "            \n",
    "        print(\"DATA FILE: \", data_file)\n",
    "      \n",
    "        if 'ventral_6v_only' not in args:\n",
    "            args['ventral_6v_only'] = False\n",
    "            \n",
    "        trainLoaders, testLoaders, loadedData = getDatasetLoaders(\n",
    "            data_file, 8, args['restricted_days'], \n",
    "            args['ventral_6v_only']\n",
    "        )\n",
    "        \n",
    "        # if true, model is a GRU\n",
    "        if 'nInputFeatures' in args.keys():\n",
    "            \n",
    "            if 'max_mask_pct' not in args:\n",
    "                args['max_mask_pct'] = 0\n",
    "            if 'num_masks' not in args:\n",
    "                args['num_masks'] = 0\n",
    "            if 'input_dropout' not in args:\n",
    "                args['input_dropout'] = 0\n",
    "            if 'linderman_lab' not in args:\n",
    "                args['linderman_lab'] = False\n",
    "                \n",
    "            print(\"Loading GRU\")\n",
    "            model = GRUDecoder(\n",
    "                neural_dim=args[\"nInputFeatures\"],\n",
    "                n_classes=args[\"nClasses\"],\n",
    "                hidden_dim=args[\"nUnits\"],\n",
    "                layer_dim=args[\"nLayers\"],\n",
    "                nDays=args['nDays'],\n",
    "                dropout=args[\"dropout\"],\n",
    "                device=device,\n",
    "                strideLen=args[\"strideLen\"],\n",
    "                kernelLen=args[\"kernelLen\"],\n",
    "                gaussianSmoothWidth=args[\"gaussianSmoothWidth\"],\n",
    "                bidirectional=args[\"bidirectional\"],\n",
    "                input_dropout=args['input_dropout'], \n",
    "                max_mask_pct=args['max_mask_pct'],\n",
    "                num_masks=args['num_masks'], \n",
    "                linderman_lab=args['linderman_lab']\n",
    "            ).to(device)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if 'mask_token_zero' not in args:\n",
    "                args['mask_token_zero'] = False\n",
    "                            \n",
    "            # Instantiate model\n",
    "            # set training relevant parameters for MEMO, doesn't matter for other runs because they are \n",
    "            # only run in eval mode.\n",
    "            model = BiT_Phoneme(\n",
    "                patch_size=args['patch_size'],\n",
    "                dim=args['dim'],\n",
    "                dim_head=args['dim_head'],\n",
    "                nClasses=args['nClasses'],\n",
    "                depth=args['depth'],\n",
    "                heads=args['heads'],\n",
    "                mlp_dim_ratio=args['mlp_dim_ratio'],\n",
    "                dropout=0,\n",
    "                input_dropout=0,\n",
    "                gaussianSmoothWidth=args['gaussianSmoothWidth'],\n",
    "                T5_style_pos=args['T5_style_pos'],\n",
    "                max_mask_pct=0.0,\n",
    "                num_masks=0, \n",
    "                mask_token_zeros=args['mask_token_zero'], \n",
    "                num_masks_channels=0, \n",
    "                max_mask_channels=0, \n",
    "                dist_dict_path=0\n",
    "            ).to(device)\n",
    "            \n",
    "            \n",
    "        ckpt_path = modelPath + '/modelWeights'\n",
    "        model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        model_outputs = {\n",
    "            \"logits\": [],\n",
    "            \"logitLengths\": [],\n",
    "            \"trueSeqs\": [],\n",
    "            \"transcriptions\": [],\n",
    "        }\n",
    "        \n",
    "        total_edit_distance = 0\n",
    "        total_seq_length = 0\n",
    "\n",
    "        if partition == \"competition\":\n",
    "        \n",
    "            testDayIdxs = [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20]\n",
    "                \n",
    "        elif partition == \"test\":\n",
    "            \n",
    "            testDayIdxs = range(len(loadedData[partition])) \n",
    "            \n",
    "        ground_truth_sentences = []\n",
    "        \n",
    "        #print(\"RESTRICTED DAYS: \", args['restricted_days'])\n",
    "        \n",
    "        for i, testDayIdx in enumerate(testDayIdxs):\n",
    "            \n",
    "            \n",
    "            day_outputs = {\n",
    "                \"logits\": [],\n",
    "                \"logitLengths\": [],\n",
    "                \"trueSeqs\": [],\n",
    "                \"transcriptions\": [],\n",
    "            }\n",
    "            \n",
    "            #if len(skip_days[mn]) > 0:\n",
    "            #    if testDayIdx in skip_days[mn]:\n",
    "            #        print(\"SKIPPING DAY: \", testDayIdx)\n",
    "            #        continue\n",
    "            \n",
    "            if len(args['restricted_days']) > 0:\n",
    "                if testDayIdx not in args['restricted_days']:\n",
    "                    continue\n",
    "                \n",
    "            test_ds = SpeechDataset([loadedData[partition][i]])\n",
    "            \n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_ds, batch_size=1, shuffle=False, num_workers=0\n",
    "            )\n",
    "            \n",
    "            for j, (X, y, X_len, y_len, _) in enumerate(test_loader):\n",
    "                        \n",
    "                X, y, X_len, y_len, dayIdx = (\n",
    "                    X.to(device),\n",
    "                    y.to(device),\n",
    "                    X_len.to(device),\n",
    "                    y_len.to(device),\n",
    "                    torch.tensor([testDayIdx], dtype=torch.int64).to(device),\n",
    "                )\n",
    "                \n",
    "                if args['ventral_6v_only']:\n",
    "                    X = X[:, :, :128]\n",
    "                \n",
    "                if fill_max_day:\n",
    "                    dayIdx.fill_(args['maxDay'])\n",
    "                      \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    pred = model.forward(X, X_len, dayIdx)\n",
    "                \n",
    "                if hasattr(model, 'compute_length'):\n",
    "                    adjustedLens = model.compute_length(X_len)\n",
    "                else:\n",
    "                    adjustedLens = ((X_len - model.kernelLen) / model.strideLen).to(torch.int32)\n",
    "                    \n",
    "                for iterIdx in range(pred.shape[0]):\n",
    "                    \n",
    "                    trueSeq = np.array(y[iterIdx][0 : y_len[iterIdx]].cpu().detach())\n",
    "                    model_outputs[\"logits\"].append(pred[iterIdx].cpu().detach().numpy())\n",
    "                    \n",
    "                    model_outputs[\"logitLengths\"].append(\n",
    "                        adjustedLens[iterIdx].cpu().detach().item()\n",
    "                    )\n",
    "                    \n",
    "                    model_outputs[\"trueSeqs\"].append(trueSeq)\n",
    "                    \n",
    "                    decodedSeq = torch.argmax(\n",
    "                        torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n",
    "                        dim=-1,\n",
    "                    ) \n",
    "                    \n",
    "                    decodedSeq = torch.unique_consecutive(decodedSeq, dim=-1)\n",
    "                    decodedSeq = decodedSeq.cpu().detach().numpy()\n",
    "                    decodedSeq = np.array([i for i in decodedSeq if i != 0])\n",
    "                    \n",
    "                    matcher = SequenceMatcher(\n",
    "                        a=trueSeq.tolist(), b=decodedSeq.tolist()\n",
    "                    )\n",
    "                    \n",
    "                    total_edit_distance += matcher.distance()\n",
    "                    total_seq_length += len(trueSeq)\n",
    "                    \n",
    "                    day_edit_distance += matcher.distance()\n",
    "                    day_seq_length += len(trueSeq)\n",
    "                    \n",
    "                transcript = loadedData[partition][i][\"transcriptions\"][j].strip()\n",
    "                transcript = re.sub(r\"[^a-zA-Z\\- \\']\", \"\", transcript)\n",
    "                transcript = transcript.replace(\"--\", \"\").lower()\n",
    "                model_outputs[\"transcriptions\"].append(transcript)\n",
    "                \n",
    "            cer_day = day_edit_distance / day_seq_length\n",
    "            day_cer_dict[seed].append(cer_day)\n",
    "            print(\"CER DAY: \", cer_day)\n",
    "            day_edit_distance = 0 \n",
    "            day_seq_length = 0\n",
    "\n",
    "        cer = total_edit_distance / total_seq_length\n",
    "        \n",
    "        print(\"Model performance: \", cer)\n",
    "        \n",
    "        if run_lm:\n",
    "            \n",
    "            print(\"Running n-gram LM\")\n",
    "            \n",
    "            llm_outputs = []\n",
    "            start_t = time.time()\n",
    "            nbest_outputs = []\n",
    "            \n",
    "            sample_times = []\n",
    "\n",
    "            for j in range(len(model_outputs[\"logits\"])):\n",
    "                logits = model_outputs[\"logits\"][j]\n",
    "\n",
    "                # Move blank token to the end\n",
    "                logits = np.concatenate([logits[:, 1:], logits[:, 0:1]], axis=-1)\n",
    "\n",
    "                # Rearrange logits for decoding\n",
    "                logits = lmDecoderUtils.rearrange_speech_logits(logits[None, :, :], has_sil=True)\n",
    "\n",
    "                # Time individual sample decoding\n",
    "                sample_start = time.time()\n",
    "                nbest = lmDecoderUtils.lm_decode(\n",
    "                    ngramDecoder,\n",
    "                    logits[0],\n",
    "                    blankPenalty=blank_penalty,\n",
    "                    returnNBest=return_n_best,\n",
    "                    rescore=rescore,\n",
    "                )\n",
    "                sample_end = time.time()\n",
    "\n",
    "                sample_times.append(sample_end - sample_start)\n",
    "                nbest_outputs.append(nbest)\n",
    "\n",
    "            # Convert to numpy array for stats\n",
    "            sample_times = np.array(sample_times)\n",
    "            mean_time = np.mean(sample_times)\n",
    "            std_time = np.std(sample_times)\n",
    "\n",
    "            print(f\"Average decoding time per sample: {mean_time:.3f} seconds\")\n",
    "            print(f\"Standard deviation: {std_time:.3f} seconds\")\n",
    " \n",
    "            if run_for_llm:\n",
    "                \n",
    "                print(\"SAVING OUTPUTS FOR LLM\")\n",
    "                with open(f\"{saveFolder_transcripts}{model_name_str}_seed_{seed}_model_outputs.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(model_outputs, f)\n",
    "                    \n",
    "                with open(f\"{saveFolder_transcripts}{model_name_str}_seed_{seed}_nbest.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(nbest_outputs, f)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # just get perf with greedy decoding\n",
    "                for i in range(len(model_outputs[\"transcriptions\"])):\n",
    "                    model_outputs[\"transcriptions\"][i] = model_outputs[\"transcriptions\"][i].strip()\n",
    "                    nbest_outputs[i] = nbest_outputs[i].strip()\n",
    "                \n",
    "                # lower case + remove puncs\n",
    "                for i in range(len(model_outputs[\"transcriptions\"])):\n",
    "                    model_outputs[\"transcriptions\"][i] = convert_sentence(model_outputs[\"transcriptions\"][i])\n",
    "\n",
    "                cer, wer = _cer_and_wer(nbest_outputs, model_outputs[\"transcriptions\"], \n",
    "                                    outputType='speech', returnCI=True)\n",
    "\n",
    "                print(\"CER and WER after 3-gram LM: \", cer, wer)       \n",
    "                \n",
    "                out_file = os.path.join(saveFolder_transcripts, output_file)   # no extension per your spec\n",
    "                \n",
    "                with open(out_file + '.txt', write_mode, encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(nbest_outputs)+ \"\\n\")   # one line per LLM output  \n",
    "                    \n",
    "                total_wer_dict[seed] = wer\n",
    "                \n",
    "#if memo:\n",
    "#    with open(f\"/data/willett_data/paper_results/{model_name_str}_per_memo.pkl\", \"wb\") as f:\n",
    "#        pickle.dump(day_cer_dict, f)\n",
    "        \n",
    "#    with open(f\"/data/willett_data/paper_results/{model_name_str}_wer_memo.pkl\", \"wb\") as f:\n",
    "#        pickle.dump(total_wer_dict, f)ls\n",
    "        \n",
    "#else:\n",
    "#    \n",
    "#    with open(f\"/data/willett_data/paper_results/{model_name_str}_per.pkl\", \"wb\") as f:\n",
    "#        pickle.dump(day_cer_dict, f)\n",
    "#        \n",
    "#    with open(f\"/data/willett_data/paper_results/{model_name_str}_wer.pkl\", \"wb\") as f:\n",
    "#        pickle.dump(total_wer_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GRU SHORT NON OVERLAPPING 4 4 1024 units\n",
    "(0.18748863429714493, 0.1733481163205817, 0.20224525762022216)\n",
    "\n",
    "#  GRU SHORT NON OVERLAPPING 4 4 768 units\n",
    "\n",
    "#  GRU SHORT NON OVERLAPPING 4 4 512 units\n",
    "CER and WER after 3-gram LM:  (0.14898485453819055, 0.13837987803968166, 0.15996022357119136) (0.20258228768867068, 0.18814850882093556, 0.21746997243144325)\n",
    "CER and WER after 3-gram LM:  (0.15410598191261396, 0.1433691371958222, 0.16531221010578537) (0.2078559738134206, 0.19354253159267176, 0.22291510680325294)\n",
    "CER and WER after 3-gram LM:  (0.15980823012385137, 0.14892442509510415, 0.17068014782871976) (0.21640298236042918, 0.20112935287334593, 0.23179312681415)\n",
    "\n",
    "# GRU SHORT 4 layers \n",
    "(0.18057828696126568, 0.16678629445964432, 0.1944448216548977)\n",
    "(0.18803418803418803, 0.1738094510763848, 0.20240924226959411\n",
    "(0.18003273322422259, 0.16660588574119756, 0.19369812058380678)\n",
    "(0.1867612293144208, 0.1725848132153764, 0.2012581255961117)\n",
    "\n",
    "\n",
    "\n",
    "# GRU OVERLAPPING 1024 Units\n",
    "(0.12759234373297498, 0.1177953831800847, 0.13754510964858174) (0.17821422076741225, 0.16463861375291677, 0.1922452709629667)\n",
    "(0.12494097991501107, 0.11526411112142802, 0.13481354564910503) (0.17585015457355882, 0.16250681574211354, 0.1895082789745812)\n",
    "(0.12839138488359422, 0.1185083430244113, 0.13813175907892178) (0.18221494817239497, 0.16842248424125197, 0.19599519410498778)\n",
    "(0.1250862601242146, 0.11504058168559311, 0.13520741295347288) (0.1778505182760502, 0.16381152203794247, 0.19241147368812195)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TM TRANSFORMER DEPTH 5\n",
    "CER and WER after 3-gram LM:  (0.115933606944394, 0.1067014506708351, 0.1252127983640603) (0.16657574104382614, 0.15368177508624756, 0.1795666612468941)\n",
    "CER and WER after 3-gram LM:  (0.12352449787527695, 0.11433927553171039, 0.1328864642207172) (0.1783960720130933, 0.1655529013791415, 0.19171298671996517)\n",
    "CER and WER after 3-gram LM:  (0.11909345149457015, 0.10942212714998742, 0.12876255652512378) (0.1691216584833606, 0.15584404058844953, 0.18245120621353922)\n",
    "CER and WER after 3-gram LM:  (0.11691424835651763, 0.10748297572304032, 0.12684070407993206) (0.16930350972904165, 0.15640483471072475, 0.1827764567863795)\n",
    "\n",
    "# TM TRANSFORMER DEPTH 7\n",
    "CER and WER after 3-gram LM:  (0.11273744234191697, 0.10379952109750006, 0.12199860729002777) (0.1691216584833606, 0.15625540724491016, 0.18251230526392123)\n",
    "CER and WER after 3-gram LM:  (0.12334289761377257, 0.11391797673479954, 0.13266513338160046) (0.17966903073286053, 0.166331520250247, 0.19331443798449613)\n",
    "CER and WER after 3-gram LM:  (0.1137180837540406, 0.10443577552864726, 0.1234524891407505) (0.1629387161302055, 0.1501740988670145, 0.17593611199732556)\n",
    "CER and WER after 3-gram LM:  (0.1206552137435078, 0.11126393883529243, 0.13027161823251315) (0.17421349336242953, 0.160873204413609, 0.18752321917567022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPTH 5:  17.0865\n",
      "DEPTH 7:  17.145475\n"
     ]
    }
   ],
   "source": [
    "print(\"DEPTH 5: \", (16.665 + 17.839 + 16.912 + 16.930)/4)\n",
    "print(\"DEPTH 7: \", (16.9121+ 17.966 + 16.2938 + 17.41)/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['restricted_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['ventral_6v_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_and_sem(file_name):\n",
    "    data_dict = pd.read_pickle(f'/home3/ebrahim/paper_results_wer/{file_name}.pkl')\n",
    "    averaged_dict = np.mean(np.array(list(data_dict.values())), axis=0)\n",
    "    std_dict = np.std(np.array(list(data_dict.values())), axis=0)\n",
    "    print(\"FILENAME: \", file_name)\n",
    "    print(\"MEAN: \", averaged_dict*100)\n",
    "    print(\"SEM: \", std_dict*100/np.sqrt(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME:  transformer_shortened_held_out_days_big_diet64corp\n",
      "MEAN:  [26.97180221 26.37997433 27.56913303 30.45539033 29.70240366 29.08466394\n",
      " 31.35994587 31.91169672]\n",
      "SEM:  [0.27679234 0.295103   0.12629396 0.40539519 0.20479693 0.38262064\n",
      " 0.75516775 1.25215305]\n",
      "FILENAME:  transformer_shortened_held_out_days_big_baseline\n",
      "MEAN:  [28.87208827 34.49935815 44.78699552 53.28996283 59.97710797 52.71073153\n",
      " 62.95669824 66.47018031]\n",
      "SEM:  [0.47007426 0.43882473 0.33980817 0.24676428 0.31259003 0.51828908\n",
      " 0.40481692 0.57613577]\n"
     ]
    }
   ],
   "source": [
    "print_mean_and_sem('transformer_shortened_held_out_days_big_diet64corp')\n",
    "print_mean_and_sem('transformer_shortened_held_out_days_big_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILENAME:  transformer_short_held_out_normal_diet64corp1e3\n",
      "MEAN:  [21.14312268 21.23235406 21.39806907 21.93279206 22.26074896]\n",
      "SEM:  [0.31526679 0.20612536 0.22255797 0.65391546 0.60046117]\n",
      "FILENAME:  transformer_short_held_out_normal_baseline\n",
      "MEAN:  [22.74163569 25.47691721 25.42703305 30.1308074  32.58206195]\n",
      "SEM:  [0.26101016 0.27886406 0.32857602 0.33030714 0.51801989]\n"
     ]
    }
   ],
   "source": [
    "print_mean_and_sem('transformer_short_held_out_normal_diet64corp1e3')\n",
    "print_mean_and_sem('transformer_short_held_out_normal_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
