{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Run this once per kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from edit_distance import SequenceMatcher\n",
    "import torch\n",
    "from dataset import SpeechDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.model import GRUDecoder\n",
    "import pickle\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_decoder.dataset import getDatasetLoaders\n",
    "import neural_decoder.lm_utils as lmDecoderUtils\n",
    "from neural_decoder.lm_utils import build_llama_1B\n",
    "from neural_decoder.model import GRUDecoder\n",
    "from neural_decoder.bit import BiT_Phoneme\n",
    "import pickle\n",
    "import argparse\n",
    "from lm_utils import _cer_and_wer\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from loss import memo_loss_from_logits, forward_ctc\n",
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFolder_data = \"/data/willett_data/paper_results_obi/\"\n",
    "saveFolder_transcripts = \"/data/willett_data/model_transcriptions_comp/\"\n",
    "\n",
    "output_file = 'leia'\n",
    "device = \"cuda:2\"\n",
    "\n",
    "if output_file == 'obi':\n",
    "    model_storage_path = '/data/willett_data/outputs/'\n",
    "elif output_file == 'leia':\n",
    "    model_storage_path = '/data/willett_data/leia_outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING IN N-GRAM MODE\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home3/skaasyap/willett\"\n",
    "\n",
    "load_lm = False\n",
    "# LM decoding hyperparameters\n",
    "acoustic_scale = 0.8\n",
    "blank_penalty = np.log(2)\n",
    "\n",
    "run_for_llm = False\n",
    "\n",
    "if run_for_llm:\n",
    "    return_n_best = True\n",
    "    rescore = False\n",
    "    nbest = 100\n",
    "    print(\"RUNNING IN LLM MODE\")\n",
    "else:\n",
    "    return_n_best = False\n",
    "    rescore = False\n",
    "    nbest = 1\n",
    "    print(\"RUNNING IN N-GRAM MODE\")\n",
    "    \n",
    "if load_lm: \n",
    "        \n",
    "    lmDir = base_dir +'/lm/languageModel'\n",
    "    ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "        lmDir,\n",
    "        acoustic_scale=acoustic_scale, #1.2\n",
    "        nbest=nbest,\n",
    "        beam=18\n",
    "    )\n",
    "    print(\"loaded LM\")\n",
    "    \n",
    "    load_lm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to shared output file\n"
     ]
    }
   ],
   "source": [
    "models_to_run = ['neurips_transformer_time_masked_held_out_days_2', \n",
    "                 'neurips_transformer_time_masked_held_out_days_1', \n",
    "                 'neurips_transformer_time_masked_held_out_days']\n",
    "\n",
    "\n",
    "shared_output_file = 'entropy_min'\n",
    "val_save_file = 'entropy_min'\n",
    "seeds_list = [0,1,2,3]\n",
    "\n",
    "if len(shared_output_file) > 0:\n",
    "    print(\"Writing to shared output file\")\n",
    "    write_mode = \"a\"\n",
    "else:\n",
    "    write_mode = \"w\"\n",
    "    \n",
    "evaluate_comp = True\n",
    "run_lm = True\n",
    "\n",
    "tta = True\n",
    "run_memo = True\n",
    "run_lang_informed = False\n",
    "\n",
    "memo_epochs = 1\n",
    "memo_augs = 0\n",
    "if memo_augs:\n",
    "    max_mask_pct = 0.05\n",
    "    num_masks = 20\n",
    "else:\n",
    "    max_mask_pct = 0\n",
    "    num_masks = 0\n",
    "\n",
    "\n",
    "nptl_augs = 0\n",
    "nptl_aug_params = [0.2, 0.05] # white noise, constant offset\n",
    "\n",
    "memo_lr = [3e-5, 6e-5, 6e-5]\n",
    "\n",
    "partition = \"competition\" \n",
    "blank_id = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_0\n",
      "DAY CER:  0.3284115035707392\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.380046403712297\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5224416517055656\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5491719863077066\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5892558916311004\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_1\n",
      "DAY CER:  0.3249372707971434\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.36751740139211136\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5191929554586646\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5494495327967435\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5809055483392095\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_2\n",
      "DAY CER:  0.31789229878401853\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.36241299303944313\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5126100709583654\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5463965214173374\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5818333642605307\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_2_seed_3\n",
      "DAY CER:  0.3188573634433507\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.3673317865429234\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5020945541591861\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5484318623369414\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "DAY CER:  0.5855446279458155\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_2\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_0\n",
      "DAY CER:  0.1893591066904382\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.2011852776044916\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.27035203520352036\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.33125247720967105\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.38925010836584306\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_1\n",
      "DAY CER:  0.20165149666885615\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.20461634435433562\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.2786028602860286\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.32382084819659135\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.39228435197225836\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_2\n",
      "DAY CER:  0.20418504269494228\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.20056144728633812\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.27805280528052806\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.32827982560443913\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.3851755526657997\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_1_seed_3\n",
      "DAY CER:  0.19208032279253073\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.18985236015803703\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.2688852218555189\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.31529924692826\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "DAY CER:  0.3811876896402254\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days_1\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_0\n",
      "DAY CER:  0.1958174904942966\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.21266887357967254\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2108843537414966\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2558579384259746\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.25327416387054874\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_1\n",
      "DAY CER:  0.20566885585896993\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.21651605976559005\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2157065357788685\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.25396658610906797\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2585777681567269\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_2\n",
      "DAY CER:  0.18899066712754925\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.20470609286928515\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.21174545767674158\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.24514027529683724\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.257387163112891\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "Running model: neurips_transformer_time_masked_held_out_days_seed_3\n",
      "DAY CER:  0.1850155547874179\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2048850317616534\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.20390941186601222\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2450352001681202\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n",
      "DAY CER:  0.2492694014503734\n",
      "SAVING VAL RESULTS FOR neurips_transformer_time_masked_held_out_days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            model.eval()\\n            day_edit_distance = day_seq_length = 0\\n\\n            with torch.no_grad():\\n                \\n                for i, (X, y, X_len, y_len, _) in enumerate(data_loader):\\n                    \\n                    X, y, X_len, y_len = map(lambda x: x.to(device), [X, y, X_len, y_len])\\n                    \\n                    dayIdx = torch.tensor([ve], dtype=torch.int64).to(device)\\n                    pred = model(X, X_len, dayIdx)\\n                    adjustedLens = model.compute_length(X_len)\\n                \\n                    if i < len(val_ds):\\n            \\n                        for idx in range(pred.shape[0]):\\n                            trueSeq = y[idx][:y_len[idx]].cpu().numpy()\\n                            decoded = decode_sequence(pred[idx], adjustedLens[idx])\\n                            dist = SequenceMatcher(a=trueSeq.tolist(), b=decoded.tolist()).distance()\\n\\n                            total_edit_distance += dist\\n                            total_seq_length += len(trueSeq)\\n                            day_edit_distance += dist\\n                            day_seq_length += len(trueSeq)\\n                            \\n                    else: \\n                    \\n                        for idx in range(pred.shape[0]):\\n                            \\n                            decoded = decode_sequence(pred[idx], adjustedLens[idx])\\n                            \\n                            transcript = loadedData[partition][i][\"transcriptions\"][j].strip()\\n                            transcript = re.sub(r\"[^a-zA-Z\\\\- \\']\", \"\", transcript).replace(\"--\", \"\").lower()\\n\\n                            model_outputs[\"logits\"].append(pred[idx].cpu().numpy())\\n                            model_outputs[\"logitLengths\"].append(adjustedLens[idx].item())\\n                            model_outputs[\"trueSeqs\"].append(y[idx][:y_len[idx]].cpu().numpy())\\n                            model_outputs[\"transcriptions\"].append(transcript)\\n                        \\n\\n\\n\\n        if run_lm:\\n            print(\"Running LM decoding...\")\\n            nbest_outputs = []\\n            for logits in model_outputs[\"logits\"]:\\n                logits = np.concatenate([logits[:, 1:], logits[:, 0:1]], axis=-1)\\n                logits = lmDecoderUtils.rearrange_speech_logits(logits[None, :, :], has_sil=True)\\n                decoded = lmDecoderUtils.lm_decode(ngramDecoder, logits[0], blankPenalty=blank_penalty,\\n                                                   returnNBest=return_n_best, rescore=rescore)\\n                nbest_outputs.append(decoded)\\n\\n            model_outputs[\"transcriptions\"] = [convert_sentence(t.strip()) for t in model_outputs[\"transcriptions\"]]\\n            nbest_outputs = [t.strip() for t in nbest_outputs]\\n            cer, wer = _cer_and_wer(nbest_outputs, model_outputs[\"transcriptions\"],\\n                                    outputType=\\'speech\\', returnCI=True)\\n            total_wer_dict[seed] = wer\\n\\n           \\n    if val_save_file:\\n        print(f\"SAVING VAL RESULTS FOR {model_name_str}\")\\n        with open(f\"{saveFolder_data}{model_name_str}_{val_save_file}.pkl\", \"wb\") as f:\\n            pickle.dump(day_cer_dict, f)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_sentence(s):\n",
    "    s = s.lower()\n",
    "    charMarks = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "                 \"'\", ' ']\n",
    "    ans = []\n",
    "    for i in s:\n",
    "        if(i in charMarks):\n",
    "            ans.append(i)\n",
    "    \n",
    "    return ''.join(ans)\n",
    "\n",
    "\n",
    "def get_phonemes(thisTranscription):\n",
    "    \n",
    "    phonemes = []\n",
    "    g2p = G2p()\n",
    "    \n",
    "    \n",
    "    for p in g2p(thisTranscription):\n",
    "        \n",
    "        if p == ' ':\n",
    "            phonemes.append('SIL')\n",
    "        p = re.sub(r'[0-9]', '', p)  # Remove stress\n",
    "        if re.match(r'^[A-Z]+$', p):  # Only keep phonemes (uppercase only)\n",
    "            phonemes.append(p)\n",
    "    \n",
    "    phonemes.append('SIL')  # Add trailing SIL\n",
    "    \n",
    "    PHONE_DEF = [\n",
    "        'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "        'AY', 'B',  'CH', 'D', 'DH',\n",
    "        'EH', 'ER', 'EY', 'F', 'G',\n",
    "        'HH', 'IH', 'IY', 'JH', 'K',\n",
    "        'L', 'M', 'N', 'NG', 'OW',\n",
    "        'OY', 'P', 'R', 'S', 'SH',\n",
    "        'T', 'TH', 'UH', 'UW', 'V',\n",
    "        'W', 'Y', 'Z', 'ZH'\n",
    "    ]\n",
    "    PHONE_DEF_SIL = PHONE_DEF + ['SIL']\n",
    "\n",
    "    phoneme_ids = [PHONE_DEF_SIL.index(p) for p in phonemes]\n",
    "\n",
    "    return torch.tensor(phoneme_ids, dtype=torch.long), torch.tensor([len(phoneme_ids)], dtype=torch.long)\n",
    "\n",
    "def get_data_file(path):\n",
    "    suffix_map = {\n",
    "        \"data_log_both\": \"/data/willett_data/ptDecoder_ctc_both\",\n",
    "        \"data\": \"/data/willett_data/ptDecoder_ctc\",\n",
    "        \"data_log_both_held_out_days\": \"/data/willett_data/ptDecoder_ctc_both_held_out_days\",\n",
    "        \"data_log_both_held_out_days_1\": \"/data/willett_data/ptDecoder_ctc_both_held_out_days_1\",\n",
    "        \"data_log_both_held_out_days_2\": \"/data/willett_data/ptDecoder_ctc_both_held_out_days_2\",\n",
    "    }\n",
    "    suffix = path.rsplit('/', 1)[-1]\n",
    "    return suffix_map.get(suffix, path)\n",
    "\n",
    "def reverse_dataset(dataset):\n",
    "    return Subset(dataset, list(reversed(range(len(dataset)))))\n",
    "\n",
    "def get_dataloader(dataset, batch_size=1):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                       shuffle=False, num_workers=0)\n",
    "\n",
    "def decode_sequence(pred, adjusted_len):\n",
    "    pred = torch.argmax(pred[:adjusted_len], dim=-1)\n",
    "    pred = torch.unique_consecutive(pred)\n",
    "    return np.array([i for i in pred.cpu().numpy() if i != 0])\n",
    "\n",
    "day_edit_distance = 0\n",
    "day_seq_length = 0\n",
    "\n",
    "for mn, model_name_str in enumerate(models_to_run):\n",
    "    day_cer_dict, total_wer_dict = {}, {}\n",
    "\n",
    "    for seed in seeds_list:\n",
    "        \n",
    "        print(f\"Running model: {model_name_str}_seed_{seed}\")\n",
    "        \n",
    "        day_cer_dict[seed], total_wer_dict[seed] = [], []\n",
    "\n",
    "        modelPath = f\"{model_storage_path}{model_name_str}_seed_{seed}\"\n",
    "        output_file = f\"{shared_output_file}_seed_{seed}\" if shared_output_file else f\"{model_name_str}_seed_{seed}\"\n",
    "\n",
    "        with open(f\"{modelPath}/args\", \"rb\") as handle:\n",
    "            args = pickle.load(handle)\n",
    "            \n",
    "        model = BiT_Phoneme(\n",
    "        patch_size=args['patch_size'], dim=args['dim'], dim_head=args['dim_head'],\n",
    "        nClasses=args['nClasses'], depth=args['depth'], heads=args['heads'],\n",
    "        mlp_dim_ratio=args['mlp_dim_ratio'], dropout=0, input_dropout=0,\n",
    "        look_ahead=args['look_ahead'], gaussianSmoothWidth=args['gaussianSmoothWidth'],\n",
    "        T5_style_pos=args['T5_style_pos'], max_mask_pct=max_mask_pct,\n",
    "        num_masks=num_masks, mask_token_zeros=args['mask_token_zero'], max_mask_channels=0,\n",
    "        num_masks_channels=0, dist_dict_path=None\n",
    "        ).to(device)\n",
    "\n",
    "        data_file = get_data_file(args['datasetPath'])\n",
    "\n",
    "        trainLoaders, testLoaders, loadedData = getDatasetLoaders(data_file, 8)\n",
    "        args.setdefault('mask_token_zero', False)\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"{modelPath}/modelWeights\", map_location=device), strict=True)\n",
    "        model.eval()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=memo_lr[mn], weight_decay=0,\n",
    "                                      betas=(args['beta1'], args['beta2']))\n",
    "\n",
    "        for name, p in model.named_parameters():\n",
    "            p.requires_grad = name in {\n",
    "                \"to_patch_embedding.1.weight\", \"to_patch_embedding.1.bias\",\n",
    "                \"to_patch_embedding.2.weight\", \"to_patch_embedding.2.bias\",\n",
    "                \"to_patch_embedding.3.weight\", \"to_patch_embedding.3.bias\"\n",
    "            }\n",
    "\n",
    "        testDayIdxs = np.arange(5)\n",
    "        valDayIdxs = [0, 1, 3, 4, 5] if mn == 2 else [0, 1, 2, 3, 4]\n",
    "\n",
    "        model_outputs = {\"logits\": [], \"logitLengths\": [], \"trueSeqs\": [], \"transcriptions\": []}\n",
    "        \n",
    "        total_edit_distance = total_seq_length = 0\n",
    "        nbest_outputs = []\n",
    "        nbest_outputs_val = []\n",
    "        \n",
    "        for i, testDayIdx in enumerate(testDayIdxs):\n",
    "            \n",
    "            ve = valDayIdxs[i]\n",
    "            val_ds = reverse_dataset(SpeechDataset([loadedData['test'][ve]]))\n",
    "            test_ds = reverse_dataset(SpeechDataset([loadedData['competition'][i]]))\n",
    "            combined_ds = ConcatDataset([val_ds, test_ds])\n",
    "            data_loader = get_dataloader(combined_ds)\n",
    "\n",
    "            if tta:\n",
    "            \n",
    "                for trial_idx, (X, y, X_len, y_len, _) in enumerate(data_loader):\n",
    "                    \n",
    "                \n",
    "                    X, y, X_len, y_len = map(lambda x: x.to(device), [X, y, X_len, y_len])\n",
    "                    \n",
    "                    dayIdx = torch.tensor([ve], dtype=torch.int64).to(device)\n",
    "                    \n",
    "                    model.train()\n",
    "                    \n",
    "                    memo_loss = li_loss = torch.tensor(0.0, device=device)\n",
    "                    for _ in range(memo_epochs):\n",
    "                        \n",
    "                        logits_aug = model(X, X_len, ve, memo_augs, nptl_augs, nptl_aug_params)\n",
    "                        logits_np = logits_aug[0].detach().cpu().numpy()\n",
    "                        logits = logits_aug[0:1]\n",
    "                        adjusted_len = model.compute_length(X_len)\n",
    "                        \n",
    "                        if run_memo:\n",
    "                            \n",
    "                            memo_loss = memo_loss_from_logits(logits_aug, adjusted_len, blank_id)\n",
    "                            \n",
    "                        if run_lang_informed:\n",
    "                            \n",
    "                            logits_np = np.concatenate([logits_np[:, 1:], logits_np[:, 0:1]], axis=-1)\n",
    "                            \n",
    "                            logit_np = lmDecoderUtils.rearrange_speech_logits(logits_np[None, :, :], \n",
    "                                                                            has_sil=True)\n",
    "                            \n",
    "                            \n",
    "                            decoded = lmDecoderUtils.lm_decode(ngramDecoder, logits_np, \n",
    "                                                        blankPenalty=blank_penalty,\n",
    "                                                    returnNBest=return_n_best, rescore=rescore)\n",
    "                            \n",
    "                            y_pseudo, y_len_pseudo = get_phonemes(decoded)\n",
    "                            y_pseudo = y_pseudo.to(device)\n",
    "                            y_len_pseudo = y_len_pseudo.to(device)\n",
    "                                                        \n",
    "                            \n",
    "                            li_loss = forward_ctc(logits, adjusted_len, \n",
    "                                                  y_pseudo, y_len_pseudo)\n",
    "                      \n",
    "                        loss = memo_loss\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    model.eval()\n",
    "                    \n",
    "                    # get validation performance \n",
    "                    if trial_idx < len(val_ds):\n",
    "            \n",
    "                        for idx in range(logits.shape[0]):\n",
    "                            trueSeq = y[idx][:y_len[idx]].cpu().numpy()\n",
    "                            decoded = decode_sequence(logits[idx], adjusted_len[idx])\n",
    "                            dist = SequenceMatcher(a=trueSeq.tolist(), b=decoded.tolist()).distance()\n",
    "\n",
    "                            total_edit_distance += dist\n",
    "                            total_seq_length += len(trueSeq)\n",
    "                            day_edit_distance += dist\n",
    "                            day_seq_length += len(trueSeq)\n",
    "                            \n",
    "                    # get test set predictions \n",
    "                    #else:   \n",
    "                    #    nbest_outputs.append(decoded)\n",
    "                        \n",
    "            print(\"DAY CER: \", day_edit_distance / day_seq_length)\n",
    "            day_cer_dict[seed].append(day_edit_distance / day_seq_length)\n",
    "            day_edit_distance = 0 \n",
    "            day_seq_length = 0\n",
    "            \n",
    "            if val_save_file:\n",
    "                print(f\"SAVING VAL RESULTS FOR {model_name_str}\")\n",
    "                with open(f\"{saveFolder_data}{model_name_str}_{val_save_file}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(day_cer_dict, f)\n",
    "            \n",
    "            \n",
    "        #out_file = os.path.join(saveFolder_transcripts, output_file)\n",
    "        #with open(out_file + '.txt', write_mode, encoding=\"utf-8\") as f:\n",
    "        #    f.write(\"\\n\".join(nbest_outputs) + \"\\n\")\n",
    "            \n",
    "        #model_outputs[\"transcriptions\"] = [convert_sentence(t.strip()) for t in model_outputs[\"transcriptions\"]]\n",
    "        #nbest_outputs = [t.strip() for t in nbest_outputs]\n",
    "        #cer, wer = _cer_and_wer(nbest_outputs, model_outputs[\"transcriptions\"],\n",
    "        #                        outputType='speech', returnCI=True)\n",
    "        #total_wer_dict[seed] = wer\n",
    "\n",
    "'''\n",
    "            model.eval()\n",
    "            day_edit_distance = day_seq_length = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                for i, (X, y, X_len, y_len, _) in enumerate(data_loader):\n",
    "                    \n",
    "                    X, y, X_len, y_len = map(lambda x: x.to(device), [X, y, X_len, y_len])\n",
    "                    \n",
    "                    dayIdx = torch.tensor([ve], dtype=torch.int64).to(device)\n",
    "                    pred = model(X, X_len, dayIdx)\n",
    "                    adjustedLens = model.compute_length(X_len)\n",
    "                \n",
    "                    if i < len(val_ds):\n",
    "            \n",
    "                        for idx in range(pred.shape[0]):\n",
    "                            trueSeq = y[idx][:y_len[idx]].cpu().numpy()\n",
    "                            decoded = decode_sequence(pred[idx], adjustedLens[idx])\n",
    "                            dist = SequenceMatcher(a=trueSeq.tolist(), b=decoded.tolist()).distance()\n",
    "\n",
    "                            total_edit_distance += dist\n",
    "                            total_seq_length += len(trueSeq)\n",
    "                            day_edit_distance += dist\n",
    "                            day_seq_length += len(trueSeq)\n",
    "                            \n",
    "                    else: \n",
    "                    \n",
    "                        for idx in range(pred.shape[0]):\n",
    "                            \n",
    "                            decoded = decode_sequence(pred[idx], adjustedLens[idx])\n",
    "                            \n",
    "                            transcript = loadedData[partition][i][\"transcriptions\"][j].strip()\n",
    "                            transcript = re.sub(r\"[^a-zA-Z\\- \\']\", \"\", transcript).replace(\"--\", \"\").lower()\n",
    "\n",
    "                            model_outputs[\"logits\"].append(pred[idx].cpu().numpy())\n",
    "                            model_outputs[\"logitLengths\"].append(adjustedLens[idx].item())\n",
    "                            model_outputs[\"trueSeqs\"].append(y[idx][:y_len[idx]].cpu().numpy())\n",
    "                            model_outputs[\"transcriptions\"].append(transcript)\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "        if run_lm:\n",
    "            print(\"Running LM decoding...\")\n",
    "            nbest_outputs = []\n",
    "            for logits in model_outputs[\"logits\"]:\n",
    "                logits = np.concatenate([logits[:, 1:], logits[:, 0:1]], axis=-1)\n",
    "                logits = lmDecoderUtils.rearrange_speech_logits(logits[None, :, :], has_sil=True)\n",
    "                decoded = lmDecoderUtils.lm_decode(ngramDecoder, logits[0], blankPenalty=blank_penalty,\n",
    "                                                   returnNBest=return_n_best, rescore=rescore)\n",
    "                nbest_outputs.append(decoded)\n",
    "\n",
    "            model_outputs[\"transcriptions\"] = [convert_sentence(t.strip()) for t in model_outputs[\"transcriptions\"]]\n",
    "            nbest_outputs = [t.strip() for t in nbest_outputs]\n",
    "            cer, wer = _cer_and_wer(nbest_outputs, model_outputs[\"transcriptions\"],\n",
    "                                    outputType='speech', returnCI=True)\n",
    "            total_wer_dict[seed] = wer\n",
    "\n",
    "           \n",
    "    if val_save_file:\n",
    "        print(f\"SAVING VAL RESULTS FOR {model_name_str}\")\n",
    "        with open(f\"{saveFolder_data}{model_name_str}_{val_save_file}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(day_cer_dict, f)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
