{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home3/skaasyap/willett\"\n",
    "batch_size=8\n",
    "device = 'cuda:3'\n",
    "sentence_path = '/home3/skaasyap/my_list.pkl'\n",
    "\n",
    "model_paths = ['/home3/skaasyap/willett/outputs/consistency_0.2']\n",
    "model_types = ['t'] #transformer or rnn for each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import neural_decoder.lm_utils as utils\n",
    "from neural_decoder.load_models import loadModel, loadTransformerModel, getDatasetLoaders, rnn_forward, transformer_forward, run_ngram_model, convert_sentence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sentence_path, 'rb') as f:\n",
    "    ground_truth_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmDir = base_dir+'/lm/languageModel'\n",
    "ngramDecoder = utils.build_lm_decoder(\n",
    "    lmDir,\n",
    "    acoustic_scale=0.8, #1.2\n",
    "    nbest=1,\n",
    "    beam=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader, testLoader, loadedData = getDatasetLoaders(\n",
    "    \"/home3/skaasyap/willett/data_log_both\",\n",
    "    batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/skaasyap/miniconda3/envs/willett/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home3/skaasyap/willett/neural_seq_decoder/src/neural_decoder/augmentations.py:170: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return self.conv(input, weight=self.weight, groups=self.groups, padding=\"same\")\n",
      "/home3/skaasyap/willett/neural_seq_decoder/src/neural_decoder/load_models.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pred[iterIdx, 0 : adjustedLens[iterIdx], :]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctc loss: 0.823501, cer: 0.138253\n",
      "880 880\n",
      "cer: (0.11273744234191697, 0.10408587030547085, 0.12155142019074096)\n",
      "wer: (0.16730314602655028, 0.1546729334880064, 0.18015954606576218)\n"
     ]
    }
   ],
   "source": [
    "for i, model_path in enumerate(model_paths):\n",
    "\n",
    "    if(model_types[i] == 't'):\n",
    "        model, args = loadTransformerModel(model_path)\n",
    "        model = model.to(device)\n",
    "        all_logits, trial_lengths, avgDayLoss, cer = transformer_forward(model, testLoader, device)\n",
    "    else:\n",
    "        model, args = loadModel(model_path)\n",
    "        all_logits, trial_lengths, avgDayLoss, cer = transformer_forward(model, testLoader, device)\n",
    "\n",
    "    \n",
    "    all_logits = [l.cpu().numpy().astype('float32') for l in all_logits]\n",
    "    trial_lengths = [l.cpu().numpy().astype('int') for l in trial_lengths]\n",
    "    all_logits = [l for batch in all_logits for l in list(batch)]\n",
    "    trial_lengths = [l for batch in trial_lengths for l in list(batch)]\n",
    "\n",
    "    blank_penalty = np.log(2)\n",
    "    llm_outputs = run_ngram_model(all_logits, trial_lengths, ngramDecoder, blank_penalty)\n",
    "\n",
    "    print(len(llm_outputs), len(ground_truth_sentences))\n",
    "\n",
    "    for i in range(len(ground_truth_sentences)):\n",
    "        ground_truth_sentences[i] = ground_truth_sentences[i].strip()\n",
    "        llm_outputs[i] = llm_outputs[i].strip()\n",
    "\n",
    "    for i in range(len(ground_truth_sentences)):\n",
    "        ground_truth_sentences[i] = convert_sentence(ground_truth_sentences[i])\n",
    "\n",
    "    cer, wer = utils._cer_and_wer(llm_outputs, ground_truth_sentences, outputType='speech_sil', returnCI=True)\n",
    "\n",
    "    print(f\"cer: {cer}\") # cer average, confidence interval \n",
    "    print(f\"wer: {wer}\") #wer average, confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "willett",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
