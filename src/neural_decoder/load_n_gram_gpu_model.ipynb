{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nemo\n",
    "nemo.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning_utilities-0.11.3.post0-py3.10.egg/lightning_utilities/core/imports.py:14: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-21 16:13:24 cloud:58] Found existing object /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo.\n",
      "[NeMo I 2025-08-21 16:13:24 cloud:64] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo\n",
      "[NeMo I 2025-08-21 16:13:24 common:884] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-08-21 16:13:26 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-21 16:13:27 modelPT:181] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    \n",
      "[NeMo W 2025-08-21 16:13:27 modelPT:188] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2025-08-21 16:13:27 modelPT:195] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 12\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-21 16:13:27 features:305] PADDING: 16\n",
      "[NeMo I 2025-08-21 16:13:28 save_restore_connector:282] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_citrinet_512/3262321355385bb7cf5a583146117d77/stt_en_citrinet_512.nemo.\n"
     ]
    }
   ],
   "source": [
    "citrinet = nemo_asr.models.EncDecCTCModelBPE.from_pretrained('stt_en_citrinet_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name              | Type                              | Params | Mode \n",
       "--------------------------------------------------------------------------------\n",
       "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0      | train\n",
       "1 | encoder           | ConvASREncoder                    | 36.3 M | train\n",
       "2 | decoder           | ConvASRDecoder                    | 657 K  | train\n",
       "3 | loss              | CTCLoss                           | 0      | train\n",
       "4 | spec_augmentation | SpectrogramAugmentation           | 0      | train\n",
       "5 | wer               | WER                               | 0      | train\n",
       "--------------------------------------------------------------------------------\n",
       "37.0 M    Trainable params\n",
       "0         Non-trainable params\n",
       "37.0 M    Total params\n",
       "147.977   Total estimated model params size (MB)\n",
       "943       Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citrinet.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 16000\n",
      "train_ds:\n",
      "  manifest_filepath: null\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  trim_silence: true\n",
      "  max_duration: 16.7\n",
      "  shuffle: true\n",
      "  is_tarred: false\n",
      "  tarred_audio_filepaths: null\n",
      "validation_ds:\n",
      "  manifest_filepath: null\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  shuffle: false\n",
      "test_ds:\n",
      "  manifest_filepath:\n",
      "  - /home/smajumdar/PycharmProjects/nemo-eval/nemo_eval/librispeech/manifests/dev_other.json\n",
      "  sample_rate: 16000\n",
      "  batch_size: 32\n",
      "  shuffle: false\n",
      "  num_workers: 12\n",
      "  pin_memory: true\n",
      "model_defaults:\n",
      "  repeat: 5\n",
      "  dropout: 0.0\n",
      "  separable: true\n",
      "  se: true\n",
      "  se_context_size: -1\n",
      "tokenizer:\n",
      "  dir: /home/smajumdar/PycharmProjects/nemo-eval/nemo_beta_eval/asrset/manifests/asrset_1.4/tokenizers/no_appen/tokenizer_spe_unigram_v1024/\n",
      "  type: bpe\n",
      "preprocessor:\n",
      "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "  sample_rate: 16000\n",
      "  normalize: per_feature\n",
      "  window_size: 0.025\n",
      "  window_stride: 0.01\n",
      "  window: hann\n",
      "  features: 80\n",
      "  n_fft: 512\n",
      "  frame_splicing: 1\n",
      "  dither: 1.0e-05\n",
      "  pad_to: 16\n",
      "  stft_conv: false\n",
      "spec_augment:\n",
      "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "  freq_masks: 2\n",
      "  time_masks: 10\n",
      "  freq_width: 27\n",
      "  time_width: 0.05\n",
      "encoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "  feat_in: 80\n",
      "  activation: relu\n",
      "  conv_mask: true\n",
      "  jasper:\n",
      "  - filters: 512\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 5\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 11\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 13\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 15\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 17\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 19\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 21\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 13\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 15\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 17\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 19\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 21\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 23\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 25\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 25\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "    stride_last: true\n",
      "    residual_mode: stride_add\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 27\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 29\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 31\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 35\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 37\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 39\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "  - filters: 640\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 41\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "    se: true\n",
      "    se_context_size: -1\n",
      "decoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
      "  feat_in: 640\n",
      "  num_classes: 1024\n",
      "  vocabulary:\n",
      "  - <unk>\n",
      "  - s\n",
      "  - ▁the\n",
      "  - t\n",
      "  - ▁a\n",
      "  - ▁i\n",
      "  - ''''\n",
      "  - ▁and\n",
      "  - ▁to\n",
      "  - ed\n",
      "  - d\n",
      "  - ▁of\n",
      "  - e\n",
      "  - ▁in\n",
      "  - ing\n",
      "  - .\n",
      "  - ▁it\n",
      "  - ▁you\n",
      "  - 'n'\n",
      "  - ▁that\n",
      "  - m\n",
      "  - 'y'\n",
      "  - er\n",
      "  - ▁he\n",
      "  - re\n",
      "  - r\n",
      "  - ▁was\n",
      "  - ▁is\n",
      "  - ▁for\n",
      "  - ▁know\n",
      "  - a\n",
      "  - p\n",
      "  - c\n",
      "  - ','\n",
      "  - ▁be\n",
      "  - o\n",
      "  - ▁but\n",
      "  - ▁they\n",
      "  - g\n",
      "  - ▁so\n",
      "  - ly\n",
      "  - b\n",
      "  - ▁s\n",
      "  - ▁yeah\n",
      "  - ▁we\n",
      "  - ▁have\n",
      "  - ▁re\n",
      "  - ▁like\n",
      "  - l\n",
      "  - ▁on\n",
      "  - ll\n",
      "  - u\n",
      "  - ▁with\n",
      "  - ▁do\n",
      "  - al\n",
      "  - ▁not\n",
      "  - ▁are\n",
      "  - or\n",
      "  - ar\n",
      "  - le\n",
      "  - ▁this\n",
      "  - ▁as\n",
      "  - es\n",
      "  - ▁c\n",
      "  - ▁de\n",
      "  - f\n",
      "  - in\n",
      "  - i\n",
      "  - ve\n",
      "  - ▁uh\n",
      "  - ent\n",
      "  - ▁or\n",
      "  - ▁what\n",
      "  - ▁me\n",
      "  - ▁t\n",
      "  - ▁at\n",
      "  - ▁my\n",
      "  - ▁his\n",
      "  - ▁there\n",
      "  - w\n",
      "  - ▁all\n",
      "  - ▁just\n",
      "  - h\n",
      "  - ▁can\n",
      "  - ri\n",
      "  - il\n",
      "  - k\n",
      "  - ic\n",
      "  - ▁e\n",
      "  - ▁\n",
      "  - ▁um\n",
      "  - ▁don\n",
      "  - ▁b\n",
      "  - ▁had\n",
      "  - ch\n",
      "  - ation\n",
      "  - en\n",
      "  - th\n",
      "  - ▁no\n",
      "  - ▁she\n",
      "  - it\n",
      "  - ▁one\n",
      "  - ▁think\n",
      "  - ▁st\n",
      "  - ▁if\n",
      "  - ▁from\n",
      "  - ter\n",
      "  - ▁an\n",
      "  - an\n",
      "  - ur\n",
      "  - ▁out\n",
      "  - 'on'\n",
      "  - ▁go\n",
      "  - ck\n",
      "  - ▁would\n",
      "  - ▁were\n",
      "  - ▁w\n",
      "  - ▁will\n",
      "  - ▁about\n",
      "  - ▁right\n",
      "  - ment\n",
      "  - ▁her\n",
      "  - te\n",
      "  - ion\n",
      "  - ▁well\n",
      "  - ▁by\n",
      "  - ce\n",
      "  - ▁g\n",
      "  - ▁oh\n",
      "  - ▁up\n",
      "  - ro\n",
      "  - ra\n",
      "  - ▁when\n",
      "  - ▁some\n",
      "  - ▁also\n",
      "  - ▁their\n",
      "  - ers\n",
      "  - ow\n",
      "  - ▁more\n",
      "  - ▁time\n",
      "  - ate\n",
      "  - ▁has\n",
      "  - ▁people\n",
      "  - ▁see\n",
      "  - ▁pa\n",
      "  - el\n",
      "  - ▁get\n",
      "  - ▁ex\n",
      "  - ▁mean\n",
      "  - li\n",
      "  - ▁really\n",
      "  - v\n",
      "  - ▁ra\n",
      "  - ▁been\n",
      "  - ▁said\n",
      "  - '-'\n",
      "  - la\n",
      "  - ge\n",
      "  - ▁how\n",
      "  - ▁po\n",
      "  - ir\n",
      "  - ▁mo\n",
      "  - ▁who\n",
      "  - ▁because\n",
      "  - ▁co\n",
      "  - ▁other\n",
      "  - ▁f\n",
      "  - id\n",
      "  - ol\n",
      "  - ▁un\n",
      "  - ▁now\n",
      "  - ▁work\n",
      "  - ist\n",
      "  - us\n",
      "  - ▁your\n",
      "  - ▁them\n",
      "  - ver\n",
      "  - as\n",
      "  - ne\n",
      "  - ▁ca\n",
      "  - lo\n",
      "  - ▁fa\n",
      "  - ▁him\n",
      "  - ng\n",
      "  - ▁good\n",
      "  - ▁could\n",
      "  - ▁pro\n",
      "  - ive\n",
      "  - ▁con\n",
      "  - de\n",
      "  - un\n",
      "  - age\n",
      "  - ▁ma\n",
      "  - '?'\n",
      "  - at\n",
      "  - ▁ro\n",
      "  - ▁ba\n",
      "  - ▁then\n",
      "  - ▁com\n",
      "  - est\n",
      "  - vi\n",
      "  - ▁dis\n",
      "  - ies\n",
      "  - ance\n",
      "  - ▁su\n",
      "  - ▁even\n",
      "  - ▁any\n",
      "  - ut\n",
      "  - ad\n",
      "  - ul\n",
      "  - ▁se\n",
      "  - ▁two\n",
      "  - ▁bu\n",
      "  - ▁lo\n",
      "  - ▁say\n",
      "  - ▁la\n",
      "  - ▁fi\n",
      "  - is\n",
      "  - ▁li\n",
      "  - ▁over\n",
      "  - ▁new\n",
      "  - ▁man\n",
      "  - ▁sp\n",
      "  - ity\n",
      "  - ▁did\n",
      "  - ▁bo\n",
      "  - ▁very\n",
      "  - x\n",
      "  - end\n",
      "  - ▁which\n",
      "  - ▁our\n",
      "  - ▁after\n",
      "  - ▁o\n",
      "  - ke\n",
      "  - ▁p\n",
      "  - im\n",
      "  - ▁want\n",
      "  - ▁ha\n",
      "  - ▁v\n",
      "  - z\n",
      "  - ▁where\n",
      "  - ard\n",
      "  - um\n",
      "  - ▁into\n",
      "  - ru\n",
      "  - ▁di\n",
      "  - ▁lot\n",
      "  - ▁dr\n",
      "  - mp\n",
      "  - ▁day\n",
      "  - ated\n",
      "  - ci\n",
      "  - ▁these\n",
      "  - ▁than\n",
      "  - ▁take\n",
      "  - ▁kind\n",
      "  - ▁got\n",
      "  - ight\n",
      "  - ▁make\n",
      "  - ence\n",
      "  - ▁pre\n",
      "  - ▁going\n",
      "  - ish\n",
      "  - ▁k\n",
      "  - able\n",
      "  - ▁look\n",
      "  - ti\n",
      "  - per\n",
      "  - ▁here\n",
      "  - ▁en\n",
      "  - ▁ah\n",
      "  - ry\n",
      "  - ▁too\n",
      "  - ▁part\n",
      "  - ant\n",
      "  - one\n",
      "  - ▁ho\n",
      "  - ▁much\n",
      "  - ▁way\n",
      "  - ▁sa\n",
      "  - ▁something\n",
      "  - mo\n",
      "  - ▁us\n",
      "  - ▁th\n",
      "  - ▁mhm\n",
      "  - ▁mi\n",
      "  - ▁off\n",
      "  - pe\n",
      "  - ▁back\n",
      "  - les\n",
      "  - ▁cr\n",
      "  - ▁ri\n",
      "  - ▁fe\n",
      "  - und\n",
      "  - ▁fl\n",
      "  - port\n",
      "  - ▁school\n",
      "  - ▁ch\n",
      "  - ▁should\n",
      "  - ▁first\n",
      "  - ▁only\n",
      "  - ▁le\n",
      "  - ot\n",
      "  - tion\n",
      "  - ▁little\n",
      "  - ▁da\n",
      "  - ▁hu\n",
      "  - ▁d\n",
      "  - me\n",
      "  - ta\n",
      "  - ▁down\n",
      "  - ▁okay\n",
      "  - ▁come\n",
      "  - ain\n",
      "  - ff\n",
      "  - ▁car\n",
      "  - co\n",
      "  - ▁need\n",
      "  - ture\n",
      "  - ▁many\n",
      "  - ▁things\n",
      "  - ▁ta\n",
      "  - qu\n",
      "  - man\n",
      "  - ty\n",
      "  - iv\n",
      "  - ▁year\n",
      "  - he\n",
      "  - ▁thing\n",
      "  - ho\n",
      "  - ▁singapore\n",
      "  - po\n",
      "  - ▁vi\n",
      "  - ▁sc\n",
      "  - ▁still\n",
      "  - der\n",
      "  - ▁hi\n",
      "  - ▁never\n",
      "  - ▁qu\n",
      "  - ia\n",
      "  - ▁fr\n",
      "  - ▁min\n",
      "  - ▁most\n",
      "  - om\n",
      "  - ful\n",
      "  - ▁bi\n",
      "  - ▁long\n",
      "  - ig\n",
      "  - ▁years\n",
      "  - ous\n",
      "  - ▁three\n",
      "  - ▁play\n",
      "  - ▁before\n",
      "  - ▁pi\n",
      "  - ical\n",
      "  - ▁those\n",
      "  - ▁comp\n",
      "  - huh\n",
      "  - ▁live\n",
      "  - tor\n",
      "  - ise\n",
      "  - ▁old\n",
      "  - am\n",
      "  - rr\n",
      "  - ▁sta\n",
      "  - ▁n\n",
      "  - ick\n",
      "  - di\n",
      "  - ma\n",
      "  - ary\n",
      "  - ction\n",
      "  - ▁friend\n",
      "  - ition\n",
      "  - ▁gu\n",
      "  - ▁through\n",
      "  - pp\n",
      "  - for\n",
      "  - ie\n",
      "  - ious\n",
      "  - ▁sh\n",
      "  - ▁home\n",
      "  - lu\n",
      "  - ▁high\n",
      "  - ian\n",
      "  - cu\n",
      "  - ▁help\n",
      "  - ▁give\n",
      "  - ▁talk\n",
      "  - ▁sha\n",
      "  - ▁such\n",
      "  - ▁didn\n",
      "  - em\n",
      "  - ▁may\n",
      "  - ▁ga\n",
      "  - ▁'\n",
      "  - ▁gra\n",
      "  - ▁guess\n",
      "  - ▁every\n",
      "  - ▁app\n",
      "  - tic\n",
      "  - ▁tra\n",
      "  - ▁\"\n",
      "  - op\n",
      "  - ▁made\n",
      "  - '\"'\n",
      "  - ▁op\n",
      "  - ▁own\n",
      "  - ▁mar\n",
      "  - 'no'\n",
      "  - ▁ph\n",
      "  - ▁life\n",
      "  - ▁y\n",
      "  - ak\n",
      "  - ine\n",
      "  - ▁pu\n",
      "  - ▁place\n",
      "  - ▁always\n",
      "  - ▁start\n",
      "  - ▁jo\n",
      "  - ▁pe\n",
      "  - ▁let\n",
      "  - ▁name\n",
      "  - ni\n",
      "  - ▁same\n",
      "  - ▁last\n",
      "  - ▁cl\n",
      "  - ph\n",
      "  - ▁both\n",
      "  - ▁pri\n",
      "  - ities\n",
      "  - ▁another\n",
      "  - and\n",
      "  - ▁al\n",
      "  - ▁boy\n",
      "  - ving\n",
      "  - ▁actually\n",
      "  - ▁person\n",
      "  - ▁went\n",
      "  - ▁yes\n",
      "  - ca\n",
      "  - ally\n",
      "  - ▁h\n",
      "  - ▁great\n",
      "  - ▁thought\n",
      "  - ▁used\n",
      "  - act\n",
      "  - ▁feel\n",
      "  - ward\n",
      "  - ▁different\n",
      "  - ▁cons\n",
      "  - ▁show\n",
      "  - ▁watch\n",
      "  - ▁being\n",
      "  - ▁money\n",
      "  - ay\n",
      "  - ▁try\n",
      "  - ▁why\n",
      "  - ▁big\n",
      "  - ens\n",
      "  - ▁cha\n",
      "  - ▁find\n",
      "  - ▁hand\n",
      "  - ▁real\n",
      "  - ▁four\n",
      "  - ial\n",
      "  - ▁ne\n",
      "  - ▁che\n",
      "  - ▁read\n",
      "  - ▁five\n",
      "  - ▁family\n",
      "  - ag\n",
      "  - ▁change\n",
      "  - ▁add\n",
      "  - ha\n",
      "  - ▁put\n",
      "  - par\n",
      "  - lic\n",
      "  - side\n",
      "  - ▁came\n",
      "  - ▁under\n",
      "  - ness\n",
      "  - ▁per\n",
      "  - j\n",
      "  - ▁around\n",
      "  - ▁end\n",
      "  - ▁house\n",
      "  - if\n",
      "  - ▁while\n",
      "  - vo\n",
      "  - ▁act\n",
      "  - ▁happen\n",
      "  - ▁plan\n",
      "  - mit\n",
      "  - ▁far\n",
      "  - ▁tri\n",
      "  - ▁ten\n",
      "  - ▁du\n",
      "  - ▁win\n",
      "  - ▁tea\n",
      "  - ze\n",
      "  - ▁better\n",
      "  - ▁sure\n",
      "  - ▁mu\n",
      "  - ▁use\n",
      "  - ▁anything\n",
      "  - ▁love\n",
      "  - ▁world\n",
      "  - ▁hard\n",
      "  - ure\n",
      "  - ▁does\n",
      "  - ▁war\n",
      "  - ▁stuff\n",
      "  - ▁ja\n",
      "  - ▁must\n",
      "  - min\n",
      "  - gg\n",
      "  - ▁ru\n",
      "  - ▁care\n",
      "  - ▁tell\n",
      "  - ▁pl\n",
      "  - ▁doing\n",
      "  - ▁probably\n",
      "  - ▁found\n",
      "  - ative\n",
      "  - ▁point\n",
      "  - ach\n",
      "  - ▁ju\n",
      "  - ip\n",
      "  - ▁again\n",
      "  - ▁interest\n",
      "  - ▁state\n",
      "  - ▁week\n",
      "  - na\n",
      "  - ▁might\n",
      "  - ▁pretty\n",
      "  - ▁ki\n",
      "  - ▁fo\n",
      "  - ber\n",
      "  - ▁am\n",
      "  - line\n",
      "  - led\n",
      "  - ▁six\n",
      "  - ▁acc\n",
      "  - ▁bri\n",
      "  - ▁call\n",
      "  - ▁sw\n",
      "  - ▁each\n",
      "  - ▁business\n",
      "  - ▁keep\n",
      "  - ▁away\n",
      "  - cause\n",
      "  - ▁pass\n",
      "  - ▁va\n",
      "  - ▁children\n",
      "  - ▁pay\n",
      "  - ▁count\n",
      "  - ▁public\n",
      "  - ▁everything\n",
      "  - land\n",
      "  - ▁though\n",
      "  - ▁men\n",
      "  - bo\n",
      "  - ▁young\n",
      "  - ▁na\n",
      "  - ▁move\n",
      "  - ough\n",
      "  - ating\n",
      "  - com\n",
      "  - ▁month\n",
      "  - ton\n",
      "  - ▁close\n",
      "  - ▁few\n",
      "  - '!'\n",
      "  - ▁maybe\n",
      "  - ▁imp\n",
      "  - son\n",
      "  - ▁grow\n",
      "  - ▁u\n",
      "  - ▁turn\n",
      "  - ible\n",
      "  - ▁em\n",
      "  - ▁air\n",
      "  - ▁ever\n",
      "  - our\n",
      "  - ▁sea\n",
      "  - ▁fun\n",
      "  - ▁government\n",
      "  - ▁miss\n",
      "  - ▁done\n",
      "  - ▁next\n",
      "  - ▁kids\n",
      "  - ▁cor\n",
      "  - ▁set\n",
      "  - ▁run\n",
      "  - way\n",
      "  - ▁wa\n",
      "  - ▁getting\n",
      "  - ▁eight\n",
      "  - ▁open\n",
      "  - ▁job\n",
      "  - ▁problem\n",
      "  - ook\n",
      "  - ▁night\n",
      "  - ▁learn\n",
      "  - ▁book\n",
      "  - ual\n",
      "  - ▁ti\n",
      "  - ▁best\n",
      "  - cept\n",
      "  - ▁during\n",
      "  - ▁small\n",
      "  - ex\n",
      "  - ▁without\n",
      "  - ▁water\n",
      "  - ▁trans\n",
      "  - ▁course\n",
      "  - ▁once\n",
      "  - ▁sit\n",
      "  - ▁area\n",
      "  - ▁country\n",
      "  - ▁mister\n",
      "  - ▁nothing\n",
      "  - ▁whole\n",
      "  - ▁believe\n",
      "  - ▁service\n",
      "  - ▁took\n",
      "  - ▁face\n",
      "  - ▁bad\n",
      "  - ▁later\n",
      "  - ▁head\n",
      "  - ▁called\n",
      "  - ▁seven\n",
      "  - ▁art\n",
      "  - ▁since\n",
      "  - ▁er\n",
      "  - ▁fact\n",
      "  - ▁city\n",
      "  - ▁market\n",
      "  - ▁hour\n",
      "  - ▁continue\n",
      "  - ship\n",
      "  - ▁invest\n",
      "  - ▁exactly\n",
      "  - ▁large\n",
      "  - ▁true\n",
      "  - ▁nine\n",
      "  - ▁sub\n",
      "  - ▁having\n",
      "  - ▁game\n",
      "  - va\n",
      "  - ▁lu\n",
      "  - ▁conf\n",
      "  - ▁case\n",
      "  - ▁doesn\n",
      "  - ▁certain\n",
      "  - ▁wi\n",
      "  - ▁law\n",
      "  - ▁else\n",
      "  - fi\n",
      "  - ▁left\n",
      "  - ▁enough\n",
      "  - ▁second\n",
      "  - ▁gonna\n",
      "  - ▁food\n",
      "  - ▁hope\n",
      "  - ▁saw\n",
      "  - ▁between\n",
      "  - ▁je\n",
      "  - bi\n",
      "  - ▁girl\n",
      "  - ▁company\n",
      "  - ▁able\n",
      "  - ▁expect\n",
      "  - ▁told\n",
      "  - ▁stand\n",
      "  - ▁group\n",
      "  - ▁main\n",
      "  - ▁walk\n",
      "  - ▁cause\n",
      "  - ▁however\n",
      "  - ▁number\n",
      "  - ▁follow\n",
      "  - ▁near\n",
      "  - ▁yet\n",
      "  - ▁sometimes\n",
      "  - ▁train\n",
      "  - ▁lead\n",
      "  - ▁system\n",
      "  - ▁remain\n",
      "  - ▁develop\n",
      "  - gra\n",
      "  - ▁word\n",
      "  - ▁exc\n",
      "  - ▁together\n",
      "  - ▁consider\n",
      "  - ▁town\n",
      "  - ▁less\n",
      "  - ator\n",
      "  - ▁important\n",
      "  - ▁remember\n",
      "  - ▁free\n",
      "  - ▁quite\n",
      "  - ▁understand\n",
      "  - ▁bra\n",
      "  - ▁support\n",
      "  - ▁idea\n",
      "  - ▁stop\n",
      "  - ▁reason\n",
      "  - ▁nice\n",
      "  - ▁mm\n",
      "  - ▁agree\n",
      "  - ▁low\n",
      "  - ▁against\n",
      "  - ▁issue\n",
      "  - ▁become\n",
      "  - ▁today\n",
      "  - ▁side\n",
      "  - ▁student\n",
      "  - ▁matter\n",
      "  - ▁question\n",
      "  - ▁mother\n",
      "  - ▁father\n",
      "  - ▁hundred\n",
      "  - ▁sort\n",
      "  - ▁eat\n",
      "  - ▁already\n",
      "  - ▁rest\n",
      "  - ▁line\n",
      "  - ▁asked\n",
      "  - ▁include\n",
      "  - ▁upon\n",
      "  - ▁office\n",
      "  - ▁won\n",
      "  - ▁class\n",
      "  - ▁wait\n",
      "  - ▁twenty\n",
      "  - ▁half\n",
      "  - ▁light\n",
      "  - ▁price\n",
      "  - ▁almost\n",
      "  - ash\n",
      "  - ▁child\n",
      "  - ▁sign\n",
      "  - ▁least\n",
      "  - ▁several\n",
      "  - press\n",
      "  - ▁either\n",
      "  - ▁minute\n",
      "  - ▁himself\n",
      "  - ▁parents\n",
      "  - ▁room\n",
      "  - ▁whatever\n",
      "  - ▁general\n",
      "  - ▁cost\n",
      "  - ▁among\n",
      "  - ▁direct\n",
      "  - ▁computer\n",
      "  - ▁appear\n",
      "  - ▁meet\n",
      "  - ▁ski\n",
      "  - ▁return\n",
      "  - ▁couple\n",
      "  - ▁product\n",
      "  - ▁suppose\n",
      "  - ▁definitely\n",
      "  - ▁america\n",
      "  - ▁term\n",
      "  - ▁usually\n",
      "  - ▁strong\n",
      "  - ▁current\n",
      "  - ▁arm\n",
      "  - ▁speak\n",
      "  - ▁local\n",
      "  - ▁south\n",
      "  - ▁experience\n",
      "  - ▁full\n",
      "  - ▁north\n",
      "  - ▁elect\n",
      "  - ▁leave\n",
      "  - ▁provide\n",
      "  - qui\n",
      "  - ▁power\n",
      "  - ▁movie\n",
      "  - ▁everyone\n",
      "  - ▁making\n",
      "  - ▁member\n",
      "  - ▁woman\n",
      "  - ▁somebody\n",
      "  - ▁wonder\n",
      "  - ▁short\n",
      "  - ▁health\n",
      "  - ▁police\n",
      "  - ▁bank\n",
      "  - ▁until\n",
      "  - ▁companies\n",
      "  - ▁everybody\n",
      "  - ▁knew\n",
      "  - ▁program\n",
      "  - ▁music\n",
      "  - ▁york\n",
      "  - ▁land\n",
      "  - ▁doctor\n",
      "  - ▁answer\n",
      "  - ▁building\n",
      "  - ▁employ\n",
      "  - ▁travel\n",
      "  - ▁major\n",
      "  - ▁seems\n",
      "  - ▁safe\n",
      "  - gue\n",
      "  - ▁college\n",
      "  - ▁along\n",
      "  - ▁clear\n",
      "  - ▁especially\n",
      "  - ▁umhu\n",
      "  - ▁result\n",
      "  - ▁type\n",
      "  - ▁court\n",
      "  - ▁black\n",
      "  - ▁hold\n",
      "  - ▁myself\n",
      "  - ▁education\n",
      "  - ▁social\n",
      "  - ▁enjoy\n",
      "  - ▁became\n",
      "  - ▁whether\n",
      "  - ▁morning\n",
      "  - ▁difficult\n",
      "  - ▁shi\n",
      "  - ▁felt\n",
      "  - ▁husband\n",
      "  - ▁white\n",
      "  - ▁taking\n",
      "  - ▁million\n",
      "  - ▁require\n",
      "  - ▁early\n",
      "  - ency\n",
      "  - ▁visit\n",
      "  - ▁level\n",
      "  - ▁brother\n",
      "  - ▁married\n",
      "  - ▁further\n",
      "  - ▁affect\n",
      "  - ▁serve\n",
      "  - ▁present\n",
      "  - ▁park\n",
      "  - ▁effect\n",
      "  - ▁wife\n",
      "  - ▁teacher\n",
      "  - ▁cannot\n",
      "  - ▁community\n",
      "  - ▁street\n",
      "  - ▁period\n",
      "  - ▁national\n",
      "  - ▁view\n",
      "  - ▁future\n",
      "  - ▁daughter\n",
      "  - ▁situation\n",
      "  - ▁grand\n",
      "  - ▁success\n",
      "  - ▁perform\n",
      "  - ▁concern\n",
      "  - ▁complete\n",
      "  - ▁example\n",
      "  - ized\n",
      "  - ▁thousand\n",
      "  - ▁increase\n",
      "  - ▁began\n",
      "  - ▁final\n",
      "  - ▁east\n",
      "  - ▁sense\n",
      "  - ▁charge\n",
      "  - ▁record\n",
      "  - ▁born\n",
      "  - ▁instead\n",
      "  - ▁receive\n",
      "  - ▁women\n",
      "  - ▁across\n",
      "  - ▁information\n",
      "  - ▁although\n",
      "  - ▁process\n",
      "  - ▁condition\n",
      "  - ▁security\n",
      "  - ▁treat\n",
      "  - ▁funny\n",
      "  - ▁custom\n",
      "  - ▁cold\n",
      "  - ▁behind\n",
      "  - ified\n",
      "  - ▁ground\n",
      "  - cycl\n",
      "  - ▁depend\n",
      "  - ▁themselves\n",
      "  - ▁design\n",
      "  - ▁slow\n",
      "  - ▁third\n",
      "  - ▁smoke\n",
      "  - ▁wrong\n",
      "  - ▁project\n",
      "  - ▁space\n",
      "  - ▁drink\n",
      "  - ▁particular\n",
      "  - ▁listen\n",
      "  - ▁thirty\n",
      "  - ▁special\n",
      "  - ability\n",
      "  - ▁improve\n",
      "  - ▁attack\n",
      "  - ▁happy\n",
      "  - ▁strange\n",
      "  - ▁english\n",
      "  - ▁value\n",
      "  - ▁brought\n",
      "  - ▁private\n",
      "  - ▁account\n",
      "  - ▁china\n",
      "  - ▁spoke\n",
      "  - ▁foreign\n",
      "  - ▁possible\n",
      "  - ▁author\n",
      "  - ▁circ\n",
      "  - ▁voice\n",
      "  - ▁figure\n",
      "  - ▁control\n",
      "  - ▁according\n",
      "  - ▁green\n",
      "  - ▁university\n",
      "  - ▁language\n",
      "  - ▁please\n",
      "  - ▁animal\n",
      "  - ▁church\n",
      "  - ▁society\n",
      "  - ▁dream\n",
      "  - ’\n",
      "  - q\n",
      "  - ':'\n",
      "  - ;\n",
      "  - —\n",
      "  - ‘\n",
      "  - ”\n",
      "  - _\n",
      "  - '3'\n",
      "  - '8'\n",
      "  - <\n",
      "  - '>'\n",
      "  - '1'\n",
      "  - –\n",
      "  - '7'\n",
      "  - (\n",
      "  - )\n",
      "  - '0'\n",
      "  - '2'\n",
      "  - '4'\n",
      "  - +\n",
      "  - '&'\n",
      "  - '5'\n",
      "  - '9'\n",
      "  - ü\n",
      "  - é\n",
      "  - /\n",
      "  - á\n",
      "  - ó\n",
      "  - ō\n",
      "  - ú\n",
      "  - ']'\n",
      "  - â\n",
      "  - í\n",
      "  - ã\n",
      "  - ð\n",
      "  - ā\n",
      "  - ć\n",
      "  - č\n",
      "  - š\n",
      "  - è\n",
      "  - ë\n",
      "  - '`'\n",
      "  - ç\n",
      "  - ū\n",
      "  - ạ\n",
      "  - ø\n",
      "  - '='\n",
      "  - à\n",
      "  - ł\n",
      "  - α\n",
      "  - ô\n",
      "  - к\n",
      "  - '}'\n",
      "  - å\n",
      "  - ă\n",
      "  - и\n",
      "  - ī\n",
      "  - π\n",
      "  - œ\n",
      "  - \\\n",
      "  - '['\n",
      "  - ñ\n",
      "  - ß\n",
      "  - ö\n",
      "  - ä\n",
      "  - '6'\n",
      "  - з\n",
      "  - н\n",
      "  - û\n",
      "  - '%'\n",
      "  - '{'\n",
      "  - ¡\n",
      "  - æ\n",
      "  - ê\n",
      "  - þ\n",
      "  - ę\n",
      "  - ě\n",
      "  - ğ\n",
      "  - ń\n",
      "  - ő\n",
      "  - ř\n",
      "  - ž\n",
      "  - ʻ\n",
      "  - в\n",
      "  - е\n",
      "  - й\n",
      "  - л\n",
      "  - ь\n",
      "  - χ\n",
      "  - “\n",
      "optim:\n",
      "  name: novograd\n",
      "  lr: 0.05\n",
      "  betas:\n",
      "  - 0.8\n",
      "  - 0.25\n",
      "  weight_decay: 0.001\n",
      "  sched:\n",
      "    name: CosineAnnealing\n",
      "    warmup_steps: 1000\n",
      "    warmup_ratio: null\n",
      "    min_lr: 1.0e-09\n",
      "    last_epoch: -1\n",
      "target: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE\n",
      "nemo_version: 2.4.0\n",
      "decoding:\n",
      "  strategy: greedy_batch\n",
      "  preserve_alignments: null\n",
      "  compute_timestamps: null\n",
      "  word_seperator: ' '\n",
      "  segment_seperators:\n",
      "  - .\n",
      "  - '!'\n",
      "  - '?'\n",
      "  segment_gap_threshold: null\n",
      "  ctc_timestamp_type: all\n",
      "  batch_dim_index: 0\n",
      "  greedy:\n",
      "    preserve_alignments: false\n",
      "    compute_timestamps: false\n",
      "    preserve_frame_confidence: false\n",
      "    confidence_method_cfg:\n",
      "      name: entropy\n",
      "      entropy_type: tsallis\n",
      "      alpha: 0.33\n",
      "      entropy_norm: exp\n",
      "      temperature: DEPRECATED\n",
      "    ngram_lm_model: null\n",
      "    ngram_lm_alpha: 0.0\n",
      "    allow_cuda_graphs: true\n",
      "  beam:\n",
      "    beam_size: 4\n",
      "    search_type: default\n",
      "    preserve_alignments: false\n",
      "    compute_timestamps: false\n",
      "    return_best_hypothesis: true\n",
      "    allow_cuda_graphs: true\n",
      "    beam_alpha: null\n",
      "    beam_beta: 1.0\n",
      "    beam_threshold: 20.0\n",
      "    kenlm_path: null\n",
      "    ngram_lm_alpha: 1.0\n",
      "    ngram_lm_model: null\n",
      "    flashlight_cfg:\n",
      "      lexicon_path: null\n",
      "      boost_path: null\n",
      "      beam_size_token: 16\n",
      "      beam_threshold: 20.0\n",
      "      unk_weight: -.inf\n",
      "      sil_weight: 0.0\n",
      "    pyctcdecode_cfg:\n",
      "      beam_prune_logp: -10.0\n",
      "      token_min_logp: -5.0\n",
      "      prune_history: false\n",
      "      hotwords: null\n",
      "      hotword_weight: 10.0\n",
      "  wfst:\n",
      "    beam_size: 4\n",
      "    search_type: riva\n",
      "    return_best_hypothesis: true\n",
      "    preserve_alignments: false\n",
      "    compute_timestamps: false\n",
      "    decoding_mode: nbest\n",
      "    open_vocabulary_decoding: false\n",
      "    beam_width: 10.0\n",
      "    lm_weight: 1.0\n",
      "    device: cuda\n",
      "    arpa_lm_path: null\n",
      "    wfst_lm_path: null\n",
      "    riva_decoding_cfg: {}\n",
      "    k2_decoding_cfg:\n",
      "      search_beam: 20.0\n",
      "      output_beam: 10.0\n",
      "      min_active_states: 30\n",
      "      max_active_states: 10000\n",
      "  confidence_cfg:\n",
      "    preserve_frame_confidence: false\n",
      "    preserve_token_confidence: false\n",
      "    preserve_word_confidence: false\n",
      "    exclude_blank: true\n",
      "    aggregation: min\n",
      "    tdt_include_duration: false\n",
      "    method_cfg:\n",
      "      name: entropy\n",
      "      entropy_type: tsallis\n",
      "      alpha: 0.33\n",
      "      entropy_norm: exp\n",
      "      temperature: DEPRECATED\n",
      "  temperature: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "cfg = copy.deepcopy(citrinet.cfg)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
